{"key": "HADOOP-19744", "project": "HADOOP", "title": "[JDK24] Do not use SecurityManager in SubjectUtil.checkThreadInheritsSubject", "status": "Open", "reporter": "Istvan Toth", "created": "2025-11-18T06:07:59.000+0000", "description": "While the code works fine, it causes Hadoop to emit warnings \r\n\r\n{noformat}\r\nWARNING: A terminally deprecated method in java.lang.System has been called\r\nWARNING: System::setSecurityManager has been called by org.apache.hadoop.security.authentication.util.SubjectUtil (file:/Users/stevel/.m2/repository/org/apache/hadoop/hadoop-auth/3.5.0-SNAPSHOT/hadoop-auth-3.5.0-SNAPSHOT.jar)\r\nWARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.SubjectUtil\r\n{noformat}\r\n\r\nThe SecurityManager check is only really needed for Java 22-23. \r\n\r\nWe  can shortcut the check by returning true for JDK21 and earlier and false for JDK24 and later, and only check and emit warnings  for 22-23, which are EOL anyway.\r\n\r\nWe can also try to duplicate the logic the JVM uses to determine whether SecurityManager is enabled without explicitly calling it, but that would be less robust than the current check.\r\n", "comments": ["I think it's better to just assume that SecurityManager is disabled, and get rid of the extra logic for JDK 22-23.\r\nThere is a small chance that we lose the optimization, but 22-23 is not a version we expect anyone to use for production anway.", "hadoop-yetus commented on PR #8088:\nURL: https://github.com/apache/hadoop/pull/8088#issuecomment-3546562685\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 42s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m  4s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  15m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 37s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 48s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m  4s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 36s | [/branch-spotbugs-hadoop-common-project_hadoop-auth-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/1/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-auth-warnings.html) |  hadoop-common-project/hadoop-auth in trunk has 8 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |   1m 29s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/1/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) |  hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  15m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 17s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 14s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 46s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 32s | [/results-checkstyle-hadoop-common-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/1/artifact/out/results-checkstyle-hadoop-common-project.txt) |  hadoop-common-project: The patch generated 1 new + 5 unchanged - 0 fixed = 6 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m  1s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  15m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 50s |  |  hadoop-auth in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  15m 56s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 32s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 122m 49s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8088 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 53d3e6e26ba4 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8944d13edc00f88f81b261b4f5d7b401f4ac2332 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/1/testReport/ |\r\n   | Max. process+thread count | 1300 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "I've got code in cloudstore to turn jdk logging up to max for debugging.\r\n\r\n{code}\r\n  protected void enableJvmLogging() {\r\n    println(\"Enabling JVM logging\");\r\n    ConsoleHandler handler = new ConsoleHandler();\r\n    handler.setLevel(ALL);\r\n    java.util.logging.Logger log = LogManager.getLogManager().getLogger(\"\");\r\n    log.addHandler(handler);\r\n    log.setLevel(ALL);\r\n  }\r\n{code}\r\nMaybe we just do something  here for the specific log, within the try/finally block?\r\n\r\nthat way: no need to do variants for different releases, and we have the JRE silent everywhere\r\n", "That could be an option, but I really don't think we need to care that much about a JDK22/23 specific issue.\r\n\r\nThe warning happens when we detect what mode Java 22/23 is in so that we can get handle the Java22-23 + enabled SecurityManager case, and is absolutely not needed for other  (17-21, 24- ) JVM versions.\r\n\r\nIn the unlikely case that someone both explcitly enables the SecurityManager  and is running Hadoop JDK22/23, we will be making some avoidable calls on new thread creation, which we always must do on Java24+ anyway.\r\n", "hadoop-yetus commented on PR #8088:\nURL: https://github.com/apache/hadoop/pull/8088#issuecomment-3547134796\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 26s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m  9s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  29m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m  1s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 17s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 10s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m  1s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m  5s | [/branch-spotbugs-hadoop-common-project_hadoop-auth-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/2/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-auth-warnings.html) |  hadoop-common-project/hadoop-auth in trunk has 8 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |   3m  0s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/2/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) |  hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  6s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 10s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m  8s | [/results-checkstyle-hadoop-common-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/2/artifact/out/results-checkstyle-hadoop-common-project.txt) |  hadoop-common-project: The patch generated 1 new + 5 unchanged - 0 fixed = 6 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  3s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m  3s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 43s |  |  hadoop-auth in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  22m 17s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 56s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 213m  1s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8088 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 89b0e0e88516 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0b795b3ad770e79466cfd06bf473731191b868d5 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/2/testReport/ |\r\n   | Max. process+thread count | 1278 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8088:\nURL: https://github.com/apache/hadoop/pull/8088#issuecomment-3548086884\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 42s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 38s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  15m 46s |  |  trunk passed  |\r\n   | -1 :x: |  compile  |   6m 47s | [/branch-compile-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/branch-compile-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   6m 45s | [/branch-compile-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/branch-compile-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  8s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m  4s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 35s | [/branch-spotbugs-hadoop-common-project_hadoop-auth-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-auth-warnings.html) |  hadoop-common-project/hadoop-auth in trunk has 8 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |   1m 32s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) |  hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  15m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 19s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 50s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |   6m 17s | [/patch-compile-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/patch-compile-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   6m 17s | [/patch-compile-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/patch-compile-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   6m 48s | [/patch-compile-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/patch-compile-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   6m 48s | [/patch-compile-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/patch-compile-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 31s | [/results-checkstyle-hadoop-common-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/results-checkstyle-hadoop-common-project.txt) |  hadoop-common-project: The patch generated 1 new + 5 unchanged - 0 fixed = 6 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  5s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m  0s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  15m  9s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 48s |  |  hadoop-auth in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  16m  6s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 115m 18s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8088 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 9fdc29dc5e2a 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6858636fe41dbaa8cd0b1810cc28ea4309579a0b |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/testReport/ |\r\n   | Max. process+thread count | 1296 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8088/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "stoty commented on PR #8088:\nURL: https://github.com/apache/hadoop/pull/8088#issuecomment-3552083250\n\n   The build failuers are caused by some kind of JS repo outage.\n\n\n", "steveloughran merged PR #8088:\nURL: https://github.com/apache/hadoop/pull/8088\n\n\n", "stoty commented on PR #8088:\nURL: https://github.com/apache/hadoop/pull/8088#issuecomment-3552439024\n\n   > +1\r\n   > \r\n   > checkstyle is complaining because you could just go\r\n   > \r\n   > ```\r\n   > return JAVA_SPEC_VER <= 21;\r\n   > ```\r\n   > \r\n   > but the structure here lines up for options of changing it in future (unlikely, I know)\r\n   \r\n   Ah, true.\r\n   Adding the rambling comments to that one would have been awkward, though.\n\n\n"], "labels": ["pull-request-available"], "summary": "While the code works fine, it causes Hadoop to emit warnings \r\n\r\n{noformat}\r\nWAR", "qna": [{"question": "What is the issue title?", "answer": "[JDK24] Do not use SecurityManager in SubjectUtil.checkThreadInheritsSubject"}, {"question": "Who reported this issue?", "answer": "Istvan Toth"}]}
{"key": "HADOOP-19743", "project": "HADOOP", "title": "S3A: ITestS3ACommitterMRJob MR process OOM on java17", "status": "Resolved", "reporter": "Steve Loughran", "created": "2025-11-17T18:35:45.000+0000", "description": "MR AppMaster launched by S3ACommitterMRJob can OOM on java17 while launching because the heap size is set for all forked processes to 128M.\r\n{code}\r\nava.lang.OutOfMemoryError: Java heap space\r\n\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\r\n\tat org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:629)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:660)\r\n{code}\r\n\r\nFix: double it. Why not larger? increases memory footprint and in parallel test runs with many junit threads running there's a lot of load on memory", "comments": ["steveloughran opened a new pull request, #8087:\nURL: https://github.com/apache/hadoop/pull/8087\n\n   \r\n   Contributed by Steve Loughran\r\n   \r\n   \r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   running ITestS3ACommitterMRJob from IDE and CLI.\r\n   \r\n   Saw a new error message coming from #8061 ; reported it on the JIRA.\r\n   ```\r\n   [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob\r\n   WARNING: A terminally deprecated method in java.lang.System has been called\r\n   WARNING: System::setSecurityManager has been called by org.apache.hadoop.security.authentication.util.SubjectUtil (file:/Users/stevel/.m2/repository/org/apache/hadoop/hadoop-auth/3.5.0-SNAPSHOT/hadoop-auth-3.5.0-SNAPSHOT.jar)\r\n   WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.SubjectUtil\r\n   WARNING: System::setSecurityManager will be removed in a future release\r\n   ```\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8087:\nURL: https://github.com/apache/hadoop/pull/8087#issuecomment-3543463698\n\n   @mukund-thakur and @ahmarsuhail something of relevance for you two; I will cp to branch-3.4 to assist in test running there too\n\n\n", "hadoop-yetus commented on PR #8087:\nURL: https://github.com/apache/hadoop/pull/8087#issuecomment-3543922973\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 31s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8087/1/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) |  hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  31m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  32m  1s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 31s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 24s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8087/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8087 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 960b3cde5241 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b9a4eeba14f1b87b1c3e9828bfeaa284d360fb02 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8087/1/testReport/ |\r\n   | Max. process+thread count | 593 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8087/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8087:\nURL: https://github.com/apache/hadoop/pull/8087#issuecomment-3546811429\n\n   jenkins couldn't update the PR; here's the results from the console. Failures are from the new spotbugs complaints. which need a review then turning off\r\n   \r\n   ```\r\n   \r\n   21:32:02  ============================================================================\r\n   21:32:02  ============================================================================\r\n   21:32:02                           Adding comment to Github\r\n   21:32:02  ============================================================================\r\n   21:32:02  ============================================================================\r\n   21:32:02  \r\n   21:32:02  \r\n   21:32:07  ERROR: Failed to write github status. Token expired or missing repo:status write?\r\n   21:32:08  \r\n   21:32:08  \r\n   21:32:08  ============================================================================\r\n   21:32:08  ============================================================================\r\n   21:32:08                               Writing HTML to \r\n   21:32:08  ============================================================================\r\n   21:32:08  ============================================================================\r\n   21:32:08  \r\n   21:32:08  \r\n   21:32:11  \r\n   21:32:11  \r\n   21:32:11  -1 overall\r\n   21:32:11  \r\n   21:32:11  | Vote |        Subsystem |  Runtime   | Comment\r\n   21:32:11  ============================================================================\r\n   21:32:11  |   0  |          reexec  |  21m 41s   | Docker mode activated. \r\n   21:32:11  +---------------------------------------------------------------------------\r\n   21:32:11  |      |                  |            | Prechecks \r\n   21:32:11  +---------------------------------------------------------------------------\r\n   21:32:11  |  +1  |         dupname  |   0m  0s   | No case conflicting files found. \r\n   21:32:11  |   0  |       codespell  |   0m  0s   | codespell was not available. \r\n   21:32:11  |   0  |      detsecrets  |   0m  0s   | detect-secrets was not available. \r\n   21:32:11  |  +1  |         @author  |   0m  0s   | The patch does not contain any @author \r\n   21:32:11  |      |                  |            | tags.\r\n   21:32:11  |  +1  |      test4tests  |   0m  0s   | The patch appears to include 1 new or \r\n   21:32:11  |      |                  |            | modified test files.\r\n   21:32:11  +---------------------------------------------------------------------------\r\n   21:32:11  |      |                  |            | trunk Compile Tests \r\n   21:32:11  +---------------------------------------------------------------------------\r\n   21:32:11  |  +1  |      mvninstall  |  42m 49s   | trunk passed \r\n   21:32:11  |  +1  |         compile  |   0m 46s   | trunk passed with JDK \r\n   21:32:11  |      |                  |            | Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04\r\n   21:32:11  |  +1  |         compile  |   0m 45s   | trunk passed with JDK \r\n   21:32:11  |      |                  |            | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04\r\n   21:32:11  |  +1  |      checkstyle  |   0m 34s   | trunk passed \r\n   21:32:11  |  +1  |         mvnsite  |   0m 53s   | trunk passed \r\n   21:32:11  |  +1  |         javadoc  |   0m 41s   | trunk passed with JDK \r\n   21:32:11  |      |                  |            | Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04\r\n   21:32:11  |  +1  |         javadoc  |   0m 37s   | trunk passed with JDK \r\n   21:32:11  |      |                  |            | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04\r\n   21:32:11  |  -1  |        spotbugs  |   1m 31s   | hadoop-tools/hadoop-aws in trunk has \r\n   21:32:11  |      |                  |            | 188 extant spotbugs warnings.\r\n   21:32:11  |  +1  |    shadedclient  |  31m 22s   | branch has no errors when building and \r\n   21:32:11  |      |                  |            | testing our client artifacts.\r\n   21:32:11  +---------------------------------------------------------------------------\r\n   21:32:11  |      |                  |            | Patch Compile Tests \r\n   21:32:11  +---------------------------------------------------------------------------\r\n   21:32:11  |  +1  |      mvninstall  |   0m 41s   | the patch passed \r\n   21:32:11  |  +1  |         compile  |   0m 39s   | the patch passed with JDK \r\n   21:32:11  |      |                  |            | Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04\r\n   21:32:11  |  +1  |           javac  |   0m 39s   | the patch passed \r\n   21:32:11  |  +1  |         compile  |   0m 37s   | the patch passed with JDK \r\n   21:32:11  |      |                  |            | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04\r\n   21:32:11  |  +1  |           javac  |   0m 37s   | the patch passed \r\n   21:32:11  |  +1  |          blanks  |   0m  0s   | The patch has no blanks issues. \r\n   21:32:11  |  +1  |      checkstyle  |   0m 22s   | the patch passed \r\n   21:32:11  |  +1  |         mvnsite  |   0m 45s   | the patch passed \r\n   21:32:11  |  +1  |         javadoc  |   0m 30s   | the patch passed with JDK \r\n   21:32:11  |      |                  |            | Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04\r\n   21:32:11  |  +1  |         javadoc  |   0m 29s   | the patch passed with JDK \r\n   21:32:11  |      |                  |            | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04\r\n   21:32:11  |  +1  |        spotbugs  |   1m 31s   | the patch passed \r\n   21:32:11  |  +1  |    shadedclient  |  32m  1s   | patch has no errors when building and \r\n   21:32:11  |      |                  |            | testing our client artifacts.\r\n   21:32:11  +---------------------------------------------------------------------------\r\n   21:32:11  |      |                  |            | Other Tests \r\n   21:32:11  +---------------------------------------------------------------------------\r\n   21:32:11  |  +1  |            unit  |   3m 31s   | hadoop-aws in the patch passed. \r\n   21:32:11  |  +1  |      asflicense  |   0m 37s   | The patch does not generate ASF \r\n   21:32:11  |      |                  |            | License warnings.\r\n   21:32:11  |      |                  | 144m 24s   |\r\n   21:32:11\r\n   ```\r\n   \n\n\n", "steveloughran merged PR #8087:\nURL: https://github.com/apache/hadoop/pull/8087\n\n\n"], "labels": ["pull-request-available"], "summary": "MR AppMaster launched by S3ACommitterMRJob can OOM on java17 while launching bec", "qna": [{"question": "What is the issue title?", "answer": "S3A: ITestS3ACommitterMRJob MR process OOM on java17"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19742", "project": "HADOOP", "title": "S3A: AAL - Upgrade version to 1.3.1", "status": "Resolved", "reporter": "Ahmar Suhail", "created": "2025-11-17T10:42:24.000+0000", "description": "Upgrade AAL version to 1.3.1", "comments": ["ahmarsuhail opened a new pull request, #8086:\nURL: https://github.com/apache/hadoop/pull/8086\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Upgrades AAL version\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #8086:\nURL: https://github.com/apache/hadoop/pull/8086#issuecomment-3544323309\n\n   LGTM.\n\n\n", "ahmarsuhail merged PR #8086:\nURL: https://github.com/apache/hadoop/pull/8086\n\n\n", "steveloughran commented on PR #8086:\nURL: https://github.com/apache/hadoop/pull/8086#issuecomment-3551942600\n\n   going to assume you tested this, even if you left that and the \"license-binary\" box unchecked\n\n\n", "steveloughran commented on PR #8086:\nURL: https://github.com/apache/hadoop/pull/8086#issuecomment-3551946733\n\n   +please backport to branch-3.4 ASAP; no need for new pr or approval\n\n\n", "ahmarsuhail commented on PR #8086:\nURL: https://github.com/apache/hadoop/pull/8086#issuecomment-3551961218\n\n   yes, tested in eu-west-1, all good, except a couple of the AAL tests are flakey due to the audit assertions which are fixed in the https://github.com/apache/hadoop/pull/8007\r\n   \r\n   backporting now\n\n\n", "ahmarsuhail opened a new pull request, #8093:\nURL: https://github.com/apache/hadoop/pull/8093\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Upgrades AAL version to 1.3.1\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   In progress\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "ahmarsuhail commented on PR #8093:\nURL: https://github.com/apache/hadoop/pull/8093#issuecomment-3552831685\n\n   tested in eu-west-1, all good\n\n\n", "ahmarsuhail merged PR #8093:\nURL: https://github.com/apache/hadoop/pull/8093\n\n\n", "hadoop-yetus commented on PR #8093:\nURL: https://github.com/apache/hadoop/pull/8093#issuecomment-3552887841\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  19m 38s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 18s |  |  https://github.com/apache/hadoop/pull/8093 does not apply to branch-3.4. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8093/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8093 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8093/1/console |\r\n   | versions | git=2.25.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "Upgrade AAL version to 1", "qna": [{"question": "What is the issue title?", "answer": "S3A: AAL - Upgrade version to 1.3.1"}, {"question": "Who reported this issue?", "answer": "Ahmar Suhail"}]}
{"key": "HADOOP-19741", "project": "HADOOP", "title": "Support AlignmentContext parse for ProtobufRpcEngine", "status": "Open", "reporter": "Ivan Andika", "created": "2025-11-15T07:43:41.000+0000", "description": "HADOOP-17046 replaces a lot of usage of ProtobufRpcEngine to ProtobufRpcEngine2 including the AlignmentContext parsing in Server.Connection#processRpcRequest. Although it makes sense to migrate to ProtobufRpcEngine2 for clients, we should still support ProtobufRpcEngine in Server (e.g. Ozone client still uses ProtobufRpcEngine).", "comments": [], "labels": [], "summary": "HADOOP-17046 replaces a lot of usage of ProtobufRpcEngine to ProtobufRpcEngine2 ", "qna": [{"question": "What is the issue title?", "answer": "Support AlignmentContext parse for ProtobufRpcEngine"}, {"question": "Who reported this issue?", "answer": "Ivan Andika"}]}
{"key": "HADOOP-19740", "project": "HADOOP", "title": "S3A: add explicit \"sdk\" and \"ec2\" regions for region resolution through SDK and EC2 IAM", "status": "Open", "reporter": "Steve Loughran", "created": "2025-11-05T15:41:06.000+0000", "description": "Add explicit regions to hand off to the sdk\r\n* sdk: \"use the sdk chain\"\r\n* ec2: \"we are in EC2, use the local region\": use the iAM logic inside the SDK directly.\r\n\r\nempty string \"\" also hands off to the SDK; the warning will be removed\r\n\r\nalso: if an endpoint is set and it is not parsed as a vpce endpoint, we will automatically add the endpoint name \"external\". This avoids the need to make up an external region when working with an endpoint.", "comments": ["steveloughran opened a new pull request, #8058:\nURL: https://github.com/apache/hadoop/pull/8058\n\n   \r\n   \r\n   * Pull out region resolution to new class RegionResolution for isolation and testing through TestRegionResolution.\r\n   * Add new \"sdk\" and \"ec2\" mappings to hand down to SDK\r\n   * Test in ITestS3AEndpointRegion to verify that either the SDK fails to resolve with an SDK message or it does resolve to *something* and the test assert fails instead.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   New tests.\r\n   \r\n   * running regression test with configured endpoint.\r\n   * planning to run a test with a bucket set to \"sdk\" to see that the sdk handles it through my .aws/config setting.\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#issuecomment-3493993227\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 25s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/1/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) |  hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) |  hadoop-tools/hadoop-aws: The patch generated 23 new + 16 unchanged - 4 fixed = 39 total (was 20)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 17 new + 928 unchanged - 0 fixed = 945 total (was 928)  |\r\n   | -1 :x: |  javadoc  |   0m 28s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 15 new + 850 unchanged - 0 fixed = 865 total (was 850)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  26m  6s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 18s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 103m  5s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8058 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux be91d7e026c0 5.15.0-160-generic #170-Ubuntu SMP Wed Oct 1 10:06:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ec5ae6bd373324a6a975c5abdfa8afab0a507a3f |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/1/testReport/ |\r\n   | Max. process+thread count | 613 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#issuecomment-3497584976\n\n   running full test suites with java17 and the test region set to sdk. as usual, it's that forked `ITestS3ACommitterMRJob` playing up, specifically with \"null\" somehow ending up as the region/endpoint. \r\n   \r\n   \r\n   ```\r\n   [ERROR] org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob.test_200_execute ", "hadoop-yetus commented on PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#issuecomment-3499104478\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   3m 34s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 54s | [/branch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/branch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in trunk failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/branch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/branch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in trunk failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-aws.txt) |  The patch fails to run checkstyle in hadoop-aws  |\r\n   | -1 :x: |  mvnsite  |   0m 24s | [/branch-mvnsite-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/branch-mvnsite-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/branch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/branch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in trunk failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/branch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/branch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in trunk failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  spotbugs  |   0m 25s | [/branch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   2m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-mvninstall-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 23s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 23s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 15s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 15s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-aws.txt) |  The patch fails to run checkstyle in hadoop-aws  |\r\n   | -1 :x: |  mvnsite  |   0m 23s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-mvnsite-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  spotbugs  |   0m 24s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   4m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 24s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 16s |  |  ASF License check generated no output?  |\r\n   |  |   |  16m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8058 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 72c378f32934 5.15.0-160-generic #170-Ubuntu SMP Wed Oct 1 10:06:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6f662bfda35bf2ca5252fc0bd2366f7e5977cc11 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/testReport/ |\r\n   | Max. process+thread count | 79 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#issuecomment-3529470343\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 26s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/3/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) |  hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) |  hadoop-tools/hadoop-aws: The patch generated 9 new + 15 unchanged - 5 fixed = 24 total (was 20)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 31s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-aws-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 18 new + 926 unchanged - 0 fixed = 944 total (was 926)  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-aws-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 17 new + 848 unchanged - 0 fixed = 865 total (was 848)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 18s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 103m 36s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8058 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1aef8133aa53 5.15.0-160-generic #170-Ubuntu SMP Wed Oct 1 10:06:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 41e32a223648b0aa79de7c29eeff49645342a52c |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/3/testReport/ |\r\n   | Max. process+thread count | 644 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8058/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#issuecomment-3532453285\n\n   also, I think we may want to add an enum of fallbacks\r\n   ```\r\n   fs.s3a.endpoint.region.fallback =  <ec2, sdk, central>\r\n   ```\r\n   \r\n   this would define the policy for calculating the region of a bucket which isn't set explicitly, or inferred from the endpoint (vpce, aws, external).\r\n   \r\n   today's behaviour is \"central\", which doesn't work when the network is locked down in a VPC in a different region.\r\n   \r\n   if you set ec2 or sdk, then unless the region is declared (or set to those explicit region names) then the fallback will be whichever of the defaults. \r\n   \r\n   There's one more thing to consider here: should we always do an EC2/IAM probe before falling back to central?\r\n   pro: works great in EC2 with local regions\r\n   con: change in behavior when working with remote buckts.\r\n   \r\n   I think the fallback option has better compatibility.\r\n   \r\n   now, given we are doing ec2 region resolution in our own code, we have a choice of what to do when the resolution fails. so what does \r\n   endopoint.region=ec2\r\n   endpoint.fallback=ec2\r\n   \r\n   mean?\r\n   \r\n   so maybe we only need two fallbacks\r\n   central: today\r\n   sdk: do the chain of env vars, sysprops and iam info. \r\n   \r\n   \r\n   \n\n\n", "steveloughran commented on PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#issuecomment-3532465585\n\n   @mukund-thakur @ahmarsuhail thoughts on what I'm doing here? I'm coming round to\r\n   1. explict sdk and ec2 regions\r\n   1. automatically use region \"external\" if third party endpoint url of any kind\r\n   1. `fs.s3a.endpoint.region.fallback` enum, currently central and sdk\r\n   2. could have `fail` where we just fail immediately.\r\n   \r\n   \r\n   \n\n\n", "mukund-thakur commented on code in PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#discussion_r2535731487\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/DefaultS3ClientFactory.java:\n##########\n@@ -319,162 +295,63 @@ protected ClientOverrideConfiguration.Builder createClientOverrideConfiguration(\n    * <li> S3 cross region is enabled by default irrespective of region or endpoint\n    *      is set or not.</li>\n    * </ol>\n-   *\n    * @param builder S3 client builder.\n    * @param parameters parameter object\n-   * @param conf  conf configuration object\n+   * @param conf conf configuration object\n    * @param <BuilderT> S3 client builder type\n    * @param <ClientT> S3 client type\n+   * @return how the region was resolved.\n    * @throws IllegalArgumentException if endpoint is set when FIPS is enabled.\n    */\n-  private <BuilderT extends S3BaseClientBuilder<BuilderT, ClientT>, ClientT> void configureEndpointAndRegion(\n-      BuilderT builder, S3ClientCreationParameters parameters, Configuration conf) {\n-    final String endpointStr = parameters.getEndpoint();\n-    final URI endpoint = getS3Endpoint(endpointStr, conf);\n-\n-    final String configuredRegion = parameters.getRegion();\n-    Region region = null;\n-    String origin = \"\";\n-\n-    // If the region was configured, set it.\n-    if (configuredRegion != null && !configuredRegion.isEmpty()) {\n-      origin = AWS_REGION;\n-      region = Region.of(configuredRegion);\n-    }\n+  private <BuilderT extends S3BaseClientBuilder<BuilderT, ClientT>, ClientT> RegionResolution.Resolution configureEndpointAndRegion(\n+      BuilderT builder, S3ClientCreationParameters parameters, Configuration conf)  throws IOException {\n \n-    // FIPs? Log it, then reject any attempt to set an endpoint\n-    final boolean fipsEnabled = parameters.isFipsEnabled();\n-    if (fipsEnabled) {\n-      LOG.debug(\"Enabling FIPS mode\");\n-    }\n-    // always setting it guarantees the value is non-null,\n-    // which tests expect.\n-    builder.fipsEnabled(fipsEnabled);\n-\n-    if (endpoint != null) {\n-      boolean endpointEndsWithCentral =\n-          endpointStr.endsWith(CENTRAL_ENDPOINT);\n-      checkArgument(!fipsEnabled || endpointEndsWithCentral, \"%s : %s\",\n-          ERROR_ENDPOINT_WITH_FIPS,\n-          endpoint);\n-\n-      // No region was configured,\n-      // determine the region from the endpoint.\n-      if (region == null) {\n-        region = getS3RegionFromEndpoint(endpointStr,\n-            endpointEndsWithCentral);\n-        if (region != null) {\n-          origin = \"endpoint\";\n-        }\n-      }\n+    final RegionResolution.Resolution resolution =\n+        calculateRegion(parameters, conf);\n+    LOG.debug(\"Region Resolution: {}\", resolution);\n \n-      // No need to override endpoint with \"s3.amazonaws.com\".\n-      // Let the client take care of endpoint resolution. Overriding\n-      // the endpoint with \"s3.amazonaws.com\" causes 400 Bad Request\n-      // errors for non-existent buckets and objects.\n-      // ref: https://github.com/aws/aws-sdk-java-v2/issues/4846\n-      if (!endpointEndsWithCentral) {\n-        builder.endpointOverride(endpoint);\n-        LOG.debug(\"Setting endpoint to {}\", endpoint);\n-      } else {\n-        origin = \"central endpoint with cross region access\";\n-        LOG.debug(\"Enabling cross region access for endpoint {}\",\n-            endpointStr);\n-      }\n-    }\n+    // always setting to true or false guarantees the value is non-null,\n+    // which tests expect.\n+    builder.fipsEnabled(resolution.isUseFips());\n \n-    if (region != null) {\n-      builder.region(region);\n-    } else if (configuredRegion == null) {\n-      // no region is configured, and none could be determined from the endpoint.\n-      // Use US_EAST_2 as default.\n-      region = Region.of(AWS_S3_DEFAULT_REGION);\n-      builder.region(region);\n-      origin = \"cross region access fallback\";\n-    } else if (configuredRegion.isEmpty()) {\n+    final RegionResolution.RegionResolutionMechanism mechanism = resolution.getMechanism();\n+    if (Sdk == mechanism) {\n+      // handing off all resolution to SDK.\n       // region configuration was set to empty string.\n       // allow this if people really want it; it is OK to rely on this\n       // when deployed in EC2.\n-      WARN_OF_DEFAULT_REGION_CHAIN.warn(SDK_REGION_CHAIN_IN_USE);\n+      DEFAULT_REGION_CHAIN.info(SDK_REGION_CHAIN_IN_USE);\n\nReview Comment:\n   So we tell sdk to determine the region by not setting anything in the builder. \n\n\n\n", "mukund-thakur commented on code in PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#discussion_r2535742542\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RegionResolution.java:\n##########\n@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import javax.annotation.Nullable;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.awscore.util.AwsHostNameUtils;\n+import software.amazon.awssdk.regions.Region;\n+import software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.s3a.Invoker;\n+import org.apache.hadoop.fs.s3a.Retries;\n+import org.apache.hadoop.fs.s3a.S3ClientFactory;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT;\n+import static org.apache.hadoop.fs.s3a.Constants.CENTRAL_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_SECURE_CONNECTIONS;\n+import static org.apache.hadoop.fs.s3a.Constants.EC2_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.EMPTY_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.FIPS_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.SDK_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.SECURE_CONNECTIONS;\n+import static org.apache.hadoop.util.Preconditions.checkArgument;\n+import static software.amazon.awssdk.regions.Region.US_EAST_2;\n+\n+/**\n+ * Region resolution.\n+ * <p>This is complicated and can be a source of support escalations.\n+ * <p>The V1 SDK was happy to take an endpoint and\n+ * work details out from there, possibly probing us-central-1 and cacheing\n+ * the result.\n+ * <p>The V2 SDK like the signing region and endpoint to be declared.\n+ * The S3A connector has tried to mimic the V1 code, but lacks some features\n+ * (use of environment variables, probing of EC2 IAM details) for which\n+ * the SDK is better.\n+ *\n+ */\n+public class RegionResolution {\n+\n+  protected static final Logger LOG =\n+      LoggerFactory.getLogger(RegionResolution.class);\n+\n+  /**\n+   * Service to ask SDK to parse.\n+   */\n+  private static final String S3_SERVICE_NAME = \"s3\";\n+\n+  /**\n+   * Pattern to match vpce endpoints on.\n+   */\n+  private static final Pattern VPC_ENDPOINT_PATTERN =\n+      Pattern.compile(\"^(?:.+\\\\.)?([a-z0-9-]+)\\\\.vpce\\\\.amazonaws\\\\.(?:com|com\\\\.cn)$\");\n+\n+  /**\n+   * Error message when an endpoint is set with FIPS enabled: {@value}.\n+   */\n+  @VisibleForTesting\n+  public static final String ERROR_ENDPOINT_WITH_FIPS =\n+      \"Only S3 central endpoint cannot be set when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * Virtual hostnames MUST be used when using the FIPS endpoint.\n+   */\n+  public static final String FIPS_PATH_ACCESS_INCOMPATIBLE =\n+      \"Path style access must be disabled when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * String value for external region: {@value}.\n+   */\n+  public static final String EXTERNAL = \"external\";\n+\n+  /**\n+   * External region, used for third party endpoints.\n+   */\n+  public static final Region EXTERNAL_REGION = Region.of(EXTERNAL);\n+\n+  /**\n+   * How was the region resolved?\n+   */\n+  public enum RegionResolutionMechanism {\n+\n+    CalculatedFromEndpoint(\"Calculated from endpoint\"),\n+    ExternalEndpoint(\"External endpoint\"),\n+    FallbackToCentral(\"Fallback to central endpoint\"),\n+    ParseVpceEndpoint(\"Parse VPCE Endpoint\"),\n+    Ec2Metadata(\"EC2 Metadata\"),\n+    Sdk(\"SDK resolution chain\"),\n+    Specified(\"region specified\");\n+\n+    /**\n+     * Text of the mechanism.\n+     */\n+    private final String mechanism;\n+\n+    RegionResolutionMechanism(String mechanism) {\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * String value of the resolution mechanism.\n+     * @return the resolution mechanism.\n+     */\n+    public String getMechanism() {\n+      return mechanism;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"RegionResolutionMechanism{\");\n+      sb.append(\"mechanism='\").append(mechanism).append('\\'');\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * The resolution of a region and endpoint..\n+   */\n+  public static final class Resolution {\n+\n+    /**\n+     * Region: if null hand down to the SDK.\n+     */\n+    private Region region;\n+\n+    /**\n+     * How was the region resolved?\n+     * Null means unresolved.\n+     */\n+    private RegionResolutionMechanism mechanism;\n+\n+    /**\n+     * Should FIPS be enabled?\n+     */\n+    private boolean useFips;\n+\n+    /**\n+     * Should cross-region access be enabled?\n+     */\n+    private boolean crossRegionAccessEnabled;\n+\n+    /**\n+     * Endpoint as string.\n+     */\n+    private String endpointStr;\n+\n+    /**\n+     * Endpoint URI.\n+     */\n+    private URI endpointUri;\n+\n+    /**\n+     * Use the central endpoint?\n+     */\n+    private boolean useCentralEndpoint;\n+\n+    public Resolution() {\n+    }\n+\n+    /**\n+     * Instantiate with a region and resolution mechanism.\n+     * @param region region\n+     * @param mechanism resolution mechanism.\n+     */\n+    public Resolution(final Region region, final RegionResolutionMechanism mechanism) {\n+      this.region = region;\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * Set the region.\n+     * Declares the region as resolved even when the value is null (i.e. resolve to SDK).\n+     * @param region region\n+     * @param resolutionMechanism resolution mechanism.\n+     * @return the builder\n+     */\n+    public Resolution withRegion(\n+        @Nullable final Region region,\n+        final RegionResolutionMechanism resolutionMechanism) {\n+      this.region = region;\n+      this.mechanism = requireNonNull(resolutionMechanism);\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseFips(final boolean value) {\n+      useFips = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withCrossRegionAccessEnabled(final boolean value) {\n+      crossRegionAccessEnabled = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointStr(final String value) {\n+      endpointStr = value;\n+      return this;\n+    }\n+\n+    public URI getEndpointUri() {\n+      return endpointUri;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointUri(final URI value) {\n+      endpointUri = value;\n+      return this;\n+    }\n+\n+    public Region getRegion() {\n+      return region;\n+    }\n+\n+    public boolean isUseFips() {\n+      return useFips;\n+    }\n+\n+    public boolean isCrossRegionAccessEnabled() {\n+      return crossRegionAccessEnabled;\n+    }\n+\n+    public RegionResolutionMechanism getMechanism() {\n+      return mechanism;\n+    }\n+\n+    public String getEndpointStr() {\n+      return endpointStr;\n+    }\n+\n+    public boolean isRegionResolved() {\n+      return mechanism != null;\n+    }\n+\n+    public boolean isUseCentralEndpoint() {\n+      return useCentralEndpoint;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseCentralEndpoint(final boolean value) {\n+      useCentralEndpoint = value;\n+      return this;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"Resolution{\");\n+      sb.append(\"region=\").append(region);\n+      sb.append(\", resolution=\").append(mechanism);\n+      sb.append(\", useFips=\").append(useFips);\n+      sb.append(\", crossRegionAccessEnabled=\").append(crossRegionAccessEnabled);\n+      sb.append(\", endpointUri=\").append(endpointUri);\n+      sb.append(\", useCentralEndpoint=\").append(useCentralEndpoint);\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * Given a endpoint string, create the endpoint URI.\n+   * @param endpoint possibly null endpoint.\n+   * @param secureConnections use secure HTTPS connection?\n+   * @return an endpoint uri or null if the endpoint was passed in was null/empty\n+   * @throws IllegalArgumentException failure to parse the endpoint.\n+   */\n+  public static URI buildEndpointUri(String endpoint, final boolean secureConnections) {\n+\n+    String protocol = secureConnections ? \"https\" : \"http\";\n+\n+    if (endpoint == null || endpoint.isEmpty()) {\n+      // don't set an endpoint if none is configured, instead let the SDK figure it out.\n+      return null;\n+    }\n+\n+    if (!endpoint.contains(\"://\")) {\n+      endpoint = String.format(\"%s://%s\", protocol, endpoint);\n+    }\n+\n+    try {\n+      return new URI(endpoint);\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n+  }\n+\n+  /**\n+   * Parses the endpoint to get the region.\n+   * If endpoint is the central one, use US_EAST_2.\n+   * @param endpoint the configure endpoint.\n+   * @param endpointEndsWithCentral true if the endpoint is configured as central.\n+   * @return the S3 region resolution if possible from parsing the endpoint\n+   */\n+  @VisibleForTesting\n+  public static Optional<Resolution> getS3RegionFromEndpoint(\n+      final String endpoint,\n+      final boolean endpointEndsWithCentral) {\n+\n+    if (!endpointEndsWithCentral) {\n+      // S3 VPC endpoint parsing\n+      Matcher matcher = VPC_ENDPOINT_PATTERN.matcher(endpoint);\n+      if (matcher.find()) {\n+        LOG.debug(\"Mapping to VPCE\");\n+        LOG.debug(\"Endpoint {} is VPC endpoint; parsing region as {}\",\n+            endpoint, matcher.group(1));\n+        return Optional.of(new Resolution(\n+            Region.of(matcher.group(1)),\n+            RegionResolutionMechanism.ParseVpceEndpoint));\n+      }\n+\n+      LOG.debug(\"Endpoint {} is not the default; parsing\", endpoint);\n+      return AwsHostNameUtils.parseSigningRegion(endpoint, S3_SERVICE_NAME)\n+          .map(r ->\n+              new Resolution(r, RegionResolutionMechanism.CalculatedFromEndpoint));\n+    }\n+\n+    // No resolution.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Is this an AWS endpoint, that is: has an endpoint been set which matches\n+   * amazon.\n+   * @param endpoint non-null endpoint URL\n+   * @return true if this is amazonaws or amazonaws china\n+   */\n+  public static boolean isAwsEndpoint(final String endpoint) {\n+    final String h = endpoint.toLowerCase(Locale.ROOT);\n+    // Common AWS partitions: global (.amazonaws.com) and China (.amazonaws.com.cn).\n+    return h.endsWith(\".amazonaws.com\")\n+        || h.endsWith(\".amazonaws.com.cn\");\n+  }\n+\n+\n+  /**\n+   * Does the region name refer to an SDK region?\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isSdkRegion(String configuredRegion) {\n+    return SDK_REGION.equalsIgnoreCase(configuredRegion)\n+        || EMPTY_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Does the region name refer to {@code \"ec2\"} in which case special handling\n+   * is required.\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isEc2Region(String configuredRegion) {\n+    return EC2_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Calculate the region and the final endpoint.\n+   * @param parameters creation parameters\n+   * @param conf configuration with other options.\n+   * @return the resolved region and endpoint.\n+   * @throws IOException if the client failed to communicate with the IAM service.\n+   * @throws IllegalArgumentException failure to parse endpoint, or FIPS settings.\n+   */\n+  @Retries.OnceTranslated\n+  public static Resolution calculateRegion(\n+      final S3ClientFactory.S3ClientCreationParameters parameters,\n+      final Configuration conf) throws IOException {\n+\n+    Resolution resolution = new Resolution();\n+\n+    // endpoint; may be null\n+    final String endpointStr = parameters.getEndpoint();\n+    boolean endpointDeclared = endpointStr != null && !endpointStr.isEmpty();\n+    // will be null if endpointStr is null/empty\n+    final URI endpoint = buildEndpointUri(endpointStr,\n+        conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS));\n+\n+    final String configuredRegion = parameters.getRegion();\n+\n+    // If the region was configured, set it.\n+    // this includes special handling of the sdk, ec2 and \"\" regions.\n+    if (configuredRegion != null) {\n\nReview Comment:\n   Where is the not configured else part?\n\n\n\n", "mukund-thakur commented on code in PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#discussion_r2535745484\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RegionResolution.java:\n##########\n@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import javax.annotation.Nullable;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.awscore.util.AwsHostNameUtils;\n+import software.amazon.awssdk.regions.Region;\n+import software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.s3a.Invoker;\n+import org.apache.hadoop.fs.s3a.Retries;\n+import org.apache.hadoop.fs.s3a.S3ClientFactory;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT;\n+import static org.apache.hadoop.fs.s3a.Constants.CENTRAL_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_SECURE_CONNECTIONS;\n+import static org.apache.hadoop.fs.s3a.Constants.EC2_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.EMPTY_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.FIPS_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.SDK_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.SECURE_CONNECTIONS;\n+import static org.apache.hadoop.util.Preconditions.checkArgument;\n+import static software.amazon.awssdk.regions.Region.US_EAST_2;\n+\n+/**\n+ * Region resolution.\n+ * <p>This is complicated and can be a source of support escalations.\n+ * <p>The V1 SDK was happy to take an endpoint and\n+ * work details out from there, possibly probing us-central-1 and cacheing\n+ * the result.\n+ * <p>The V2 SDK like the signing region and endpoint to be declared.\n+ * The S3A connector has tried to mimic the V1 code, but lacks some features\n+ * (use of environment variables, probing of EC2 IAM details) for which\n+ * the SDK is better.\n+ *\n+ */\n+public class RegionResolution {\n+\n+  protected static final Logger LOG =\n+      LoggerFactory.getLogger(RegionResolution.class);\n+\n+  /**\n+   * Service to ask SDK to parse.\n+   */\n+  private static final String S3_SERVICE_NAME = \"s3\";\n+\n+  /**\n+   * Pattern to match vpce endpoints on.\n+   */\n+  private static final Pattern VPC_ENDPOINT_PATTERN =\n+      Pattern.compile(\"^(?:.+\\\\.)?([a-z0-9-]+)\\\\.vpce\\\\.amazonaws\\\\.(?:com|com\\\\.cn)$\");\n+\n+  /**\n+   * Error message when an endpoint is set with FIPS enabled: {@value}.\n+   */\n+  @VisibleForTesting\n+  public static final String ERROR_ENDPOINT_WITH_FIPS =\n+      \"Only S3 central endpoint cannot be set when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * Virtual hostnames MUST be used when using the FIPS endpoint.\n+   */\n+  public static final String FIPS_PATH_ACCESS_INCOMPATIBLE =\n+      \"Path style access must be disabled when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * String value for external region: {@value}.\n+   */\n+  public static final String EXTERNAL = \"external\";\n+\n+  /**\n+   * External region, used for third party endpoints.\n+   */\n+  public static final Region EXTERNAL_REGION = Region.of(EXTERNAL);\n+\n+  /**\n+   * How was the region resolved?\n+   */\n+  public enum RegionResolutionMechanism {\n+\n+    CalculatedFromEndpoint(\"Calculated from endpoint\"),\n+    ExternalEndpoint(\"External endpoint\"),\n+    FallbackToCentral(\"Fallback to central endpoint\"),\n+    ParseVpceEndpoint(\"Parse VPCE Endpoint\"),\n+    Ec2Metadata(\"EC2 Metadata\"),\n+    Sdk(\"SDK resolution chain\"),\n+    Specified(\"region specified\");\n+\n+    /**\n+     * Text of the mechanism.\n+     */\n+    private final String mechanism;\n+\n+    RegionResolutionMechanism(String mechanism) {\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * String value of the resolution mechanism.\n+     * @return the resolution mechanism.\n+     */\n+    public String getMechanism() {\n+      return mechanism;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"RegionResolutionMechanism{\");\n+      sb.append(\"mechanism='\").append(mechanism).append('\\'');\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * The resolution of a region and endpoint..\n+   */\n+  public static final class Resolution {\n+\n+    /**\n+     * Region: if null hand down to the SDK.\n+     */\n+    private Region region;\n+\n+    /**\n+     * How was the region resolved?\n+     * Null means unresolved.\n+     */\n+    private RegionResolutionMechanism mechanism;\n+\n+    /**\n+     * Should FIPS be enabled?\n+     */\n+    private boolean useFips;\n+\n+    /**\n+     * Should cross-region access be enabled?\n+     */\n+    private boolean crossRegionAccessEnabled;\n+\n+    /**\n+     * Endpoint as string.\n+     */\n+    private String endpointStr;\n+\n+    /**\n+     * Endpoint URI.\n+     */\n+    private URI endpointUri;\n+\n+    /**\n+     * Use the central endpoint?\n+     */\n+    private boolean useCentralEndpoint;\n+\n+    public Resolution() {\n+    }\n+\n+    /**\n+     * Instantiate with a region and resolution mechanism.\n+     * @param region region\n+     * @param mechanism resolution mechanism.\n+     */\n+    public Resolution(final Region region, final RegionResolutionMechanism mechanism) {\n+      this.region = region;\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * Set the region.\n+     * Declares the region as resolved even when the value is null (i.e. resolve to SDK).\n+     * @param region region\n+     * @param resolutionMechanism resolution mechanism.\n+     * @return the builder\n+     */\n+    public Resolution withRegion(\n+        @Nullable final Region region,\n+        final RegionResolutionMechanism resolutionMechanism) {\n+      this.region = region;\n+      this.mechanism = requireNonNull(resolutionMechanism);\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseFips(final boolean value) {\n+      useFips = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withCrossRegionAccessEnabled(final boolean value) {\n+      crossRegionAccessEnabled = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointStr(final String value) {\n+      endpointStr = value;\n+      return this;\n+    }\n+\n+    public URI getEndpointUri() {\n+      return endpointUri;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointUri(final URI value) {\n+      endpointUri = value;\n+      return this;\n+    }\n+\n+    public Region getRegion() {\n+      return region;\n+    }\n+\n+    public boolean isUseFips() {\n+      return useFips;\n+    }\n+\n+    public boolean isCrossRegionAccessEnabled() {\n+      return crossRegionAccessEnabled;\n+    }\n+\n+    public RegionResolutionMechanism getMechanism() {\n+      return mechanism;\n+    }\n+\n+    public String getEndpointStr() {\n+      return endpointStr;\n+    }\n+\n+    public boolean isRegionResolved() {\n+      return mechanism != null;\n+    }\n+\n+    public boolean isUseCentralEndpoint() {\n+      return useCentralEndpoint;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseCentralEndpoint(final boolean value) {\n+      useCentralEndpoint = value;\n+      return this;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"Resolution{\");\n+      sb.append(\"region=\").append(region);\n+      sb.append(\", resolution=\").append(mechanism);\n+      sb.append(\", useFips=\").append(useFips);\n+      sb.append(\", crossRegionAccessEnabled=\").append(crossRegionAccessEnabled);\n+      sb.append(\", endpointUri=\").append(endpointUri);\n+      sb.append(\", useCentralEndpoint=\").append(useCentralEndpoint);\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * Given a endpoint string, create the endpoint URI.\n+   * @param endpoint possibly null endpoint.\n+   * @param secureConnections use secure HTTPS connection?\n+   * @return an endpoint uri or null if the endpoint was passed in was null/empty\n+   * @throws IllegalArgumentException failure to parse the endpoint.\n+   */\n+  public static URI buildEndpointUri(String endpoint, final boolean secureConnections) {\n+\n+    String protocol = secureConnections ? \"https\" : \"http\";\n+\n+    if (endpoint == null || endpoint.isEmpty()) {\n+      // don't set an endpoint if none is configured, instead let the SDK figure it out.\n+      return null;\n+    }\n+\n+    if (!endpoint.contains(\"://\")) {\n+      endpoint = String.format(\"%s://%s\", protocol, endpoint);\n+    }\n+\n+    try {\n+      return new URI(endpoint);\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n+  }\n+\n+  /**\n+   * Parses the endpoint to get the region.\n+   * If endpoint is the central one, use US_EAST_2.\n+   * @param endpoint the configure endpoint.\n+   * @param endpointEndsWithCentral true if the endpoint is configured as central.\n+   * @return the S3 region resolution if possible from parsing the endpoint\n+   */\n+  @VisibleForTesting\n+  public static Optional<Resolution> getS3RegionFromEndpoint(\n+      final String endpoint,\n+      final boolean endpointEndsWithCentral) {\n+\n+    if (!endpointEndsWithCentral) {\n+      // S3 VPC endpoint parsing\n+      Matcher matcher = VPC_ENDPOINT_PATTERN.matcher(endpoint);\n+      if (matcher.find()) {\n+        LOG.debug(\"Mapping to VPCE\");\n+        LOG.debug(\"Endpoint {} is VPC endpoint; parsing region as {}\",\n+            endpoint, matcher.group(1));\n+        return Optional.of(new Resolution(\n+            Region.of(matcher.group(1)),\n+            RegionResolutionMechanism.ParseVpceEndpoint));\n+      }\n+\n+      LOG.debug(\"Endpoint {} is not the default; parsing\", endpoint);\n+      return AwsHostNameUtils.parseSigningRegion(endpoint, S3_SERVICE_NAME)\n+          .map(r ->\n+              new Resolution(r, RegionResolutionMechanism.CalculatedFromEndpoint));\n+    }\n+\n+    // No resolution.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Is this an AWS endpoint, that is: has an endpoint been set which matches\n+   * amazon.\n+   * @param endpoint non-null endpoint URL\n+   * @return true if this is amazonaws or amazonaws china\n+   */\n+  public static boolean isAwsEndpoint(final String endpoint) {\n+    final String h = endpoint.toLowerCase(Locale.ROOT);\n+    // Common AWS partitions: global (.amazonaws.com) and China (.amazonaws.com.cn).\n+    return h.endsWith(\".amazonaws.com\")\n+        || h.endsWith(\".amazonaws.com.cn\");\n+  }\n+\n+\n+  /**\n+   * Does the region name refer to an SDK region?\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isSdkRegion(String configuredRegion) {\n+    return SDK_REGION.equalsIgnoreCase(configuredRegion)\n+        || EMPTY_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Does the region name refer to {@code \"ec2\"} in which case special handling\n+   * is required.\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isEc2Region(String configuredRegion) {\n+    return EC2_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Calculate the region and the final endpoint.\n+   * @param parameters creation parameters\n+   * @param conf configuration with other options.\n+   * @return the resolved region and endpoint.\n+   * @throws IOException if the client failed to communicate with the IAM service.\n+   * @throws IllegalArgumentException failure to parse endpoint, or FIPS settings.\n+   */\n+  @Retries.OnceTranslated\n+  public static Resolution calculateRegion(\n+      final S3ClientFactory.S3ClientCreationParameters parameters,\n+      final Configuration conf) throws IOException {\n+\n+    Resolution resolution = new Resolution();\n+\n+    // endpoint; may be null\n+    final String endpointStr = parameters.getEndpoint();\n+    boolean endpointDeclared = endpointStr != null && !endpointStr.isEmpty();\n+    // will be null if endpointStr is null/empty\n+    final URI endpoint = buildEndpointUri(endpointStr,\n+        conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS));\n+\n+    final String configuredRegion = parameters.getRegion();\n+\n+    // If the region was configured, set it.\n+    // this includes special handling of the sdk, ec2 and \"\" regions.\n+    if (configuredRegion != null) {\n\nReview Comment:\n   Okay from line 451 I guess. \r\n   If fs.s3a.endpoint.region is not set we fall back to fs.s3a.endpoint.. old stuff. \n\n\n\n", "mukund-thakur commented on code in PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#discussion_r2535749021\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RegionResolution.java:\n##########\n@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import javax.annotation.Nullable;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.awscore.util.AwsHostNameUtils;\n+import software.amazon.awssdk.regions.Region;\n+import software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.s3a.Invoker;\n+import org.apache.hadoop.fs.s3a.Retries;\n+import org.apache.hadoop.fs.s3a.S3ClientFactory;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT;\n+import static org.apache.hadoop.fs.s3a.Constants.CENTRAL_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_SECURE_CONNECTIONS;\n+import static org.apache.hadoop.fs.s3a.Constants.EC2_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.EMPTY_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.FIPS_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.SDK_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.SECURE_CONNECTIONS;\n+import static org.apache.hadoop.util.Preconditions.checkArgument;\n+import static software.amazon.awssdk.regions.Region.US_EAST_2;\n+\n+/**\n+ * Region resolution.\n+ * <p>This is complicated and can be a source of support escalations.\n+ * <p>The V1 SDK was happy to take an endpoint and\n+ * work details out from there, possibly probing us-central-1 and cacheing\n+ * the result.\n+ * <p>The V2 SDK like the signing region and endpoint to be declared.\n+ * The S3A connector has tried to mimic the V1 code, but lacks some features\n+ * (use of environment variables, probing of EC2 IAM details) for which\n+ * the SDK is better.\n+ *\n+ */\n+public class RegionResolution {\n+\n+  protected static final Logger LOG =\n+      LoggerFactory.getLogger(RegionResolution.class);\n+\n+  /**\n+   * Service to ask SDK to parse.\n+   */\n+  private static final String S3_SERVICE_NAME = \"s3\";\n+\n+  /**\n+   * Pattern to match vpce endpoints on.\n+   */\n+  private static final Pattern VPC_ENDPOINT_PATTERN =\n+      Pattern.compile(\"^(?:.+\\\\.)?([a-z0-9-]+)\\\\.vpce\\\\.amazonaws\\\\.(?:com|com\\\\.cn)$\");\n+\n+  /**\n+   * Error message when an endpoint is set with FIPS enabled: {@value}.\n+   */\n+  @VisibleForTesting\n+  public static final String ERROR_ENDPOINT_WITH_FIPS =\n+      \"Only S3 central endpoint cannot be set when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * Virtual hostnames MUST be used when using the FIPS endpoint.\n+   */\n+  public static final String FIPS_PATH_ACCESS_INCOMPATIBLE =\n+      \"Path style access must be disabled when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * String value for external region: {@value}.\n+   */\n+  public static final String EXTERNAL = \"external\";\n+\n+  /**\n+   * External region, used for third party endpoints.\n+   */\n+  public static final Region EXTERNAL_REGION = Region.of(EXTERNAL);\n+\n+  /**\n+   * How was the region resolved?\n+   */\n+  public enum RegionResolutionMechanism {\n+\n+    CalculatedFromEndpoint(\"Calculated from endpoint\"),\n+    ExternalEndpoint(\"External endpoint\"),\n+    FallbackToCentral(\"Fallback to central endpoint\"),\n+    ParseVpceEndpoint(\"Parse VPCE Endpoint\"),\n+    Ec2Metadata(\"EC2 Metadata\"),\n+    Sdk(\"SDK resolution chain\"),\n+    Specified(\"region specified\");\n+\n+    /**\n+     * Text of the mechanism.\n+     */\n+    private final String mechanism;\n+\n+    RegionResolutionMechanism(String mechanism) {\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * String value of the resolution mechanism.\n+     * @return the resolution mechanism.\n+     */\n+    public String getMechanism() {\n+      return mechanism;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"RegionResolutionMechanism{\");\n+      sb.append(\"mechanism='\").append(mechanism).append('\\'');\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * The resolution of a region and endpoint..\n+   */\n+  public static final class Resolution {\n+\n+    /**\n+     * Region: if null hand down to the SDK.\n+     */\n+    private Region region;\n+\n+    /**\n+     * How was the region resolved?\n+     * Null means unresolved.\n+     */\n+    private RegionResolutionMechanism mechanism;\n+\n+    /**\n+     * Should FIPS be enabled?\n+     */\n+    private boolean useFips;\n+\n+    /**\n+     * Should cross-region access be enabled?\n+     */\n+    private boolean crossRegionAccessEnabled;\n+\n+    /**\n+     * Endpoint as string.\n+     */\n+    private String endpointStr;\n+\n+    /**\n+     * Endpoint URI.\n+     */\n+    private URI endpointUri;\n+\n+    /**\n+     * Use the central endpoint?\n+     */\n+    private boolean useCentralEndpoint;\n+\n+    public Resolution() {\n+    }\n+\n+    /**\n+     * Instantiate with a region and resolution mechanism.\n+     * @param region region\n+     * @param mechanism resolution mechanism.\n+     */\n+    public Resolution(final Region region, final RegionResolutionMechanism mechanism) {\n+      this.region = region;\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * Set the region.\n+     * Declares the region as resolved even when the value is null (i.e. resolve to SDK).\n+     * @param region region\n+     * @param resolutionMechanism resolution mechanism.\n+     * @return the builder\n+     */\n+    public Resolution withRegion(\n+        @Nullable final Region region,\n+        final RegionResolutionMechanism resolutionMechanism) {\n+      this.region = region;\n+      this.mechanism = requireNonNull(resolutionMechanism);\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseFips(final boolean value) {\n+      useFips = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withCrossRegionAccessEnabled(final boolean value) {\n+      crossRegionAccessEnabled = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointStr(final String value) {\n+      endpointStr = value;\n+      return this;\n+    }\n+\n+    public URI getEndpointUri() {\n+      return endpointUri;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointUri(final URI value) {\n+      endpointUri = value;\n+      return this;\n+    }\n+\n+    public Region getRegion() {\n+      return region;\n+    }\n+\n+    public boolean isUseFips() {\n+      return useFips;\n+    }\n+\n+    public boolean isCrossRegionAccessEnabled() {\n+      return crossRegionAccessEnabled;\n+    }\n+\n+    public RegionResolutionMechanism getMechanism() {\n+      return mechanism;\n+    }\n+\n+    public String getEndpointStr() {\n+      return endpointStr;\n+    }\n+\n+    public boolean isRegionResolved() {\n+      return mechanism != null;\n+    }\n+\n+    public boolean isUseCentralEndpoint() {\n+      return useCentralEndpoint;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseCentralEndpoint(final boolean value) {\n+      useCentralEndpoint = value;\n+      return this;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"Resolution{\");\n+      sb.append(\"region=\").append(region);\n+      sb.append(\", resolution=\").append(mechanism);\n+      sb.append(\", useFips=\").append(useFips);\n+      sb.append(\", crossRegionAccessEnabled=\").append(crossRegionAccessEnabled);\n+      sb.append(\", endpointUri=\").append(endpointUri);\n+      sb.append(\", useCentralEndpoint=\").append(useCentralEndpoint);\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * Given a endpoint string, create the endpoint URI.\n+   * @param endpoint possibly null endpoint.\n+   * @param secureConnections use secure HTTPS connection?\n+   * @return an endpoint uri or null if the endpoint was passed in was null/empty\n+   * @throws IllegalArgumentException failure to parse the endpoint.\n+   */\n+  public static URI buildEndpointUri(String endpoint, final boolean secureConnections) {\n+\n+    String protocol = secureConnections ? \"https\" : \"http\";\n+\n+    if (endpoint == null || endpoint.isEmpty()) {\n+      // don't set an endpoint if none is configured, instead let the SDK figure it out.\n+      return null;\n+    }\n+\n+    if (!endpoint.contains(\"://\")) {\n+      endpoint = String.format(\"%s://%s\", protocol, endpoint);\n+    }\n+\n+    try {\n+      return new URI(endpoint);\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n+  }\n+\n+  /**\n+   * Parses the endpoint to get the region.\n+   * If endpoint is the central one, use US_EAST_2.\n+   * @param endpoint the configure endpoint.\n+   * @param endpointEndsWithCentral true if the endpoint is configured as central.\n+   * @return the S3 region resolution if possible from parsing the endpoint\n+   */\n+  @VisibleForTesting\n+  public static Optional<Resolution> getS3RegionFromEndpoint(\n+      final String endpoint,\n+      final boolean endpointEndsWithCentral) {\n+\n+    if (!endpointEndsWithCentral) {\n+      // S3 VPC endpoint parsing\n+      Matcher matcher = VPC_ENDPOINT_PATTERN.matcher(endpoint);\n+      if (matcher.find()) {\n+        LOG.debug(\"Mapping to VPCE\");\n+        LOG.debug(\"Endpoint {} is VPC endpoint; parsing region as {}\",\n+            endpoint, matcher.group(1));\n+        return Optional.of(new Resolution(\n+            Region.of(matcher.group(1)),\n+            RegionResolutionMechanism.ParseVpceEndpoint));\n+      }\n+\n+      LOG.debug(\"Endpoint {} is not the default; parsing\", endpoint);\n+      return AwsHostNameUtils.parseSigningRegion(endpoint, S3_SERVICE_NAME)\n+          .map(r ->\n+              new Resolution(r, RegionResolutionMechanism.CalculatedFromEndpoint));\n+    }\n+\n+    // No resolution.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Is this an AWS endpoint, that is: has an endpoint been set which matches\n+   * amazon.\n+   * @param endpoint non-null endpoint URL\n+   * @return true if this is amazonaws or amazonaws china\n+   */\n+  public static boolean isAwsEndpoint(final String endpoint) {\n\nReview Comment:\n   can there be other cases. How do we test this? \n\n\n\n", "mukund-thakur commented on code in PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#discussion_r2535754279\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RegionResolution.java:\n##########\n@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import javax.annotation.Nullable;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.awscore.util.AwsHostNameUtils;\n+import software.amazon.awssdk.regions.Region;\n+import software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.s3a.Invoker;\n+import org.apache.hadoop.fs.s3a.Retries;\n+import org.apache.hadoop.fs.s3a.S3ClientFactory;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT;\n+import static org.apache.hadoop.fs.s3a.Constants.CENTRAL_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_SECURE_CONNECTIONS;\n+import static org.apache.hadoop.fs.s3a.Constants.EC2_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.EMPTY_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.FIPS_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.SDK_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.SECURE_CONNECTIONS;\n+import static org.apache.hadoop.util.Preconditions.checkArgument;\n+import static software.amazon.awssdk.regions.Region.US_EAST_2;\n+\n+/**\n+ * Region resolution.\n+ * <p>This is complicated and can be a source of support escalations.\n+ * <p>The V1 SDK was happy to take an endpoint and\n+ * work details out from there, possibly probing us-central-1 and cacheing\n+ * the result.\n+ * <p>The V2 SDK like the signing region and endpoint to be declared.\n+ * The S3A connector has tried to mimic the V1 code, but lacks some features\n+ * (use of environment variables, probing of EC2 IAM details) for which\n+ * the SDK is better.\n+ *\n+ */\n+public class RegionResolution {\n+\n+  protected static final Logger LOG =\n+      LoggerFactory.getLogger(RegionResolution.class);\n+\n+  /**\n+   * Service to ask SDK to parse.\n+   */\n+  private static final String S3_SERVICE_NAME = \"s3\";\n+\n+  /**\n+   * Pattern to match vpce endpoints on.\n+   */\n+  private static final Pattern VPC_ENDPOINT_PATTERN =\n+      Pattern.compile(\"^(?:.+\\\\.)?([a-z0-9-]+)\\\\.vpce\\\\.amazonaws\\\\.(?:com|com\\\\.cn)$\");\n+\n+  /**\n+   * Error message when an endpoint is set with FIPS enabled: {@value}.\n+   */\n+  @VisibleForTesting\n+  public static final String ERROR_ENDPOINT_WITH_FIPS =\n+      \"Only S3 central endpoint cannot be set when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * Virtual hostnames MUST be used when using the FIPS endpoint.\n+   */\n+  public static final String FIPS_PATH_ACCESS_INCOMPATIBLE =\n+      \"Path style access must be disabled when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * String value for external region: {@value}.\n+   */\n+  public static final String EXTERNAL = \"external\";\n+\n+  /**\n+   * External region, used for third party endpoints.\n+   */\n+  public static final Region EXTERNAL_REGION = Region.of(EXTERNAL);\n+\n+  /**\n+   * How was the region resolved?\n+   */\n+  public enum RegionResolutionMechanism {\n+\n+    CalculatedFromEndpoint(\"Calculated from endpoint\"),\n+    ExternalEndpoint(\"External endpoint\"),\n+    FallbackToCentral(\"Fallback to central endpoint\"),\n+    ParseVpceEndpoint(\"Parse VPCE Endpoint\"),\n+    Ec2Metadata(\"EC2 Metadata\"),\n+    Sdk(\"SDK resolution chain\"),\n+    Specified(\"region specified\");\n+\n+    /**\n+     * Text of the mechanism.\n+     */\n+    private final String mechanism;\n+\n+    RegionResolutionMechanism(String mechanism) {\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * String value of the resolution mechanism.\n+     * @return the resolution mechanism.\n+     */\n+    public String getMechanism() {\n+      return mechanism;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"RegionResolutionMechanism{\");\n+      sb.append(\"mechanism='\").append(mechanism).append('\\'');\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * The resolution of a region and endpoint..\n+   */\n+  public static final class Resolution {\n+\n+    /**\n+     * Region: if null hand down to the SDK.\n+     */\n+    private Region region;\n+\n+    /**\n+     * How was the region resolved?\n+     * Null means unresolved.\n+     */\n+    private RegionResolutionMechanism mechanism;\n+\n+    /**\n+     * Should FIPS be enabled?\n+     */\n+    private boolean useFips;\n+\n+    /**\n+     * Should cross-region access be enabled?\n+     */\n+    private boolean crossRegionAccessEnabled;\n+\n+    /**\n+     * Endpoint as string.\n+     */\n+    private String endpointStr;\n+\n+    /**\n+     * Endpoint URI.\n+     */\n+    private URI endpointUri;\n+\n+    /**\n+     * Use the central endpoint?\n+     */\n+    private boolean useCentralEndpoint;\n+\n+    public Resolution() {\n+    }\n+\n+    /**\n+     * Instantiate with a region and resolution mechanism.\n+     * @param region region\n+     * @param mechanism resolution mechanism.\n+     */\n+    public Resolution(final Region region, final RegionResolutionMechanism mechanism) {\n+      this.region = region;\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * Set the region.\n+     * Declares the region as resolved even when the value is null (i.e. resolve to SDK).\n+     * @param region region\n+     * @param resolutionMechanism resolution mechanism.\n+     * @return the builder\n+     */\n+    public Resolution withRegion(\n+        @Nullable final Region region,\n+        final RegionResolutionMechanism resolutionMechanism) {\n+      this.region = region;\n+      this.mechanism = requireNonNull(resolutionMechanism);\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseFips(final boolean value) {\n+      useFips = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withCrossRegionAccessEnabled(final boolean value) {\n+      crossRegionAccessEnabled = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointStr(final String value) {\n+      endpointStr = value;\n+      return this;\n+    }\n+\n+    public URI getEndpointUri() {\n+      return endpointUri;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointUri(final URI value) {\n+      endpointUri = value;\n+      return this;\n+    }\n+\n+    public Region getRegion() {\n+      return region;\n+    }\n+\n+    public boolean isUseFips() {\n+      return useFips;\n+    }\n+\n+    public boolean isCrossRegionAccessEnabled() {\n+      return crossRegionAccessEnabled;\n+    }\n+\n+    public RegionResolutionMechanism getMechanism() {\n+      return mechanism;\n+    }\n+\n+    public String getEndpointStr() {\n+      return endpointStr;\n+    }\n+\n+    public boolean isRegionResolved() {\n+      return mechanism != null;\n+    }\n+\n+    public boolean isUseCentralEndpoint() {\n+      return useCentralEndpoint;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseCentralEndpoint(final boolean value) {\n+      useCentralEndpoint = value;\n+      return this;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"Resolution{\");\n+      sb.append(\"region=\").append(region);\n+      sb.append(\", resolution=\").append(mechanism);\n+      sb.append(\", useFips=\").append(useFips);\n+      sb.append(\", crossRegionAccessEnabled=\").append(crossRegionAccessEnabled);\n+      sb.append(\", endpointUri=\").append(endpointUri);\n+      sb.append(\", useCentralEndpoint=\").append(useCentralEndpoint);\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * Given a endpoint string, create the endpoint URI.\n+   * @param endpoint possibly null endpoint.\n+   * @param secureConnections use secure HTTPS connection?\n+   * @return an endpoint uri or null if the endpoint was passed in was null/empty\n+   * @throws IllegalArgumentException failure to parse the endpoint.\n+   */\n+  public static URI buildEndpointUri(String endpoint, final boolean secureConnections) {\n+\n+    String protocol = secureConnections ? \"https\" : \"http\";\n+\n+    if (endpoint == null || endpoint.isEmpty()) {\n+      // don't set an endpoint if none is configured, instead let the SDK figure it out.\n+      return null;\n+    }\n+\n+    if (!endpoint.contains(\"://\")) {\n+      endpoint = String.format(\"%s://%s\", protocol, endpoint);\n+    }\n+\n+    try {\n+      return new URI(endpoint);\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n+  }\n+\n+  /**\n+   * Parses the endpoint to get the region.\n+   * If endpoint is the central one, use US_EAST_2.\n+   * @param endpoint the configure endpoint.\n+   * @param endpointEndsWithCentral true if the endpoint is configured as central.\n+   * @return the S3 region resolution if possible from parsing the endpoint\n+   */\n+  @VisibleForTesting\n+  public static Optional<Resolution> getS3RegionFromEndpoint(\n+      final String endpoint,\n+      final boolean endpointEndsWithCentral) {\n+\n+    if (!endpointEndsWithCentral) {\n+      // S3 VPC endpoint parsing\n+      Matcher matcher = VPC_ENDPOINT_PATTERN.matcher(endpoint);\n+      if (matcher.find()) {\n+        LOG.debug(\"Mapping to VPCE\");\n+        LOG.debug(\"Endpoint {} is VPC endpoint; parsing region as {}\",\n+            endpoint, matcher.group(1));\n+        return Optional.of(new Resolution(\n+            Region.of(matcher.group(1)),\n+            RegionResolutionMechanism.ParseVpceEndpoint));\n+      }\n+\n+      LOG.debug(\"Endpoint {} is not the default; parsing\", endpoint);\n+      return AwsHostNameUtils.parseSigningRegion(endpoint, S3_SERVICE_NAME)\n+          .map(r ->\n+              new Resolution(r, RegionResolutionMechanism.CalculatedFromEndpoint));\n+    }\n+\n+    // No resolution.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Is this an AWS endpoint, that is: has an endpoint been set which matches\n+   * amazon.\n+   * @param endpoint non-null endpoint URL\n+   * @return true if this is amazonaws or amazonaws china\n+   */\n+  public static boolean isAwsEndpoint(final String endpoint) {\n+    final String h = endpoint.toLowerCase(Locale.ROOT);\n+    // Common AWS partitions: global (.amazonaws.com) and China (.amazonaws.com.cn).\n+    return h.endsWith(\".amazonaws.com\")\n+        || h.endsWith(\".amazonaws.com.cn\");\n+  }\n+\n+\n+  /**\n+   * Does the region name refer to an SDK region?\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isSdkRegion(String configuredRegion) {\n+    return SDK_REGION.equalsIgnoreCase(configuredRegion)\n+        || EMPTY_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Does the region name refer to {@code \"ec2\"} in which case special handling\n+   * is required.\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isEc2Region(String configuredRegion) {\n+    return EC2_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Calculate the region and the final endpoint.\n+   * @param parameters creation parameters\n+   * @param conf configuration with other options.\n+   * @return the resolved region and endpoint.\n+   * @throws IOException if the client failed to communicate with the IAM service.\n+   * @throws IllegalArgumentException failure to parse endpoint, or FIPS settings.\n+   */\n+  @Retries.OnceTranslated\n+  public static Resolution calculateRegion(\n+      final S3ClientFactory.S3ClientCreationParameters parameters,\n+      final Configuration conf) throws IOException {\n+\n+    Resolution resolution = new Resolution();\n+\n+    // endpoint; may be null\n+    final String endpointStr = parameters.getEndpoint();\n+    boolean endpointDeclared = endpointStr != null && !endpointStr.isEmpty();\n+    // will be null if endpointStr is null/empty\n+    final URI endpoint = buildEndpointUri(endpointStr,\n+        conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS));\n+\n+    final String configuredRegion = parameters.getRegion();\n+\n+    // If the region was configured, set it.\n+    // this includes special handling of the sdk, ec2 and \"\" regions.\n+    if (configuredRegion != null) {\n+      checkArgument(!\"null\".equals(configuredRegion),\n+          \"null is region name\");\n+      if (isSdkRegion(configuredRegion)) {\n+        resolution.withRegion(null, RegionResolutionMechanism.Sdk);\n+      } else if (isEc2Region(configuredRegion)) {\n+        // special EC2 handling\n+        final Resolution r = getS3RegionFromEc2IAM();\n+        resolution.withRegion(r.getRegion(), r.getMechanism());\n+      } else {\n+        resolution.withRegion(Region.of(configuredRegion),\n+            RegionResolutionMechanism.Specified);\n+      }\n+    }\n+\n+    // central endpoint if no endpoint has been set, or it is explicitly\n+    // requested\n+    boolean endpointEndsWithCentral = !endpointDeclared\n+        || endpointStr.endsWith(CENTRAL_ENDPOINT);\n+\n+    if (!resolution.isRegionResolved()) {\n+      // parse from the endpoint and set if calculated\n+      LOG.debug(\"Falling back to parsing region endpoint {}; endpointEndsWithCentral={}\",\n+          endpointStr, endpointEndsWithCentral);\n+      final Optional<Resolution> regionFromEndpoint =\n+          getS3RegionFromEndpoint(endpointStr, endpointEndsWithCentral);\n+      if (regionFromEndpoint.isPresent()) {\n+        regionFromEndpoint\n+            .map(r ->\n+                resolution.withRegion(r.getRegion(), r.getMechanism()));\n+      }\n+    }\n+\n+    // cross region setting.\n+    resolution.withCrossRegionAccessEnabled(\n+        conf.getBoolean(AWS_S3_CROSS_REGION_ACCESS_ENABLED,\n+            AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT));\n+\n+    // fips settings.\n+    final boolean fipsEnabled = parameters.isFipsEnabled();\n+    resolution.withUseFips(fipsEnabled);\n+    if (fipsEnabled) {\n+      // validate the FIPS settings\n+      checkArgument(endpoint == null || endpointEndsWithCentral,\n+          \"%s : %s\", ERROR_ENDPOINT_WITH_FIPS, endpoint);\n+      checkArgument(!parameters.isPathStyleAccess(),\n+          FIPS_PATH_ACCESS_INCOMPATIBLE);\n+    }\n+\n+\n+    if (!resolution.isRegionResolved()) {\n+      // still not resolved.\n+      if (!endpointDeclared || isAwsEndpoint(endpointStr)) {\n+        // still failing to resolve the region\n+        // fall back to central\n+        resolution.withRegion(US_EAST_2, RegionResolutionMechanism.FallbackToCentral);\n+      } else {\n+        // we are not resolved and not an aws region.\n+        // set the region to being \"external\"\n+        resolution.withRegion(EXTERNAL_REGION, RegionResolutionMechanism.ExternalEndpoint);\n+      }\n+    }\n+\n+    // No need to override endpoint with \"s3.amazonaws.com\".\n+    // Let the client take care of endpoint resolution. Overriding\n+    // the endpoint with \"s3.amazonaws.com\" causes 400 Bad Request\n+    // errors for non-existent buckets and objects.\n+    // ref: https://github.com/aws/aws-sdk-java-v2/issues/4846\n+    if (!endpointEndsWithCentral) {\n+      LOG.debug(\"Setting endpoint to {}\", endpoint);\n+      resolution.withEndpointStr(endpointStr)\n+          .withEndpointUri(endpoint)\n+          .withUseCentralEndpoint(false);\n+    } else {\n+      resolution.withUseCentralEndpoint(true);\n+    }\n+\n+    final Region r = resolution.getRegion();\n+    if (r != null && Region.regions().contains(r)) {\n\nReview Comment:\n   shouldn't this be a !? doesn't contain?\n\n\n\n", "steveloughran commented on code in PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#discussion_r2547539286\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RegionResolution.java:\n##########\n@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import javax.annotation.Nullable;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.awscore.util.AwsHostNameUtils;\n+import software.amazon.awssdk.regions.Region;\n+import software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.s3a.Invoker;\n+import org.apache.hadoop.fs.s3a.Retries;\n+import org.apache.hadoop.fs.s3a.S3ClientFactory;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT;\n+import static org.apache.hadoop.fs.s3a.Constants.CENTRAL_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_SECURE_CONNECTIONS;\n+import static org.apache.hadoop.fs.s3a.Constants.EC2_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.EMPTY_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.FIPS_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.SDK_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.SECURE_CONNECTIONS;\n+import static org.apache.hadoop.util.Preconditions.checkArgument;\n+import static software.amazon.awssdk.regions.Region.US_EAST_2;\n+\n+/**\n+ * Region resolution.\n+ * <p>This is complicated and can be a source of support escalations.\n+ * <p>The V1 SDK was happy to take an endpoint and\n+ * work details out from there, possibly probing us-central-1 and cacheing\n+ * the result.\n+ * <p>The V2 SDK like the signing region and endpoint to be declared.\n+ * The S3A connector has tried to mimic the V1 code, but lacks some features\n+ * (use of environment variables, probing of EC2 IAM details) for which\n+ * the SDK is better.\n+ *\n+ */\n+public class RegionResolution {\n+\n+  protected static final Logger LOG =\n+      LoggerFactory.getLogger(RegionResolution.class);\n+\n+  /**\n+   * Service to ask SDK to parse.\n+   */\n+  private static final String S3_SERVICE_NAME = \"s3\";\n+\n+  /**\n+   * Pattern to match vpce endpoints on.\n+   */\n+  private static final Pattern VPC_ENDPOINT_PATTERN =\n+      Pattern.compile(\"^(?:.+\\\\.)?([a-z0-9-]+)\\\\.vpce\\\\.amazonaws\\\\.(?:com|com\\\\.cn)$\");\n+\n+  /**\n+   * Error message when an endpoint is set with FIPS enabled: {@value}.\n+   */\n+  @VisibleForTesting\n+  public static final String ERROR_ENDPOINT_WITH_FIPS =\n+      \"Only S3 central endpoint cannot be set when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * Virtual hostnames MUST be used when using the FIPS endpoint.\n+   */\n+  public static final String FIPS_PATH_ACCESS_INCOMPATIBLE =\n+      \"Path style access must be disabled when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * String value for external region: {@value}.\n+   */\n+  public static final String EXTERNAL = \"external\";\n+\n+  /**\n+   * External region, used for third party endpoints.\n+   */\n+  public static final Region EXTERNAL_REGION = Region.of(EXTERNAL);\n+\n+  /**\n+   * How was the region resolved?\n+   */\n+  public enum RegionResolutionMechanism {\n+\n+    CalculatedFromEndpoint(\"Calculated from endpoint\"),\n+    ExternalEndpoint(\"External endpoint\"),\n+    FallbackToCentral(\"Fallback to central endpoint\"),\n+    ParseVpceEndpoint(\"Parse VPCE Endpoint\"),\n+    Ec2Metadata(\"EC2 Metadata\"),\n+    Sdk(\"SDK resolution chain\"),\n+    Specified(\"region specified\");\n+\n+    /**\n+     * Text of the mechanism.\n+     */\n+    private final String mechanism;\n+\n+    RegionResolutionMechanism(String mechanism) {\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * String value of the resolution mechanism.\n+     * @return the resolution mechanism.\n+     */\n+    public String getMechanism() {\n+      return mechanism;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"RegionResolutionMechanism{\");\n+      sb.append(\"mechanism='\").append(mechanism).append('\\'');\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * The resolution of a region and endpoint..\n+   */\n+  public static final class Resolution {\n+\n+    /**\n+     * Region: if null hand down to the SDK.\n+     */\n+    private Region region;\n+\n+    /**\n+     * How was the region resolved?\n+     * Null means unresolved.\n+     */\n+    private RegionResolutionMechanism mechanism;\n+\n+    /**\n+     * Should FIPS be enabled?\n+     */\n+    private boolean useFips;\n+\n+    /**\n+     * Should cross-region access be enabled?\n+     */\n+    private boolean crossRegionAccessEnabled;\n+\n+    /**\n+     * Endpoint as string.\n+     */\n+    private String endpointStr;\n+\n+    /**\n+     * Endpoint URI.\n+     */\n+    private URI endpointUri;\n+\n+    /**\n+     * Use the central endpoint?\n+     */\n+    private boolean useCentralEndpoint;\n+\n+    public Resolution() {\n+    }\n+\n+    /**\n+     * Instantiate with a region and resolution mechanism.\n+     * @param region region\n+     * @param mechanism resolution mechanism.\n+     */\n+    public Resolution(final Region region, final RegionResolutionMechanism mechanism) {\n+      this.region = region;\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * Set the region.\n+     * Declares the region as resolved even when the value is null (i.e. resolve to SDK).\n+     * @param region region\n+     * @param resolutionMechanism resolution mechanism.\n+     * @return the builder\n+     */\n+    public Resolution withRegion(\n+        @Nullable final Region region,\n+        final RegionResolutionMechanism resolutionMechanism) {\n+      this.region = region;\n+      this.mechanism = requireNonNull(resolutionMechanism);\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseFips(final boolean value) {\n+      useFips = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withCrossRegionAccessEnabled(final boolean value) {\n+      crossRegionAccessEnabled = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointStr(final String value) {\n+      endpointStr = value;\n+      return this;\n+    }\n+\n+    public URI getEndpointUri() {\n+      return endpointUri;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointUri(final URI value) {\n+      endpointUri = value;\n+      return this;\n+    }\n+\n+    public Region getRegion() {\n+      return region;\n+    }\n+\n+    public boolean isUseFips() {\n+      return useFips;\n+    }\n+\n+    public boolean isCrossRegionAccessEnabled() {\n+      return crossRegionAccessEnabled;\n+    }\n+\n+    public RegionResolutionMechanism getMechanism() {\n+      return mechanism;\n+    }\n+\n+    public String getEndpointStr() {\n+      return endpointStr;\n+    }\n+\n+    public boolean isRegionResolved() {\n+      return mechanism != null;\n+    }\n+\n+    public boolean isUseCentralEndpoint() {\n+      return useCentralEndpoint;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseCentralEndpoint(final boolean value) {\n+      useCentralEndpoint = value;\n+      return this;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"Resolution{\");\n+      sb.append(\"region=\").append(region);\n+      sb.append(\", resolution=\").append(mechanism);\n+      sb.append(\", useFips=\").append(useFips);\n+      sb.append(\", crossRegionAccessEnabled=\").append(crossRegionAccessEnabled);\n+      sb.append(\", endpointUri=\").append(endpointUri);\n+      sb.append(\", useCentralEndpoint=\").append(useCentralEndpoint);\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * Given a endpoint string, create the endpoint URI.\n+   * @param endpoint possibly null endpoint.\n+   * @param secureConnections use secure HTTPS connection?\n+   * @return an endpoint uri or null if the endpoint was passed in was null/empty\n+   * @throws IllegalArgumentException failure to parse the endpoint.\n+   */\n+  public static URI buildEndpointUri(String endpoint, final boolean secureConnections) {\n+\n+    String protocol = secureConnections ? \"https\" : \"http\";\n+\n+    if (endpoint == null || endpoint.isEmpty()) {\n+      // don't set an endpoint if none is configured, instead let the SDK figure it out.\n+      return null;\n+    }\n+\n+    if (!endpoint.contains(\"://\")) {\n+      endpoint = String.format(\"%s://%s\", protocol, endpoint);\n+    }\n+\n+    try {\n+      return new URI(endpoint);\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n+  }\n+\n+  /**\n+   * Parses the endpoint to get the region.\n+   * If endpoint is the central one, use US_EAST_2.\n+   * @param endpoint the configure endpoint.\n+   * @param endpointEndsWithCentral true if the endpoint is configured as central.\n+   * @return the S3 region resolution if possible from parsing the endpoint\n+   */\n+  @VisibleForTesting\n+  public static Optional<Resolution> getS3RegionFromEndpoint(\n+      final String endpoint,\n+      final boolean endpointEndsWithCentral) {\n+\n+    if (!endpointEndsWithCentral) {\n+      // S3 VPC endpoint parsing\n+      Matcher matcher = VPC_ENDPOINT_PATTERN.matcher(endpoint);\n+      if (matcher.find()) {\n+        LOG.debug(\"Mapping to VPCE\");\n+        LOG.debug(\"Endpoint {} is VPC endpoint; parsing region as {}\",\n+            endpoint, matcher.group(1));\n+        return Optional.of(new Resolution(\n+            Region.of(matcher.group(1)),\n+            RegionResolutionMechanism.ParseVpceEndpoint));\n+      }\n+\n+      LOG.debug(\"Endpoint {} is not the default; parsing\", endpoint);\n+      return AwsHostNameUtils.parseSigningRegion(endpoint, S3_SERVICE_NAME)\n+          .map(r ->\n+              new Resolution(r, RegionResolutionMechanism.CalculatedFromEndpoint));\n+    }\n+\n+    // No resolution.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Is this an AWS endpoint, that is: has an endpoint been set which matches\n+   * amazon.\n+   * @param endpoint non-null endpoint URL\n+   * @return true if this is amazonaws or amazonaws china\n+   */\n+  public static boolean isAwsEndpoint(final String endpoint) {\n+    final String h = endpoint.toLowerCase(Locale.ROOT);\n+    // Common AWS partitions: global (.amazonaws.com) and China (.amazonaws.com.cn).\n+    return h.endsWith(\".amazonaws.com\")\n+        || h.endsWith(\".amazonaws.com.cn\");\n+  }\n+\n+\n+  /**\n+   * Does the region name refer to an SDK region?\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isSdkRegion(String configuredRegion) {\n+    return SDK_REGION.equalsIgnoreCase(configuredRegion)\n+        || EMPTY_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Does the region name refer to {@code \"ec2\"} in which case special handling\n+   * is required.\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isEc2Region(String configuredRegion) {\n+    return EC2_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Calculate the region and the final endpoint.\n+   * @param parameters creation parameters\n+   * @param conf configuration with other options.\n+   * @return the resolved region and endpoint.\n+   * @throws IOException if the client failed to communicate with the IAM service.\n+   * @throws IllegalArgumentException failure to parse endpoint, or FIPS settings.\n+   */\n+  @Retries.OnceTranslated\n+  public static Resolution calculateRegion(\n+      final S3ClientFactory.S3ClientCreationParameters parameters,\n+      final Configuration conf) throws IOException {\n+\n+    Resolution resolution = new Resolution();\n+\n+    // endpoint; may be null\n+    final String endpointStr = parameters.getEndpoint();\n+    boolean endpointDeclared = endpointStr != null && !endpointStr.isEmpty();\n+    // will be null if endpointStr is null/empty\n+    final URI endpoint = buildEndpointUri(endpointStr,\n+        conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS));\n+\n+    final String configuredRegion = parameters.getRegion();\n+\n+    // If the region was configured, set it.\n+    // this includes special handling of the sdk, ec2 and \"\" regions.\n+    if (configuredRegion != null) {\n+      checkArgument(!\"null\".equals(configuredRegion),\n+          \"null is region name\");\n+      if (isSdkRegion(configuredRegion)) {\n+        resolution.withRegion(null, RegionResolutionMechanism.Sdk);\n+      } else if (isEc2Region(configuredRegion)) {\n+        // special EC2 handling\n+        final Resolution r = getS3RegionFromEc2IAM();\n+        resolution.withRegion(r.getRegion(), r.getMechanism());\n\nReview Comment:\n   been thinking about that. \r\n   \r\n   I'm thinking\r\n   * \"ec2 maps to sdk\"\r\n   * its possible to configure the fallback mechanism if region is not declared, so rather than via central if you declare a fallback of \"sdk\" in `fs.s3a.endpoint.region.fallback` then we don't do the central stuff and go straight to sdk.\n\n\n\n", "steveloughran commented on code in PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#discussion_r2547542006\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RegionResolution.java:\n##########\n@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import javax.annotation.Nullable;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.awscore.util.AwsHostNameUtils;\n+import software.amazon.awssdk.regions.Region;\n+import software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.s3a.Invoker;\n+import org.apache.hadoop.fs.s3a.Retries;\n+import org.apache.hadoop.fs.s3a.S3ClientFactory;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT;\n+import static org.apache.hadoop.fs.s3a.Constants.CENTRAL_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_SECURE_CONNECTIONS;\n+import static org.apache.hadoop.fs.s3a.Constants.EC2_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.EMPTY_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.FIPS_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.SDK_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.SECURE_CONNECTIONS;\n+import static org.apache.hadoop.util.Preconditions.checkArgument;\n+import static software.amazon.awssdk.regions.Region.US_EAST_2;\n+\n+/**\n+ * Region resolution.\n+ * <p>This is complicated and can be a source of support escalations.\n+ * <p>The V1 SDK was happy to take an endpoint and\n+ * work details out from there, possibly probing us-central-1 and cacheing\n+ * the result.\n+ * <p>The V2 SDK like the signing region and endpoint to be declared.\n+ * The S3A connector has tried to mimic the V1 code, but lacks some features\n+ * (use of environment variables, probing of EC2 IAM details) for which\n+ * the SDK is better.\n+ *\n+ */\n+public class RegionResolution {\n+\n+  protected static final Logger LOG =\n+      LoggerFactory.getLogger(RegionResolution.class);\n+\n+  /**\n+   * Service to ask SDK to parse.\n+   */\n+  private static final String S3_SERVICE_NAME = \"s3\";\n+\n+  /**\n+   * Pattern to match vpce endpoints on.\n+   */\n+  private static final Pattern VPC_ENDPOINT_PATTERN =\n+      Pattern.compile(\"^(?:.+\\\\.)?([a-z0-9-]+)\\\\.vpce\\\\.amazonaws\\\\.(?:com|com\\\\.cn)$\");\n+\n+  /**\n+   * Error message when an endpoint is set with FIPS enabled: {@value}.\n+   */\n+  @VisibleForTesting\n+  public static final String ERROR_ENDPOINT_WITH_FIPS =\n+      \"Only S3 central endpoint cannot be set when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * Virtual hostnames MUST be used when using the FIPS endpoint.\n+   */\n+  public static final String FIPS_PATH_ACCESS_INCOMPATIBLE =\n+      \"Path style access must be disabled when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * String value for external region: {@value}.\n+   */\n+  public static final String EXTERNAL = \"external\";\n+\n+  /**\n+   * External region, used for third party endpoints.\n+   */\n+  public static final Region EXTERNAL_REGION = Region.of(EXTERNAL);\n+\n+  /**\n+   * How was the region resolved?\n+   */\n+  public enum RegionResolutionMechanism {\n+\n+    CalculatedFromEndpoint(\"Calculated from endpoint\"),\n+    ExternalEndpoint(\"External endpoint\"),\n+    FallbackToCentral(\"Fallback to central endpoint\"),\n+    ParseVpceEndpoint(\"Parse VPCE Endpoint\"),\n+    Ec2Metadata(\"EC2 Metadata\"),\n+    Sdk(\"SDK resolution chain\"),\n+    Specified(\"region specified\");\n+\n+    /**\n+     * Text of the mechanism.\n+     */\n+    private final String mechanism;\n+\n+    RegionResolutionMechanism(String mechanism) {\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * String value of the resolution mechanism.\n+     * @return the resolution mechanism.\n+     */\n+    public String getMechanism() {\n+      return mechanism;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"RegionResolutionMechanism{\");\n+      sb.append(\"mechanism='\").append(mechanism).append('\\'');\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * The resolution of a region and endpoint..\n+   */\n+  public static final class Resolution {\n+\n+    /**\n+     * Region: if null hand down to the SDK.\n+     */\n+    private Region region;\n+\n+    /**\n+     * How was the region resolved?\n+     * Null means unresolved.\n+     */\n+    private RegionResolutionMechanism mechanism;\n+\n+    /**\n+     * Should FIPS be enabled?\n+     */\n+    private boolean useFips;\n+\n+    /**\n+     * Should cross-region access be enabled?\n+     */\n+    private boolean crossRegionAccessEnabled;\n+\n+    /**\n+     * Endpoint as string.\n+     */\n+    private String endpointStr;\n+\n+    /**\n+     * Endpoint URI.\n+     */\n+    private URI endpointUri;\n+\n+    /**\n+     * Use the central endpoint?\n+     */\n+    private boolean useCentralEndpoint;\n+\n+    public Resolution() {\n+    }\n+\n+    /**\n+     * Instantiate with a region and resolution mechanism.\n+     * @param region region\n+     * @param mechanism resolution mechanism.\n+     */\n+    public Resolution(final Region region, final RegionResolutionMechanism mechanism) {\n+      this.region = region;\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * Set the region.\n+     * Declares the region as resolved even when the value is null (i.e. resolve to SDK).\n+     * @param region region\n+     * @param resolutionMechanism resolution mechanism.\n+     * @return the builder\n+     */\n+    public Resolution withRegion(\n+        @Nullable final Region region,\n+        final RegionResolutionMechanism resolutionMechanism) {\n+      this.region = region;\n+      this.mechanism = requireNonNull(resolutionMechanism);\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseFips(final boolean value) {\n+      useFips = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withCrossRegionAccessEnabled(final boolean value) {\n+      crossRegionAccessEnabled = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointStr(final String value) {\n+      endpointStr = value;\n+      return this;\n+    }\n+\n+    public URI getEndpointUri() {\n+      return endpointUri;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointUri(final URI value) {\n+      endpointUri = value;\n+      return this;\n+    }\n+\n+    public Region getRegion() {\n+      return region;\n+    }\n+\n+    public boolean isUseFips() {\n+      return useFips;\n+    }\n+\n+    public boolean isCrossRegionAccessEnabled() {\n+      return crossRegionAccessEnabled;\n+    }\n+\n+    public RegionResolutionMechanism getMechanism() {\n+      return mechanism;\n+    }\n+\n+    public String getEndpointStr() {\n+      return endpointStr;\n+    }\n+\n+    public boolean isRegionResolved() {\n+      return mechanism != null;\n+    }\n+\n+    public boolean isUseCentralEndpoint() {\n+      return useCentralEndpoint;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseCentralEndpoint(final boolean value) {\n+      useCentralEndpoint = value;\n+      return this;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"Resolution{\");\n+      sb.append(\"region=\").append(region);\n+      sb.append(\", resolution=\").append(mechanism);\n+      sb.append(\", useFips=\").append(useFips);\n+      sb.append(\", crossRegionAccessEnabled=\").append(crossRegionAccessEnabled);\n+      sb.append(\", endpointUri=\").append(endpointUri);\n+      sb.append(\", useCentralEndpoint=\").append(useCentralEndpoint);\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * Given a endpoint string, create the endpoint URI.\n+   * @param endpoint possibly null endpoint.\n+   * @param secureConnections use secure HTTPS connection?\n+   * @return an endpoint uri or null if the endpoint was passed in was null/empty\n+   * @throws IllegalArgumentException failure to parse the endpoint.\n+   */\n+  public static URI buildEndpointUri(String endpoint, final boolean secureConnections) {\n+\n+    String protocol = secureConnections ? \"https\" : \"http\";\n+\n+    if (endpoint == null || endpoint.isEmpty()) {\n+      // don't set an endpoint if none is configured, instead let the SDK figure it out.\n+      return null;\n+    }\n+\n+    if (!endpoint.contains(\"://\")) {\n+      endpoint = String.format(\"%s://%s\", protocol, endpoint);\n+    }\n+\n+    try {\n+      return new URI(endpoint);\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n+  }\n+\n+  /**\n+   * Parses the endpoint to get the region.\n+   * If endpoint is the central one, use US_EAST_2.\n+   * @param endpoint the configure endpoint.\n+   * @param endpointEndsWithCentral true if the endpoint is configured as central.\n+   * @return the S3 region resolution if possible from parsing the endpoint\n+   */\n+  @VisibleForTesting\n+  public static Optional<Resolution> getS3RegionFromEndpoint(\n+      final String endpoint,\n+      final boolean endpointEndsWithCentral) {\n+\n+    if (!endpointEndsWithCentral) {\n+      // S3 VPC endpoint parsing\n+      Matcher matcher = VPC_ENDPOINT_PATTERN.matcher(endpoint);\n+      if (matcher.find()) {\n+        LOG.debug(\"Mapping to VPCE\");\n+        LOG.debug(\"Endpoint {} is VPC endpoint; parsing region as {}\",\n+            endpoint, matcher.group(1));\n+        return Optional.of(new Resolution(\n+            Region.of(matcher.group(1)),\n+            RegionResolutionMechanism.ParseVpceEndpoint));\n+      }\n+\n+      LOG.debug(\"Endpoint {} is not the default; parsing\", endpoint);\n+      return AwsHostNameUtils.parseSigningRegion(endpoint, S3_SERVICE_NAME)\n+          .map(r ->\n+              new Resolution(r, RegionResolutionMechanism.CalculatedFromEndpoint));\n+    }\n+\n+    // No resolution.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Is this an AWS endpoint, that is: has an endpoint been set which matches\n+   * amazon.\n+   * @param endpoint non-null endpoint URL\n+   * @return true if this is amazonaws or amazonaws china\n+   */\n+  public static boolean isAwsEndpoint(final String endpoint) {\n+    final String h = endpoint.toLowerCase(Locale.ROOT);\n+    // Common AWS partitions: global (.amazonaws.com) and China (.amazonaws.com.cn).\n+    return h.endsWith(\".amazonaws.com\")\n+        || h.endsWith(\".amazonaws.com.cn\");\n+  }\n+\n+\n+  /**\n+   * Does the region name refer to an SDK region?\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isSdkRegion(String configuredRegion) {\n+    return SDK_REGION.equalsIgnoreCase(configuredRegion)\n+        || EMPTY_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Does the region name refer to {@code \"ec2\"} in which case special handling\n+   * is required.\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isEc2Region(String configuredRegion) {\n+    return EC2_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Calculate the region and the final endpoint.\n+   * @param parameters creation parameters\n+   * @param conf configuration with other options.\n+   * @return the resolved region and endpoint.\n+   * @throws IOException if the client failed to communicate with the IAM service.\n+   * @throws IllegalArgumentException failure to parse endpoint, or FIPS settings.\n+   */\n+  @Retries.OnceTranslated\n+  public static Resolution calculateRegion(\n+      final S3ClientFactory.S3ClientCreationParameters parameters,\n+      final Configuration conf) throws IOException {\n+\n+    Resolution resolution = new Resolution();\n+\n+    // endpoint; may be null\n+    final String endpointStr = parameters.getEndpoint();\n+    boolean endpointDeclared = endpointStr != null && !endpointStr.isEmpty();\n+    // will be null if endpointStr is null/empty\n+    final URI endpoint = buildEndpointUri(endpointStr,\n+        conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS));\n+\n+    final String configuredRegion = parameters.getRegion();\n+\n+    // If the region was configured, set it.\n+    // this includes special handling of the sdk, ec2 and \"\" regions.\n+    if (configuredRegion != null) {\n+      checkArgument(!\"null\".equals(configuredRegion),\n+          \"null is region name\");\n+      if (isSdkRegion(configuredRegion)) {\n+        resolution.withRegion(null, RegionResolutionMechanism.Sdk);\n+      } else if (isEc2Region(configuredRegion)) {\n+        // special EC2 handling\n+        final Resolution r = getS3RegionFromEc2IAM();\n+        resolution.withRegion(r.getRegion(), r.getMechanism());\n+      } else {\n+        resolution.withRegion(Region.of(configuredRegion),\n+            RegionResolutionMechanism.Specified);\n+      }\n+    }\n+\n+    // central endpoint if no endpoint has been set, or it is explicitly\n+    // requested\n+    boolean endpointEndsWithCentral = !endpointDeclared\n+        || endpointStr.endsWith(CENTRAL_ENDPOINT);\n+\n+    if (!resolution.isRegionResolved()) {\n+      // parse from the endpoint and set if calculated\n+      LOG.debug(\"Falling back to parsing region endpoint {}; endpointEndsWithCentral={}\",\n+          endpointStr, endpointEndsWithCentral);\n+      final Optional<Resolution> regionFromEndpoint =\n+          getS3RegionFromEndpoint(endpointStr, endpointEndsWithCentral);\n+      if (regionFromEndpoint.isPresent()) {\n+        regionFromEndpoint\n+            .map(r ->\n+                resolution.withRegion(r.getRegion(), r.getMechanism()));\n+      }\n+    }\n+\n+    // cross region setting.\n+    resolution.withCrossRegionAccessEnabled(\n+        conf.getBoolean(AWS_S3_CROSS_REGION_ACCESS_ENABLED,\n+            AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT));\n+\n+    // fips settings.\n+    final boolean fipsEnabled = parameters.isFipsEnabled();\n+    resolution.withUseFips(fipsEnabled);\n+    if (fipsEnabled) {\n+      // validate the FIPS settings\n+      checkArgument(endpoint == null || endpointEndsWithCentral,\n+          \"%s : %s\", ERROR_ENDPOINT_WITH_FIPS, endpoint);\n+      checkArgument(!parameters.isPathStyleAccess(),\n+          FIPS_PATH_ACCESS_INCOMPATIBLE);\n+    }\n+\n+\n+    if (!resolution.isRegionResolved()) {\n+      // still not resolved.\n+      if (!endpointDeclared || isAwsEndpoint(endpointStr)) {\n+        // still failing to resolve the region\n+        // fall back to central\n+        resolution.withRegion(US_EAST_2, RegionResolutionMechanism.FallbackToCentral);\n\nReview Comment:\n   was your customer running in EC2? in which case the SDK was what they needed, wasn't it? which is where the fallback option gets involved.\n\n\n\n", "mukund-thakur commented on code in PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#discussion_r2550602481\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RegionResolution.java:\n##########\n@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import javax.annotation.Nullable;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.awscore.util.AwsHostNameUtils;\n+import software.amazon.awssdk.regions.Region;\n+import software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.s3a.Invoker;\n+import org.apache.hadoop.fs.s3a.Retries;\n+import org.apache.hadoop.fs.s3a.S3ClientFactory;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT;\n+import static org.apache.hadoop.fs.s3a.Constants.CENTRAL_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_SECURE_CONNECTIONS;\n+import static org.apache.hadoop.fs.s3a.Constants.EC2_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.EMPTY_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.FIPS_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.SDK_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.SECURE_CONNECTIONS;\n+import static org.apache.hadoop.util.Preconditions.checkArgument;\n+import static software.amazon.awssdk.regions.Region.US_EAST_2;\n+\n+/**\n+ * Region resolution.\n+ * <p>This is complicated and can be a source of support escalations.\n+ * <p>The V1 SDK was happy to take an endpoint and\n+ * work details out from there, possibly probing us-central-1 and cacheing\n+ * the result.\n+ * <p>The V2 SDK like the signing region and endpoint to be declared.\n+ * The S3A connector has tried to mimic the V1 code, but lacks some features\n+ * (use of environment variables, probing of EC2 IAM details) for which\n+ * the SDK is better.\n+ *\n+ */\n+public class RegionResolution {\n+\n+  protected static final Logger LOG =\n+      LoggerFactory.getLogger(RegionResolution.class);\n+\n+  /**\n+   * Service to ask SDK to parse.\n+   */\n+  private static final String S3_SERVICE_NAME = \"s3\";\n+\n+  /**\n+   * Pattern to match vpce endpoints on.\n+   */\n+  private static final Pattern VPC_ENDPOINT_PATTERN =\n+      Pattern.compile(\"^(?:.+\\\\.)?([a-z0-9-]+)\\\\.vpce\\\\.amazonaws\\\\.(?:com|com\\\\.cn)$\");\n+\n+  /**\n+   * Error message when an endpoint is set with FIPS enabled: {@value}.\n+   */\n+  @VisibleForTesting\n+  public static final String ERROR_ENDPOINT_WITH_FIPS =\n+      \"Only S3 central endpoint cannot be set when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * Virtual hostnames MUST be used when using the FIPS endpoint.\n+   */\n+  public static final String FIPS_PATH_ACCESS_INCOMPATIBLE =\n+      \"Path style access must be disabled when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * String value for external region: {@value}.\n+   */\n+  public static final String EXTERNAL = \"external\";\n+\n+  /**\n+   * External region, used for third party endpoints.\n+   */\n+  public static final Region EXTERNAL_REGION = Region.of(EXTERNAL);\n+\n+  /**\n+   * How was the region resolved?\n+   */\n+  public enum RegionResolutionMechanism {\n+\n+    CalculatedFromEndpoint(\"Calculated from endpoint\"),\n+    ExternalEndpoint(\"External endpoint\"),\n+    FallbackToCentral(\"Fallback to central endpoint\"),\n+    ParseVpceEndpoint(\"Parse VPCE Endpoint\"),\n+    Ec2Metadata(\"EC2 Metadata\"),\n+    Sdk(\"SDK resolution chain\"),\n+    Specified(\"region specified\");\n+\n+    /**\n+     * Text of the mechanism.\n+     */\n+    private final String mechanism;\n+\n+    RegionResolutionMechanism(String mechanism) {\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * String value of the resolution mechanism.\n+     * @return the resolution mechanism.\n+     */\n+    public String getMechanism() {\n+      return mechanism;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"RegionResolutionMechanism{\");\n+      sb.append(\"mechanism='\").append(mechanism).append('\\'');\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * The resolution of a region and endpoint..\n+   */\n+  public static final class Resolution {\n+\n+    /**\n+     * Region: if null hand down to the SDK.\n+     */\n+    private Region region;\n+\n+    /**\n+     * How was the region resolved?\n+     * Null means unresolved.\n+     */\n+    private RegionResolutionMechanism mechanism;\n+\n+    /**\n+     * Should FIPS be enabled?\n+     */\n+    private boolean useFips;\n+\n+    /**\n+     * Should cross-region access be enabled?\n+     */\n+    private boolean crossRegionAccessEnabled;\n+\n+    /**\n+     * Endpoint as string.\n+     */\n+    private String endpointStr;\n+\n+    /**\n+     * Endpoint URI.\n+     */\n+    private URI endpointUri;\n+\n+    /**\n+     * Use the central endpoint?\n+     */\n+    private boolean useCentralEndpoint;\n+\n+    public Resolution() {\n+    }\n+\n+    /**\n+     * Instantiate with a region and resolution mechanism.\n+     * @param region region\n+     * @param mechanism resolution mechanism.\n+     */\n+    public Resolution(final Region region, final RegionResolutionMechanism mechanism) {\n+      this.region = region;\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * Set the region.\n+     * Declares the region as resolved even when the value is null (i.e. resolve to SDK).\n+     * @param region region\n+     * @param resolutionMechanism resolution mechanism.\n+     * @return the builder\n+     */\n+    public Resolution withRegion(\n+        @Nullable final Region region,\n+        final RegionResolutionMechanism resolutionMechanism) {\n+      this.region = region;\n+      this.mechanism = requireNonNull(resolutionMechanism);\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseFips(final boolean value) {\n+      useFips = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withCrossRegionAccessEnabled(final boolean value) {\n+      crossRegionAccessEnabled = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointStr(final String value) {\n+      endpointStr = value;\n+      return this;\n+    }\n+\n+    public URI getEndpointUri() {\n+      return endpointUri;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointUri(final URI value) {\n+      endpointUri = value;\n+      return this;\n+    }\n+\n+    public Region getRegion() {\n+      return region;\n+    }\n+\n+    public boolean isUseFips() {\n+      return useFips;\n+    }\n+\n+    public boolean isCrossRegionAccessEnabled() {\n+      return crossRegionAccessEnabled;\n+    }\n+\n+    public RegionResolutionMechanism getMechanism() {\n+      return mechanism;\n+    }\n+\n+    public String getEndpointStr() {\n+      return endpointStr;\n+    }\n+\n+    public boolean isRegionResolved() {\n+      return mechanism != null;\n+    }\n+\n+    public boolean isUseCentralEndpoint() {\n+      return useCentralEndpoint;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseCentralEndpoint(final boolean value) {\n+      useCentralEndpoint = value;\n+      return this;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"Resolution{\");\n+      sb.append(\"region=\").append(region);\n+      sb.append(\", resolution=\").append(mechanism);\n+      sb.append(\", useFips=\").append(useFips);\n+      sb.append(\", crossRegionAccessEnabled=\").append(crossRegionAccessEnabled);\n+      sb.append(\", endpointUri=\").append(endpointUri);\n+      sb.append(\", useCentralEndpoint=\").append(useCentralEndpoint);\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * Given a endpoint string, create the endpoint URI.\n+   * @param endpoint possibly null endpoint.\n+   * @param secureConnections use secure HTTPS connection?\n+   * @return an endpoint uri or null if the endpoint was passed in was null/empty\n+   * @throws IllegalArgumentException failure to parse the endpoint.\n+   */\n+  public static URI buildEndpointUri(String endpoint, final boolean secureConnections) {\n+\n+    String protocol = secureConnections ? \"https\" : \"http\";\n+\n+    if (endpoint == null || endpoint.isEmpty()) {\n+      // don't set an endpoint if none is configured, instead let the SDK figure it out.\n+      return null;\n+    }\n+\n+    if (!endpoint.contains(\"://\")) {\n+      endpoint = String.format(\"%s://%s\", protocol, endpoint);\n+    }\n+\n+    try {\n+      return new URI(endpoint);\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n+  }\n+\n+  /**\n+   * Parses the endpoint to get the region.\n+   * If endpoint is the central one, use US_EAST_2.\n+   * @param endpoint the configure endpoint.\n+   * @param endpointEndsWithCentral true if the endpoint is configured as central.\n+   * @return the S3 region resolution if possible from parsing the endpoint\n+   */\n+  @VisibleForTesting\n+  public static Optional<Resolution> getS3RegionFromEndpoint(\n+      final String endpoint,\n+      final boolean endpointEndsWithCentral) {\n+\n+    if (!endpointEndsWithCentral) {\n+      // S3 VPC endpoint parsing\n+      Matcher matcher = VPC_ENDPOINT_PATTERN.matcher(endpoint);\n+      if (matcher.find()) {\n+        LOG.debug(\"Mapping to VPCE\");\n+        LOG.debug(\"Endpoint {} is VPC endpoint; parsing region as {}\",\n+            endpoint, matcher.group(1));\n+        return Optional.of(new Resolution(\n+            Region.of(matcher.group(1)),\n+            RegionResolutionMechanism.ParseVpceEndpoint));\n+      }\n+\n+      LOG.debug(\"Endpoint {} is not the default; parsing\", endpoint);\n+      return AwsHostNameUtils.parseSigningRegion(endpoint, S3_SERVICE_NAME)\n+          .map(r ->\n+              new Resolution(r, RegionResolutionMechanism.CalculatedFromEndpoint));\n+    }\n+\n+    // No resolution.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Is this an AWS endpoint, that is: has an endpoint been set which matches\n+   * amazon.\n+   * @param endpoint non-null endpoint URL\n+   * @return true if this is amazonaws or amazonaws china\n+   */\n+  public static boolean isAwsEndpoint(final String endpoint) {\n+    final String h = endpoint.toLowerCase(Locale.ROOT);\n+    // Common AWS partitions: global (.amazonaws.com) and China (.amazonaws.com.cn).\n+    return h.endsWith(\".amazonaws.com\")\n+        || h.endsWith(\".amazonaws.com.cn\");\n+  }\n+\n+\n+  /**\n+   * Does the region name refer to an SDK region?\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isSdkRegion(String configuredRegion) {\n+    return SDK_REGION.equalsIgnoreCase(configuredRegion)\n+        || EMPTY_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Does the region name refer to {@code \"ec2\"} in which case special handling\n+   * is required.\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isEc2Region(String configuredRegion) {\n+    return EC2_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Calculate the region and the final endpoint.\n+   * @param parameters creation parameters\n+   * @param conf configuration with other options.\n+   * @return the resolved region and endpoint.\n+   * @throws IOException if the client failed to communicate with the IAM service.\n+   * @throws IllegalArgumentException failure to parse endpoint, or FIPS settings.\n+   */\n+  @Retries.OnceTranslated\n+  public static Resolution calculateRegion(\n+      final S3ClientFactory.S3ClientCreationParameters parameters,\n+      final Configuration conf) throws IOException {\n+\n+    Resolution resolution = new Resolution();\n+\n+    // endpoint; may be null\n+    final String endpointStr = parameters.getEndpoint();\n+    boolean endpointDeclared = endpointStr != null && !endpointStr.isEmpty();\n+    // will be null if endpointStr is null/empty\n+    final URI endpoint = buildEndpointUri(endpointStr,\n+        conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS));\n+\n+    final String configuredRegion = parameters.getRegion();\n+\n+    // If the region was configured, set it.\n+    // this includes special handling of the sdk, ec2 and \"\" regions.\n+    if (configuredRegion != null) {\n+      checkArgument(!\"null\".equals(configuredRegion),\n+          \"null is region name\");\n+      if (isSdkRegion(configuredRegion)) {\n+        resolution.withRegion(null, RegionResolutionMechanism.Sdk);\n+      } else if (isEc2Region(configuredRegion)) {\n+        // special EC2 handling\n+        final Resolution r = getS3RegionFromEc2IAM();\n+        resolution.withRegion(r.getRegion(), r.getMechanism());\n\nReview Comment:\n   Ah sorry I always forget this. What's the order for SDK resolution chain right now? could someone point me to the latest doc if we have. \n\n\n\n", "mukund-thakur commented on code in PR #8058:\nURL: https://github.com/apache/hadoop/pull/8058#discussion_r2550628896\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RegionResolution.java:\n##########\n@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import javax.annotation.Nullable;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.awscore.util.AwsHostNameUtils;\n+import software.amazon.awssdk.regions.Region;\n+import software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.s3a.Invoker;\n+import org.apache.hadoop.fs.s3a.Retries;\n+import org.apache.hadoop.fs.s3a.S3ClientFactory;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED;\n+import static org.apache.hadoop.fs.s3a.Constants.AWS_S3_CROSS_REGION_ACCESS_ENABLED_DEFAULT;\n+import static org.apache.hadoop.fs.s3a.Constants.CENTRAL_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_SECURE_CONNECTIONS;\n+import static org.apache.hadoop.fs.s3a.Constants.EC2_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.EMPTY_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.FIPS_ENDPOINT;\n+import static org.apache.hadoop.fs.s3a.Constants.SDK_REGION;\n+import static org.apache.hadoop.fs.s3a.Constants.SECURE_CONNECTIONS;\n+import static org.apache.hadoop.util.Preconditions.checkArgument;\n+import static software.amazon.awssdk.regions.Region.US_EAST_2;\n+\n+/**\n+ * Region resolution.\n+ * <p>This is complicated and can be a source of support escalations.\n+ * <p>The V1 SDK was happy to take an endpoint and\n+ * work details out from there, possibly probing us-central-1 and cacheing\n+ * the result.\n+ * <p>The V2 SDK like the signing region and endpoint to be declared.\n+ * The S3A connector has tried to mimic the V1 code, but lacks some features\n+ * (use of environment variables, probing of EC2 IAM details) for which\n+ * the SDK is better.\n+ *\n+ */\n+public class RegionResolution {\n+\n+  protected static final Logger LOG =\n+      LoggerFactory.getLogger(RegionResolution.class);\n+\n+  /**\n+   * Service to ask SDK to parse.\n+   */\n+  private static final String S3_SERVICE_NAME = \"s3\";\n+\n+  /**\n+   * Pattern to match vpce endpoints on.\n+   */\n+  private static final Pattern VPC_ENDPOINT_PATTERN =\n+      Pattern.compile(\"^(?:.+\\\\.)?([a-z0-9-]+)\\\\.vpce\\\\.amazonaws\\\\.(?:com|com\\\\.cn)$\");\n+\n+  /**\n+   * Error message when an endpoint is set with FIPS enabled: {@value}.\n+   */\n+  @VisibleForTesting\n+  public static final String ERROR_ENDPOINT_WITH_FIPS =\n+      \"Only S3 central endpoint cannot be set when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * Virtual hostnames MUST be used when using the FIPS endpoint.\n+   */\n+  public static final String FIPS_PATH_ACCESS_INCOMPATIBLE =\n+      \"Path style access must be disabled when \" + FIPS_ENDPOINT + \" is true\";\n+\n+  /**\n+   * String value for external region: {@value}.\n+   */\n+  public static final String EXTERNAL = \"external\";\n+\n+  /**\n+   * External region, used for third party endpoints.\n+   */\n+  public static final Region EXTERNAL_REGION = Region.of(EXTERNAL);\n+\n+  /**\n+   * How was the region resolved?\n+   */\n+  public enum RegionResolutionMechanism {\n+\n+    CalculatedFromEndpoint(\"Calculated from endpoint\"),\n+    ExternalEndpoint(\"External endpoint\"),\n+    FallbackToCentral(\"Fallback to central endpoint\"),\n+    ParseVpceEndpoint(\"Parse VPCE Endpoint\"),\n+    Ec2Metadata(\"EC2 Metadata\"),\n+    Sdk(\"SDK resolution chain\"),\n+    Specified(\"region specified\");\n+\n+    /**\n+     * Text of the mechanism.\n+     */\n+    private final String mechanism;\n+\n+    RegionResolutionMechanism(String mechanism) {\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * String value of the resolution mechanism.\n+     * @return the resolution mechanism.\n+     */\n+    public String getMechanism() {\n+      return mechanism;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"RegionResolutionMechanism{\");\n+      sb.append(\"mechanism='\").append(mechanism).append('\\'');\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * The resolution of a region and endpoint..\n+   */\n+  public static final class Resolution {\n+\n+    /**\n+     * Region: if null hand down to the SDK.\n+     */\n+    private Region region;\n+\n+    /**\n+     * How was the region resolved?\n+     * Null means unresolved.\n+     */\n+    private RegionResolutionMechanism mechanism;\n+\n+    /**\n+     * Should FIPS be enabled?\n+     */\n+    private boolean useFips;\n+\n+    /**\n+     * Should cross-region access be enabled?\n+     */\n+    private boolean crossRegionAccessEnabled;\n+\n+    /**\n+     * Endpoint as string.\n+     */\n+    private String endpointStr;\n+\n+    /**\n+     * Endpoint URI.\n+     */\n+    private URI endpointUri;\n+\n+    /**\n+     * Use the central endpoint?\n+     */\n+    private boolean useCentralEndpoint;\n+\n+    public Resolution() {\n+    }\n+\n+    /**\n+     * Instantiate with a region and resolution mechanism.\n+     * @param region region\n+     * @param mechanism resolution mechanism.\n+     */\n+    public Resolution(final Region region, final RegionResolutionMechanism mechanism) {\n+      this.region = region;\n+      this.mechanism = mechanism;\n+    }\n+\n+    /**\n+     * Set the region.\n+     * Declares the region as resolved even when the value is null (i.e. resolve to SDK).\n+     * @param region region\n+     * @param resolutionMechanism resolution mechanism.\n+     * @return the builder\n+     */\n+    public Resolution withRegion(\n+        @Nullable final Region region,\n+        final RegionResolutionMechanism resolutionMechanism) {\n+      this.region = region;\n+      this.mechanism = requireNonNull(resolutionMechanism);\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseFips(final boolean value) {\n+      useFips = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withCrossRegionAccessEnabled(final boolean value) {\n+      crossRegionAccessEnabled = value;\n+      return this;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointStr(final String value) {\n+      endpointStr = value;\n+      return this;\n+    }\n+\n+    public URI getEndpointUri() {\n+      return endpointUri;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withEndpointUri(final URI value) {\n+      endpointUri = value;\n+      return this;\n+    }\n+\n+    public Region getRegion() {\n+      return region;\n+    }\n+\n+    public boolean isUseFips() {\n+      return useFips;\n+    }\n+\n+    public boolean isCrossRegionAccessEnabled() {\n+      return crossRegionAccessEnabled;\n+    }\n+\n+    public RegionResolutionMechanism getMechanism() {\n+      return mechanism;\n+    }\n+\n+    public String getEndpointStr() {\n+      return endpointStr;\n+    }\n+\n+    public boolean isRegionResolved() {\n+      return mechanism != null;\n+    }\n+\n+    public boolean isUseCentralEndpoint() {\n+      return useCentralEndpoint;\n+    }\n+\n+    /**\n+     * Set builder value.\n+     * @param value new value\n+     * @return the builder\n+     */\n+    public Resolution withUseCentralEndpoint(final boolean value) {\n+      useCentralEndpoint = value;\n+      return this;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      final StringBuilder sb = new StringBuilder(\"Resolution{\");\n+      sb.append(\"region=\").append(region);\n+      sb.append(\", resolution=\").append(mechanism);\n+      sb.append(\", useFips=\").append(useFips);\n+      sb.append(\", crossRegionAccessEnabled=\").append(crossRegionAccessEnabled);\n+      sb.append(\", endpointUri=\").append(endpointUri);\n+      sb.append(\", useCentralEndpoint=\").append(useCentralEndpoint);\n+      sb.append('}');\n+      return sb.toString();\n+    }\n+  }\n+\n+  /**\n+   * Given a endpoint string, create the endpoint URI.\n+   * @param endpoint possibly null endpoint.\n+   * @param secureConnections use secure HTTPS connection?\n+   * @return an endpoint uri or null if the endpoint was passed in was null/empty\n+   * @throws IllegalArgumentException failure to parse the endpoint.\n+   */\n+  public static URI buildEndpointUri(String endpoint, final boolean secureConnections) {\n+\n+    String protocol = secureConnections ? \"https\" : \"http\";\n+\n+    if (endpoint == null || endpoint.isEmpty()) {\n+      // don't set an endpoint if none is configured, instead let the SDK figure it out.\n+      return null;\n+    }\n+\n+    if (!endpoint.contains(\"://\")) {\n+      endpoint = String.format(\"%s://%s\", protocol, endpoint);\n+    }\n+\n+    try {\n+      return new URI(endpoint);\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n+  }\n+\n+  /**\n+   * Parses the endpoint to get the region.\n+   * If endpoint is the central one, use US_EAST_2.\n+   * @param endpoint the configure endpoint.\n+   * @param endpointEndsWithCentral true if the endpoint is configured as central.\n+   * @return the S3 region resolution if possible from parsing the endpoint\n+   */\n+  @VisibleForTesting\n+  public static Optional<Resolution> getS3RegionFromEndpoint(\n+      final String endpoint,\n+      final boolean endpointEndsWithCentral) {\n+\n+    if (!endpointEndsWithCentral) {\n+      // S3 VPC endpoint parsing\n+      Matcher matcher = VPC_ENDPOINT_PATTERN.matcher(endpoint);\n+      if (matcher.find()) {\n+        LOG.debug(\"Mapping to VPCE\");\n+        LOG.debug(\"Endpoint {} is VPC endpoint; parsing region as {}\",\n+            endpoint, matcher.group(1));\n+        return Optional.of(new Resolution(\n+            Region.of(matcher.group(1)),\n+            RegionResolutionMechanism.ParseVpceEndpoint));\n+      }\n+\n+      LOG.debug(\"Endpoint {} is not the default; parsing\", endpoint);\n+      return AwsHostNameUtils.parseSigningRegion(endpoint, S3_SERVICE_NAME)\n+          .map(r ->\n+              new Resolution(r, RegionResolutionMechanism.CalculatedFromEndpoint));\n+    }\n+\n+    // No resolution.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Is this an AWS endpoint, that is: has an endpoint been set which matches\n+   * amazon.\n+   * @param endpoint non-null endpoint URL\n+   * @return true if this is amazonaws or amazonaws china\n+   */\n+  public static boolean isAwsEndpoint(final String endpoint) {\n+    final String h = endpoint.toLowerCase(Locale.ROOT);\n+    // Common AWS partitions: global (.amazonaws.com) and China (.amazonaws.com.cn).\n+    return h.endsWith(\".amazonaws.com\")\n+        || h.endsWith(\".amazonaws.com.cn\");\n+  }\n+\n+\n+  /**\n+   * Does the region name refer to an SDK region?\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isSdkRegion(String configuredRegion) {\n+    return SDK_REGION.equalsIgnoreCase(configuredRegion)\n+        || EMPTY_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Does the region name refer to {@code \"ec2\"} in which case special handling\n+   * is required.\n+   * @param configuredRegion region in the configuration\n+   * @return true if this is considered to refer to an SDK region.\n+   */\n+  public static boolean isEc2Region(String configuredRegion) {\n+    return EC2_REGION.equalsIgnoreCase(configuredRegion);\n+  }\n+\n+  /**\n+   * Calculate the region and the final endpoint.\n+   * @param parameters creation parameters\n+   * @param conf configuration with other options.\n+   * @return the resolved region and endpoint.\n+   * @throws IOException if the client failed to communicate with the IAM service.\n+   * @throws IllegalArgumentException failure to parse endpoint, or FIPS settings.\n+   */\n+  @Retries.OnceTranslated\n+  public static Resolution calculateRegion(\n+      final S3ClientFactory.S3ClientCreationParameters parameters,\n+      final Configuration conf) throws IOException {\n+\n+    Resolution resolution = new Resolution();\n+\n+    // endpoint; may be null\n+    final String endpointStr = parameters.getEndpoint();\n+    boolean endpointDeclared = endpointStr != null && !endpointStr.isEmpty();\n+    // will be null if endpointStr is null/empty\n+    final URI endpoint = buildEndpointUri(endpointStr,\n+        conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS));\n+\n+    final String configuredRegion = parameters.getRegion();\n+\n+    // If the region was configured, set it.\n+    // this includes special handling of the sdk, ec2 and \"\" regions.\n+    if (configuredRegion != null) {\n+      checkArgument(!\"null\".equals(configuredRegion),\n+          \"null is region name\");\n+      if (isSdkRegion(configuredRegion)) {\n+        resolution.withRegion(null, RegionResolutionMechanism.Sdk);\n+      } else if (isEc2Region(configuredRegion)) {\n+        // special EC2 handling\n+        final Resolution r = getS3RegionFromEc2IAM();\n+        resolution.withRegion(r.getRegion(), r.getMechanism());\n\nReview Comment:\n   never mind found it https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/region-selection.html\r\n   I agree with Ahmar here. I don't even think we need ec2. Just \"sdk\" is fine which we set in case of public cloud deployments. \"sdk\" will find it from running ec2. \r\n   In case of private cloud, the older flow will work and customer either has to set the region properly using `fs.s3a.endpoint` or `fs.s3a.endpoint.region`  preferably the latter or enable global read access to the \"us-east-2\" endpoint for the first call to find correct region. \n\n\n\n"], "labels": ["pull-request-available"], "summary": "Add explicit regions to hand off to the sdk\r\n* sdk: \"use the sdk chain\"\r\n* ec2: ", "qna": [{"question": "What is the issue title?", "answer": "S3A: add explicit \"sdk\" and \"ec2\" regions for region resolution through SDK and EC2 IAM"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19739", "project": "HADOOP", "title": "JUnit5: scale test timeouts in hadoop-aws and hadoop-azure are too low", "status": "Open", "reporter": "Steve Loughran", "created": "2025-11-04T15:06:27.000+0000", "description": " For the move to JUnit 5, we went from an @Timeout method which could be overridden and was extended in scale tests to a fixed class attribute\r\n\r\nthis can cause timeouts of slow tests, especially on parallel runs where each  test thread gets less cpu time.\r\n\r\nAdding a longer timeout to the scale test base is the obvious fix, with all the dynamic timeout calculation being cut to avoid anyone thinking it actually works. That is: unless someone knows how to restore it?", "comments": [], "labels": [], "summary": " For the move to JUnit 5, we went from an @Timeout method which could be overrid", "qna": [{"question": "What is the issue title?", "answer": "JUnit5: scale test timeouts in hadoop-aws and hadoop-azure are too low"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19738", "project": "HADOOP", "title": "Upgrade to hadoop-thirdparty 1.5.0", "status": "Resolved", "reporter": "Steve Loughran", "created": "2025-11-04T14:32:11.000+0000", "description": "Upgrade to the new hadoop thirdparty jar and the newer guava release", "comments": ["steveloughran opened a new pull request, #8055:\nURL: https://github.com/apache/hadoop/pull/8055\n\n   \r\n   ### Description of PR\r\n   \r\n   HADOOP-19738. Upgrade to hadoop-thirdparty 1.5.0\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   recompilation during release process.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran merged PR #8055:\nURL: https://github.com/apache/hadoop/pull/8055\n\n\n", "steveloughran opened a new pull request, #8069:\nURL: https://github.com/apache/hadoop/pull/8069\n\n   This brings the shaded version of Guava, the one used by Hadoop, to 33.4.8-jre\r\n   \r\n   Contributed by Steve Loughran\r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8069:\nURL: https://github.com/apache/hadoop/pull/8069#issuecomment-3512914746\n\n   will merge if all is good\n\n\n", "steveloughran merged PR #8069:\nURL: https://github.com/apache/hadoop/pull/8069\n\n\n"], "labels": ["pull-request-available"], "summary": "Upgrade to the new hadoop thirdparty jar and the newer guava release", "qna": [{"question": "What is the issue title?", "answer": "Upgrade to hadoop-thirdparty 1.5.0"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19737", "project": "HADOOP", "title": "ABFS: Add metrics to identify improvements with read and write aggressiveness", "status": "Open", "reporter": "Anmol Asrani", "created": "2025-11-04T12:09:06.000+0000", "description": "Introduces new performance metrics in the ABFS driver to monitor and evaluate the effectiveness of read and write aggressiveness tuning. These metrics help in understanding how thread pool behavior, CPU utilization, and heap availability impact overall I/O throughput and latency. By capturing detailed statistics such as active thread count, pool size, and system resource utilization, this enhancement enables data-driven analysis of optimizations made to improve ABFS read and write performance under varying workloads.", "comments": ["anmolanmol1234 opened a new pull request, #8056:\nURL: https://github.com/apache/hadoop/pull/8056\n\n   Introduces new performance metrics in the ABFS driver to monitor and evaluate the effectiveness of read and write aggressiveness tuning. These metrics help in understanding how thread pool behavior, CPU utilization, and heap availability impact overall I/O throughput and latency. By capturing detailed statistics such as active thread count, pool size, and system resource utilization, this enhancement enables data-driven analysis of optimizations made to improve ABFS read and write performance under varying workloads.\n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3490113903\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/1/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 55s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 26 new + 2 unchanged - 0 fixed = 28 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 21 new + 1544 unchanged - 0 fixed = 1565 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 9 new + 1437 unchanged - 0 fixed = 1446 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/1/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 178 unchanged - 0 fixed = 180 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 15s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  61m 13s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.AbfsReadThreadPoolMetrics.getUpdatedAtLeastOnce() may expose internal representation by returning AbfsReadThreadPoolMetrics.updatedAtLeastOnce  At AbfsReadThreadPoolMetrics.java:by returning AbfsReadThreadPoolMetrics.updatedAtLeastOnce  At AbfsReadThreadPoolMetrics.java:[line 132] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.AbfsWriteThreadPoolMetrics.getUpdatedAtLeastOnce() may expose internal representation by returning AbfsWriteThreadPoolMetrics.updatedAtLeastOnce  At AbfsWriteThreadPoolMetrics.java:by returning AbfsWriteThreadPoolMetrics.updatedAtLeastOnce  At AbfsWriteThreadPoolMetrics.java:[line 136] |\r\n   | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsOutputStream |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 4f0cefa11539 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e1d1570e654ceb7189cb8cbe9e56fd470c891626 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/1/testReport/ |\r\n   | Max. process+thread count | 634 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3491576442\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/2/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  15m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  15m 37s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 19s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 20 new + 1544 unchanged - 0 fixed = 1564 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 8 new + 1437 unchanged - 0 fixed = 1445 total (was 1437)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  15m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  9s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  63m  3s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3be4c2201150 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3198f84d57c9e595c60a170d8389e4d05ee246fb |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/2/testReport/ |\r\n   | Max. process+thread count | 743 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3497380583\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/3/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 20s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 4 unchanged - 0 fixed = 6 total (was 4)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 19s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 22 new + 1544 unchanged - 0 fixed = 1566 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 10 new + 1437 unchanged - 0 fixed = 1447 total (was 1437)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  9s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  59m 41s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a44e69c020b4 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / aa15783a814f8847a6336b44929b817ad20555cb |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/3/testReport/ |\r\n   | Max. process+thread count | 635 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3497583272\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 26s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  1s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/4/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 19s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 4 unchanged - 0 fixed = 6 total (was 4)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 22 new + 1544 unchanged - 0 fixed = 1566 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 10 new + 1437 unchanged - 0 fixed = 1447 total (was 1437)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 13s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  60m 45s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 85504ab4b30d 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bc3dd5425a9958648d729fbc5b5fd3734067b701 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/4/testReport/ |\r\n   | Max. process+thread count | 612 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/4/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3515807497\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  20m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 44s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/5/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 20s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/5/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 22 new + 1544 unchanged - 0 fixed = 1566 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 15s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/5/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 10 new + 1437 unchanged - 0 fixed = 1447 total (was 1437)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 14s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  59m  2s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 9be6cffb49bc 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1c444bc9c062a8dde6f02776a4750947235c0dd6 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/5/testReport/ |\r\n   | Max. process+thread count | 616 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/5/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "manika137 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2517799307\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsCountersImpl.java:\n##########\n@@ -267,6 +298,22 @@ public AbfsReadFooterMetrics getAbfsReadFooterMetrics() {\n     return abfsReadFooterMetrics != null ? abfsReadFooterMetrics : null;\n   }\n \n+  /**\n+   * Returns the write thread pool metrics instance, or {@code null} if uninitialized.\n+   */\n+  @Override\n+  public AbfsWriteThreadPoolMetrics getAbfsWriteThreadPoolMetrics() {\n+    return abfsWriteThreadPoolMetrics != null ? abfsWriteThreadPoolMetrics : null;\n\nReview Comment:\n   we can have it as- return abfsWriteThreadPoolMetrics;\r\n   Same for getAbfsReadThreadPoolMetrics() below\n\n\n\n", "manika137 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2537251184\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -255,19 +289,43 @@ private double getCpuUtilization() {\n     OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean(\n         OperatingSystemMXBean.class);\n     double cpuLoad = osBean.getSystemCpuLoad();\n-    if (cpuLoad < 0) {\n+    if (cpuLoad < ZERO) {\n       LOG.warn(\"System CPU load value unavailable (returned -1.0). Defaulting to 0.0.\");\n-      return 0.0;\n+      return ZERO_D;\n     }\n     return cpuLoad;\n   }\n \n+  /**\n+   * Returns the CPU utilization of the JVM process as a percentage (0\u2013100).\n+   */\n+  public static double getJvmCpuUtilization() {\n+    OperatingSystemMXBean osBean =\n+        (OperatingSystemMXBean) ManagementFactory.getOperatingSystemMXBean();\n+    long cpuTime = osBean.getProcessCpuTime();\n+    long now = System.nanoTime();\n+    if (lastTime == ZERO) {\n+      lastCpuTime = cpuTime;\n+      lastTime = now;\n+      return 0.0; // first call has no previous data\n\nReview Comment:\n   nit: we can use ZERO_D here too\n\n\n\n", "manika137 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2537271997\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -363,11 +443,18 @@ public ScheduledExecutorService getCpuMonitorExecutor() {\n     return cpuMonitorExecutor;\n   }\n \n+  /**\n+   * Checks if monitoring has started.\n+   *\n+   * @return true if monitoring has started, false otherwise.\n+   */\n+  public synchronized boolean isMonitoringStarted() {\n+    return isMonitoringStarted;\n+  }\n+\n   /**\n    * Closes this manager by shutting down executors and cleaning up resources.\n    * Removes the instance from the active manager map.\n-   *\n-   * @throws IOException if an error occurs during shutdown.\n\nReview Comment:\n   we are throwing this exception right?\n\n\n\n", "manika137 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2537331384\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -394,4 +481,99 @@ public void close() throws IOException {\n       }\n     }\n   }\n+\n+  /**\n+   * Represents current statistics of the write thread pool and system.\n+   */\n+  public static class WriteThreadPoolStats {\n+    private final int currentPoolSize;   // matches CURRENT_POOL_SIZE metric\n+    private final int maxPoolSize;       // matches MAX_POOL_SIZE metric\n+    private final int activeThreads;     // matches ACTIVE_THREADS metric\n+    private final double jvmCpuUtilization; // matches JVM_CPU_UTILIZATION metric\n+    private final double cpuUtilization; // matches CPU_UTILIZATION metric\n+    private final long availableHeapGB;  // matches MEMORY_UTILIZATION metric\n+\n+    /**\n+     * Constructs a {@link WriteThreadPoolStats} instance with the given thread pool\n+     * and system utilization metrics.\n+     *\n+     * @param currentPoolSize     the current number of threads in the pool.\n+     * @param maxPoolSize         the maximum allowed thread pool size.\n+     * @param activeThreads       the number of currently active threads.\n+     * @param jvmCpuUtilization   the JVM CPU utilization percentage.\n+     * @param cpuUtilization      the overall system CPU utilization percentage.\n+     * @param availableHeapGB     the available heap memory in gigabytes.\n+     */\n+    public WriteThreadPoolStats(int currentPoolSize, int maxPoolSize,\n+        int activeThreads, double jvmCpuUtilization, double cpuUtilization, long availableHeapGB) {\n+      this.currentPoolSize = currentPoolSize;\n+      this.maxPoolSize = maxPoolSize;\n+      this.activeThreads = activeThreads;\n+      this.jvmCpuUtilization = jvmCpuUtilization;\n+      this.cpuUtilization = cpuUtilization;\n+      this.availableHeapGB = availableHeapGB;\n+    }\n+\n+    /** @return the current number of threads in the pool. */\n+    public int getCurrentPoolSize() {\n+      return currentPoolSize;\n+    }\n+\n+    /** @return the maximum allowed size of the thread pool. */\n+    public int getMaxPoolSize() {\n+      return maxPoolSize;\n+    }\n+\n+    /** @return the number of threads currently executing tasks. */\n+    public int getActiveThreads() {\n+      return activeThreads;\n+    }\n+\n+    /** @return the JVM process CPU utilization percentage. */\n+    public double getJvmCpuUtilization() {\n+      return jvmCpuUtilization;\n+    }\n+\n+    /** @return the overall system CPU utilization percentage. */\n+    public double getCpuUtilization() {\n+      return cpuUtilization;\n+    }\n+\n+    /** @return the available heap memory in gigabytes. */\n+    public long getMemoryUtilization() {\n+      return availableHeapGB;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return String.format(\n+          \"currentPoolSize=%d, maxPoolSize=%d, activeThreads=%d, jvmCpuUtilization=%.2f%%, cpuUtilization=%.2f%%, availableHeap=%dGB\",\n+          currentPoolSize, maxPoolSize, activeThreads, jvmCpuUtilization,  cpuUtilization * HUNDRED, availableHeapGB);\n\nReview Comment:\n   we dont need to multiply jvmCpuUtilization also by 100?\r\n   Also, should we be multiplying with HUNDRED_D instead?\n\n\n\n", "manika137 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2540424700\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -920,7 +920,7 @@ public void testPrefetchReadAddsPriorityHeaderWithDifferentConfigs()\n \n     Configuration configuration2 = new Configuration(getRawConfiguration());\n     //use the default value for the config: false\n\nReview Comment:\n   we can remove the comment as well. The default traffic priority value is true now\n\n\n\n", "manika137 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2540434237\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -1021,6 +1070,30 @@ public double getCpuLoad() {\n     return cpuLoad;\n   }\n \n+  /**\n+   * Returns the CPU utilization of the JVM process as a percentage (0\u2013100).\n+   */\n+  public static double getJvmCpuUtilization() {\n+    OperatingSystemMXBean osBean =\n+        (OperatingSystemMXBean) ManagementFactory.getOperatingSystemMXBean();\n+    long cpuTime = osBean.getProcessCpuTime();\n+    long now = System.nanoTime();\n+    if (lastTime == ZERO) {\n+      lastCpuTime = cpuTime;\n+      lastTime = now;\n+      return 0.0; // first call has no previous data\n\nReview Comment:\n   nit: we can use ZERO_D\n\n\n\n", "manika137 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2540449416\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java:\n##########\n@@ -256,7 +261,7 @@ public void verifyWriteRequest() throws Exception {\n    */\n   @Test\n   public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n-\n+    AbfsCounters abfsCounters = Mockito.spy(new AbfsCountersImpl(new URI(\"abcd\")));\n\nReview Comment:\n   we can have a constant for URI \"abcd\"\n\n\n\n", "manika137 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2540450852\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java:\n##########\n@@ -256,7 +261,7 @@ public void verifyWriteRequest() throws Exception {\n    */\n   @Test\n   public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n-\n+    AbfsCounters abfsCounters = Mockito.spy(new AbfsCountersImpl(new URI(\"abcd\")));\n\nReview Comment:\n   if creating mock client handlers, client, abfsrestop, spying abfscounters is repeated- then we could have it as a helper method maybe\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2545918450\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -920,7 +920,7 @@ public void testPrefetchReadAddsPriorityHeaderWithDifferentConfigs()\n \n     Configuration configuration2 = new Configuration(getRawConfiguration());\n     //use the default value for the config: false\n\nReview Comment:\n   taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -1021,6 +1070,30 @@ public double getCpuLoad() {\n     return cpuLoad;\n   }\n \n+  /**\n+   * Returns the CPU utilization of the JVM process as a percentage (0\u2013100).\n+   */\n+  public static double getJvmCpuUtilization() {\n+    OperatingSystemMXBean osBean =\n+        (OperatingSystemMXBean) ManagementFactory.getOperatingSystemMXBean();\n+    long cpuTime = osBean.getProcessCpuTime();\n+    long now = System.nanoTime();\n+    if (lastTime == ZERO) {\n+      lastCpuTime = cpuTime;\n+      lastTime = now;\n+      return 0.0; // first call has no previous data\n\nReview Comment:\n   removed this piece of code\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java:\n##########\n@@ -256,7 +261,7 @@ public void verifyWriteRequest() throws Exception {\n    */\n   @Test\n   public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n-\n+    AbfsCounters abfsCounters = Mockito.spy(new AbfsCountersImpl(new URI(\"abcd\")));\n\nReview Comment:\n   if creating mock client handlers, client, abfsrestop, spying abfscounters is repeated- then we could have it as a helper method maybe -> was existing code will take it up later\n\n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3558073816\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 45s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  15m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  15m 17s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 14s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 14s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 14s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 14s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 14s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 11 new + 4 unchanged - 0 fixed = 15 total (was 4)  |\r\n   | -1 :x: |  mvnsite  |   0m 14s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 14s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 14s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  spotbugs  |   0m 14s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  17m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 16s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  asflicense  |   0m 18s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  60m 25s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 9cbdbc30e9cc 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 312bfef35f5f540429e55824474da9856fbbe81e |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/testReport/ |\r\n   | Max. process+thread count | 610 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/6/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3558092627\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 46s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  15m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  15m 35s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 14s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 14s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 14s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 14s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 14s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 11 new + 4 unchanged - 0 fixed = 15 total (was 4)  |\r\n   | -1 :x: |  mvnsite  |   0m 16s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 14s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 13s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  spotbugs  |   0m 15s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  16m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 17s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  asflicense  |   0m 17s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  60m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3246317e80c2 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 312bfef35f5f540429e55824474da9856fbbe81e |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/testReport/ |\r\n   | Max. process+thread count | 611 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/7/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3558184233\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 46s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 6 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/8/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  15m 34s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  15m 47s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 5 unchanged - 0 fixed = 9 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 15s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/8/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 32 new + 1544 unchanged - 0 fixed = 1576 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/8/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 10 new + 1437 unchanged - 0 fixed = 1447 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/8/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 178 unchanged - 0 fixed = 180 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 11s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 11s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 19s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/8/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  62m 56s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.AbfsCountersImpl.getAbfsReadThreadPoolMetrics() may expose internal representation by returning AbfsCountersImpl.abfsReadThreadPoolMetrics  At AbfsCountersImpl.java:by returning AbfsCountersImpl.abfsReadThreadPoolMetrics  At AbfsCountersImpl.java:[line 314] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.AbfsCountersImpl.getAbfsWriteThreadPoolMetrics() may expose internal representation by returning AbfsCountersImpl.abfsWriteThreadPoolMetrics  At AbfsCountersImpl.java:by returning AbfsCountersImpl.abfsWriteThreadPoolMetrics  At AbfsCountersImpl.java:[line 306] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2a6ed6c4058f 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5ebcefaf72c7b554ba116d7818f62103cf68b0ee |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/8/testReport/ |\r\n   | Max. process+thread count | 637 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/8/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3561874110\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 6 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  20m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/9/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 14s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/9/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 5 unchanged - 0 fixed = 9 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/9/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 32 new + 1544 unchanged - 0 fixed = 1576 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/9/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 10 new + 1437 unchanged - 0 fixed = 1447 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/9/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 178 unchanged - 0 fixed = 180 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 20s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/9/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  59m 34s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.AbfsCountersImpl.getAbfsReadThreadPoolMetrics() may expose internal representation by returning AbfsCountersImpl.abfsReadThreadPoolMetrics  At AbfsCountersImpl.java:by returning AbfsCountersImpl.abfsReadThreadPoolMetrics  At AbfsCountersImpl.java:[line 314] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.AbfsCountersImpl.getAbfsWriteThreadPoolMetrics() may expose internal representation by returning AbfsCountersImpl.abfsWriteThreadPoolMetrics  At AbfsCountersImpl.java:by returning AbfsCountersImpl.abfsWriteThreadPoolMetrics  At AbfsCountersImpl.java:[line 306] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 29461eb4ad88 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1fe8cba1540704389aba6eaed187e588054b11a6 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/9/testReport/ |\r\n   | Max. process+thread count | 617 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/9/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3562069094\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 6 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  20m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/10/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 24s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/10/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 5 unchanged - 0 fixed = 7 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/10/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 30 new + 1544 unchanged - 0 fixed = 1574 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/10/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 8 new + 1437 unchanged - 0 fixed = 1445 total (was 1437)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 34s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 12s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 20s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/10/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  60m 17s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 3ff71ce1e0cb 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fad1d5a5360fbf89d4917b022eb0df1e87001924 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/10/testReport/ |\r\n   | Max. process+thread count | 636 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/10/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3562694172\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 36s |  |  https://github.com/apache/hadoop/pull/8056 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/11/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2549241207\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/enums/AbfsWriteThreadPoolMetricsEnum.java:\n##########\n@@ -0,0 +1,95 @@\n+/**\n\nReview Comment:\n   Do we need 2 searate classes for these?\n   All the metrics seems to be common in both.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -104,14 +109,39 @@ public final class ReadBufferManagerV2 extends ReadBufferManager {\n \n   private static AtomicBoolean isConfigured = new AtomicBoolean(false);\n \n+  /* Metrics collector for monitoring the performance of the ABFS read thread pool.  */\n+  private final AbfsReadThreadPoolMetrics readThreadPoolMetrics;\n+\n+  /* Last recorded CPU time used for computing CPU utilization deltas.  */\n+  private static long lastCpuTime = 0;\n+\n+  /* Last recorded system time used for utilization calculations.  */\n+  private static long lastTime = 0;\n+\n+  private final AbfsClient abfsClient;\n+  /* Tracks the last scale direction applied, or empty if none. */\n+  private volatile String lastScaleDirection = EMPTY_STRING;\n+  /* Maximum CPU utilization observed during the monitoring interval. */\n+  private volatile double maxCpuUtilization = 0.0;\n+\n   /**\n-   * Private constructor to prevent instantiation as this needs to be singleton.\n+   * Initializes a new instance of {@code ReadBufferManagerV2} for the given ABFS client.\n+   *\n+   * @param abfsClient the {@link AbfsClient} used for managing read operations.\n    */\n-  private ReadBufferManagerV2() {\n+  private ReadBufferManagerV2(AbfsClient abfsClient) {\n+    this.abfsClient = abfsClient;\n\nReview Comment:\n   Why do we need client here?\n   IMO we should not pass client to RBM.\n   Any client related work should happen in AbfsInputStream how its happening today.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1346,7 +1346,13 @@ public AbfsRestOperation read(final String path,\n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     String sasTokenForReuse = appendSASTokenToQuery(path, SASTokenProvider.READ_OPERATION,\n         abfsUriQueryBuilder, cachedSasToken);\n-\n+    // Retrieve the read thread pool metrics from the ABFS counters.\n+    AbfsReadThreadPoolMetrics metrics = getAbfsCounters()\n\nReview Comment:\n   Common code in Blob and Dfs client Can be a common method in base class.\n   \n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -104,14 +109,39 @@ public final class ReadBufferManagerV2 extends ReadBufferManager {\n \n   private static AtomicBoolean isConfigured = new AtomicBoolean(false);\n \n+  /* Metrics collector for monitoring the performance of the ABFS read thread pool.  */\n+  private final AbfsReadThreadPoolMetrics readThreadPoolMetrics;\n+\n+  /* Last recorded CPU time used for computing CPU utilization deltas.  */\n+  private static long lastCpuTime = 0;\n+\n+  /* Last recorded system time used for utilization calculations.  */\n+  private static long lastTime = 0;\n+\n+  private final AbfsClient abfsClient;\n+  /* Tracks the last scale direction applied, or empty if none. */\n+  private volatile String lastScaleDirection = EMPTY_STRING;\n+  /* Maximum CPU utilization observed during the monitoring interval. */\n+  private volatile double maxCpuUtilization = 0.0;\n+\n   /**\n-   * Private constructor to prevent instantiation as this needs to be singleton.\n+   * Initializes a new instance of {@code ReadBufferManagerV2} for the given ABFS client.\n\nReview Comment:\n   All the clients in JVM ae supposed to use same ReadBufferManager. This comment seems wrong\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureDFSIngressHandler.java:\n##########\n@@ -117,6 +117,13 @@ protected AbfsRestOperation remoteWrite(AbfsBlock blockToUpload,\n       AppendRequestParameters reqParams,\n       TracingContext tracingContext) throws IOException {\n     TracingContext tracingContextAppend = new TracingContext(tracingContext);\n+    // Fetches write thread pool metrics from the ABFS client and adds them to the tracing context.\n\nReview Comment:\n   Common code Can be moved to base IngressHandler\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java:\n##########\n@@ -242,6 +253,18 @@ public AzureIngressHandler getIngressHandler() {\n \n   private volatile boolean switchCompleted = false;\n \n+  /**\n+   * Starts CPU monitoring in the thread pool size manager if it\n+   * is initialized and not already monitoring.\n+   */\n+  private void initializeMonitoringIfNeeded() {\n+    if (poolSizeManager != null && !poolSizeManager.isMonitoringStarted()) {\n\nReview Comment:\n   Do we need to repeat this check inside synchronized block as well?\n   What if 2 threads check together and both try to startCPUMonitoring?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1346,7 +1346,13 @@ public AbfsRestOperation read(final String path,\n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     String sasTokenForReuse = appendSASTokenToQuery(path, SASTokenProvider.READ_OPERATION,\n         abfsUriQueryBuilder, cachedSasToken);\n-\n+    // Retrieve the read thread pool metrics from the ABFS counters.\n\nReview Comment:\n   Don't we need a similar change in append as well?\n\n\n\n##########\nhadoop-tools/hadoop-azure/pom.xml:\n##########\n@@ -632,6 +632,8 @@\n                     <exclude>**/azurebfs/ITestSmallWriteOptimization.java</exclude>\n                     <exclude>**/azurebfs/ITestAbfsStreamStatistics*.java</exclude>\n                     <exclude>**/azurebfs/services/ITestReadBufferManager.java</exclude>\n+                    <exclude>**/azurebfs/WriteThreadPoolSizeManager.java</exclude>\n\nReview Comment:\n   Why this change?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsWriteThreadPoolMetrics.java:\n##########\n@@ -0,0 +1,165 @@\n+/**\n\nReview Comment:\n   Same here.\n   A lot ff redundant code can be combined.\n\n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3562931177\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 38s |  |  https://github.com/apache/hadoop/pull/8056 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/12/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3563067662\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 36s |  |  https://github.com/apache/hadoop/pull/8056 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/13/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3563987033\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 37s |  |  https://github.com/apache/hadoop/pull/8056 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/14/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#issuecomment-3564300158\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 48s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 6 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/15/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 24s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/15/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 5 unchanged - 0 fixed = 9 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 20s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/15/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 33 new + 1544 unchanged - 0 fixed = 1577 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/15/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 9 new + 1437 unchanged - 0 fixed = 1446 total (was 1437)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 14s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 19s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/15/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  62m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/15/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8056 |\r\n   | JIRA Issue | HADOOP-19737 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 76c390687dc8 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c479a5408b5afe4c3ebc1c1d2e404aeba532f4ab |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/15/testReport/ |\r\n   | Max. process+thread count | 795 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8056/15/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2549506028\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1346,7 +1346,13 @@ public AbfsRestOperation read(final String path,\n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     String sasTokenForReuse = appendSASTokenToQuery(path, SASTokenProvider.READ_OPERATION,\n         abfsUriQueryBuilder, cachedSasToken);\n-\n+    // Retrieve the read thread pool metrics from the ABFS counters.\n\nReview Comment:\n   Don't we need a similar change in append as well?\n\n\n\n", "anujmodi2021 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2554604616\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -244,9 +243,12 @@ public void constructHeader(AbfsHttpOperation httpOperation, String previousFail\n     if (listener != null) { //for testing\n       listener.callTracingHeaderValidator(header, format);\n     }\n-    httpOperation.setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID, header);\n+    // If metricHeader is present, append it to the client request ID header for tracing\n     if (!metricHeader.equals(EMPTY_STRING)) {\n-      httpOperation.setRequestProperty(HttpHeaderConfigurations.X_MS_FECLIENT_METRICS, metricHeader);\n+      httpOperation.setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID, header + COLON + metricHeader);\n\nReview Comment:\n   This would cause disparity in schema. With v2 we have made every field mandatory. If metrics are not there we should simply keep empty string but colon should always be there.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -398,4 +400,12 @@ public void setReadType(ReadType readType) {\n   public ReadType getReadType() {\n     return readType;\n   }\n+\n+  /**\n+   * Sets the metric results string used for tracing or logging.\n+   * @param metricResults the formatted metric data to store.\n+   */\n+  public void setMetricResults(final String metricResults) {\n\nReview Comment:\n   These are only for Thread Pool metrics right?\n   Let's rename accordingly.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n+        lastScaleDirection = \"+F\";   // Already full, cannot scale up\n+      } else {\n+        lastScaleDirection = \"I\";    // Normal scale-up\n+      }\n       // Create new Worker Threads\n-      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n-        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n-        workerRefs.add(worker);\n-        workerPool.submit(worker);\n+      if (\"I\".equals(lastScaleDirection)) {\n\nReview Comment:\n   Let's define constants.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n\nReview Comment:\n   There should be a check on currentPoolSize as well. new can become max even after upscale\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -38,7 +38,16 @@ public enum TracingHeaderVersion {\n    *         :primaryRequestId:streamId:opType:retryHeader:ingressHandler\n    *         :position:operatedBlobCount:operationSpecificHeader:httpOperationHeader\n    */\n-  V1(\"v1\", 13);\n+  V1(\"v1\", 13),\n+  /**\n+   * Version 1 of the tracing header, which includes a version prefix and has 13 permanent fields.\n+   * This version is used for the current tracing header schema.\n+   * Schema: version:clientCorrelationId:clientRequestId:fileSystemId\n+   *         :primaryRequestId:streamId:opType:retryHeader:ingressHandler\n+   *         :position:operatedBlobCount:operationSpecificHeader:httpOperationHeader\n+   *         :networkLibrary:operationMetrics\n\nReview Comment:\n   Why 15?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamContext.java:\n##########\n@@ -328,6 +338,10 @@ public AbfsClientHandler getClientHandler() {\n     return clientHandler;\n   }\n \n+  public WriteThreadPoolSizeManager getPoolSizeManager() {\n\nReview Comment:\n   Nit: `getWriteThreadPoolSizeManager`\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -105,14 +110,39 @@ public final class ReadBufferManagerV2 extends ReadBufferManager {\n \n   private static AtomicBoolean isConfigured = new AtomicBoolean(false);\n \n+  /* Metrics collector for monitoring the performance of the ABFS read thread pool.  */\n+  private final AbfsReadThreadPoolMetrics readThreadPoolMetrics;\n+\n+  /* Last recorded CPU time used for computing CPU utilization deltas.  */\n+  private static long lastCpuTime = 0;\n\nReview Comment:\n   Remove no longer used variables\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1346,7 +1346,13 @@ public AbfsRestOperation read(final String path,\n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     String sasTokenForReuse = appendSASTokenToQuery(path, SASTokenProvider.READ_OPERATION,\n         abfsUriQueryBuilder, cachedSasToken);\n-\n+    // Retrieve the read thread pool metrics from the ABFS counters.\n+    AbfsReadThreadPoolMetrics metrics = getAbfsCounters()\n\nReview Comment:\n   Does `getAbfsReadThreadPoolMetrics()` returns aggregated metrics?\n   What defines the period over which these metrics are aggregated?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java:\n##########\n@@ -242,6 +253,18 @@ public AzureIngressHandler getIngressHandler() {\n \n   private volatile boolean switchCompleted = false;\n \n+  /**\n+   * Starts CPU monitoring in the thread pool size manager if it\n+   * is initialized and not already monitoring.\n+   */\n+  private void initializeMonitoringIfNeeded() {\n+    if (poolSizeManager != null && !poolSizeManager.isMonitoringStarted()) {\n+      synchronized (this) {\n+        poolSizeManager.startCPUMonitoring();\n+      }\n+    }\n+  }\n\nReview Comment:\n   There was a discussion to accomodate stopping of resource utilization as well if all output streams are closed. Have we taken that?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n+        lastScaleDirection = \"+F\";   // Already full, cannot scale up\n+      } else {\n+        lastScaleDirection = \"I\";    // Normal scale-up\n\nReview Comment:\n   Nit: Redundant, Already set above.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/enums/AbfsReadThreadPoolMetricsEnum.java:\n##########\n@@ -0,0 +1,95 @@\n+/**\n\nReview Comment:\n   All of these metrics might be a overhead for some customer who does not wish to use new design for read and write and they might not wish to get these metrics.\n   \n   If not already done, can we put this change behind a config so that these metrics comes only when enabled.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsCounters.java:\n##########\n@@ -83,5 +83,14 @@ String formString(String prefix, String separator, String suffix,\n \n   AbfsReadFooterMetrics getAbfsReadFooterMetrics();\n \n+  void initializeReadMetrics();\n\nReview Comment:\n   Nit: rename to accomodate the fact that these are resource management related.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -1097,4 +1222,210 @@ private void incrementActiveBufferCount() {\n   private void decrementActiveBufferCount() {\n     numberOfActiveBuffers.getAndDecrement();\n   }\n+\n+  /**\n+   * Returns the process ID (PID) of the currently running JVM.\n+   * This method uses {@link ProcessHandle#current()} to obtain the ID of the\n+   * Java process.\n+   *\n+   * @return the PID of the current JVM process\n+   */\n+  public long getJvmProcessId() {\n+    return ProcessHandle.current().pid();\n+  }\n+\n+\n+  /**\n+   * Represents current statistics of the read thread pool and system.\n+   */\n+  public static class ReadThreadPoolStats {\n\nReview Comment:\n   These again seems to be common for both read and write. We can Move out of this file as `ResourceUtilizationMetrics`\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n+        lastScaleDirection = \"+F\";   // Already full, cannot scale up\n+      } else {\n+        lastScaleDirection = \"I\";    // Normal scale-up\n+      }\n       // Create new Worker Threads\n-      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n-        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n-        workerRefs.add(worker);\n-        workerPool.submit(worker);\n+      if (\"I\".equals(lastScaleDirection)) {\n+        for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+          ReadBufferWorker worker = new ReadBufferWorker(i,\n+              getBufferManager(abfsClient));\n+          workerRefs.add(worker);\n+          workerPool.submit(worker);\n+        }\n       }\n+      // Capture the latest thread pool statistics (pool size, CPU, memory, etc.)\n+      ReadThreadPoolStats stats = getCurrentStats(cpuLoad, maxCpuUtilization);\n+      // Update the read thread pool metrics with the latest statistics snapshot.\n+      readThreadPoolMetrics.update(stats);\n       printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize,\n           newThreadPoolSize);\n+    } else if (cpuLoad < cpuThreshold && currentPoolSize > requiredPoolSize) {\n+      lastScaleDirection = \"NA\";\n+      // Capture the latest thread pool statistics (pool size, CPU, memory, etc.)\n+      ReadThreadPoolStats stats = getCurrentStats(cpuLoad, maxCpuUtilization);\n+      // Update the read thread pool metrics with the latest statistics snapshot.\n+      readThreadPoolMetrics.update(stats);\n     } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n       newThreadPoolSize = Math.max(minThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D - threadPoolDownscalePercentage))\n                   / HUNDRED_D));\n-      // Signal the extra workers to stop\n-      while (workerRefs.size() > newThreadPoolSize) {\n-        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n-        worker.stop();\n+      if (newThreadPoolSize == minThreadPoolSize) {\n\nReview Comment:\n   Same here. check for current also needed\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n+        lastScaleDirection = \"+F\";   // Already full, cannot scale up\n+      } else {\n+        lastScaleDirection = \"I\";    // Normal scale-up\n+      }\n       // Create new Worker Threads\n-      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n-        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n-        workerRefs.add(worker);\n-        workerPool.submit(worker);\n+      if (\"I\".equals(lastScaleDirection)) {\n+        for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+          ReadBufferWorker worker = new ReadBufferWorker(i,\n+              getBufferManager(abfsClient));\n+          workerRefs.add(worker);\n+          workerPool.submit(worker);\n+        }\n       }\n+      // Capture the latest thread pool statistics (pool size, CPU, memory, etc.)\n+      ReadThreadPoolStats stats = getCurrentStats(cpuLoad, maxCpuUtilization);\n\nReview Comment:\n   Nit: These 2 statements are repeated in if elseif else block. They can be moved at last in common flow.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n\n\nReview Comment:\n   Do we need to make changes here as well as per the latest finding on resource utilization APIs\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -1057,6 +1171,17 @@ public ScheduledExecutorService getCpuMonitoringThread() {\n     return cpuMonitorThread;\n   }\n \n+  /**\n+   * Returns the maximum JVM CPU utilization observed during the current\n+   * monitoring interval or since the last reset.\n+   *\n+   * @return the highest JVM CPU utilization percentage recorded\n+   */\n+  @VisibleForTesting\n+  public double getMaxCpuUtilization() {\n\nReview Comment:\n   Nit: maxJvmCPUUtilization\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -38,7 +38,16 @@ public enum TracingHeaderVersion {\n    *         :primaryRequestId:streamId:opType:retryHeader:ingressHandler\n    *         :position:operatedBlobCount:operationSpecificHeader:httpOperationHeader\n    */\n-  V1(\"v1\", 13);\n+  V1(\"v1\", 13),\n+  /**\n+   * Version 1 of the tracing header, which includes a version prefix and has 13 permanent fields.\n+   * This version is used for the current tracing header schema.\n+   * Schema: version:clientCorrelationId:clientRequestId:fileSystemId\n+   *         :primaryRequestId:streamId:opType:retryHeader:ingressHandler\n+   *         :position:operatedBlobCount:operationSpecificHeader:httpOperationHeader\n+   *         :networkLibrary:operationMetrics\n\nReview Comment:\n   nit: instaed of operationMetrics, use resourceUtilizationMetrics\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -38,7 +38,16 @@ public enum TracingHeaderVersion {\n    *         :primaryRequestId:streamId:opType:retryHeader:ingressHandler\n    *         :position:operatedBlobCount:operationSpecificHeader:httpOperationHeader\n    */\n-  V1(\"v1\", 13);\n+  V1(\"v1\", 13),\n+  /**\n+   * Version 1 of the tracing header, which includes a version prefix and has 13 permanent fields.\n\nReview Comment:\n   Nit: Edit javadoc to reflect v2 \n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -154,15 +173,51 @@ private int getComputedMaxPoolSize(final int availableProcessors, long initialAv\n    *\n    * @return the available heap memory in gigabytes\n    */\n-  private long getAvailableHeapMemory() {\n+  @VisibleForTesting\n+  public long getAvailableHeapMemory() {\n\nReview Comment:\n   Common methods between read and write.\n   Can be moved to some util class\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/enums/AbfsWriteThreadPoolMetricsEnum.java:\n##########\n@@ -0,0 +1,95 @@\n+/**\n\nReview Comment:\n   Also these are not only threads but memory related metrics as well.\n   We can rename to something like: `ABFSResourceUtilizationMetrics`\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2554986880\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/enums/AbfsWriteThreadPoolMetricsEnum.java:\n##########\n@@ -0,0 +1,95 @@\n+/**\n\nReview Comment:\n   We can create a single class but in future, there may be some metrics not relevant to both of them, hence for separation of concerns felt it would be better to have 2 separate classes\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2555127500\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -104,14 +109,39 @@ public final class ReadBufferManagerV2 extends ReadBufferManager {\n \n   private static AtomicBoolean isConfigured = new AtomicBoolean(false);\n \n+  /* Metrics collector for monitoring the performance of the ABFS read thread pool.  */\n+  private final AbfsReadThreadPoolMetrics readThreadPoolMetrics;\n+\n+  /* Last recorded CPU time used for computing CPU utilization deltas.  */\n+  private static long lastCpuTime = 0;\n+\n+  /* Last recorded system time used for utilization calculations.  */\n+  private static long lastTime = 0;\n+\n+  private final AbfsClient abfsClient;\n+  /* Tracks the last scale direction applied, or empty if none. */\n+  private volatile String lastScaleDirection = EMPTY_STRING;\n+  /* Maximum CPU utilization observed during the monitoring interval. */\n+  private volatile double maxCpuUtilization = 0.0;\n+\n   /**\n-   * Private constructor to prevent instantiation as this needs to be singleton.\n+   * Initializes a new instance of {@code ReadBufferManagerV2} for the given ABFS client.\n+   *\n+   * @param abfsClient the {@link AbfsClient} used for managing read operations.\n    */\n-  private ReadBufferManagerV2() {\n+  private ReadBufferManagerV2(AbfsClient abfsClient) {\n+    this.abfsClient = abfsClient;\n\nReview Comment:\n   Passed the abfscounters directly instead of client\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2555249600\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1346,7 +1346,13 @@ public AbfsRestOperation read(final String path,\n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     String sasTokenForReuse = appendSASTokenToQuery(path, SASTokenProvider.READ_OPERATION,\n         abfsUriQueryBuilder, cachedSasToken);\n-\n+    // Retrieve the read thread pool metrics from the ABFS counters.\n+    AbfsReadThreadPoolMetrics metrics = getAbfsCounters()\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2555412134\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -104,14 +109,39 @@ public final class ReadBufferManagerV2 extends ReadBufferManager {\n \n   private static AtomicBoolean isConfigured = new AtomicBoolean(false);\n \n+  /* Metrics collector for monitoring the performance of the ABFS read thread pool.  */\n+  private final AbfsReadThreadPoolMetrics readThreadPoolMetrics;\n+\n+  /* Last recorded CPU time used for computing CPU utilization deltas.  */\n+  private static long lastCpuTime = 0;\n+\n+  /* Last recorded system time used for utilization calculations.  */\n+  private static long lastTime = 0;\n+\n+  private final AbfsClient abfsClient;\n+  /* Tracks the last scale direction applied, or empty if none. */\n+  private volatile String lastScaleDirection = EMPTY_STRING;\n+  /* Maximum CPU utilization observed during the monitoring interval. */\n+  private volatile double maxCpuUtilization = 0.0;\n+\n   /**\n-   * Private constructor to prevent instantiation as this needs to be singleton.\n+   * Initializes a new instance of {@code ReadBufferManagerV2} for the given ABFS client.\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2555642257\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java:\n##########\n@@ -242,6 +253,18 @@ public AzureIngressHandler getIngressHandler() {\n \n   private volatile boolean switchCompleted = false;\n \n+  /**\n+   * Starts CPU monitoring in the thread pool size manager if it\n+   * is initialized and not already monitoring.\n+   */\n+  private void initializeMonitoringIfNeeded() {\n+    if (poolSizeManager != null && !poolSizeManager.isMonitoringStarted()) {\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2555815660\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsWriteThreadPoolMetrics.java:\n##########\n@@ -0,0 +1,165 @@\n+/**\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2555868249\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureDFSIngressHandler.java:\n##########\n@@ -117,6 +117,13 @@ protected AbfsRestOperation remoteWrite(AbfsBlock blockToUpload,\n       AppendRequestParameters reqParams,\n       TracingContext tracingContext) throws IOException {\n     TracingContext tracingContextAppend = new TracingContext(tracingContext);\n+    // Fetches write thread pool metrics from the ABFS client and adds them to the tracing context.\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2555875754\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/enums/AbfsReadThreadPoolMetricsEnum.java:\n##########\n@@ -0,0 +1,95 @@\n+/**\n\nReview Comment:\n   already taken care of in AbfsClient class\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2555890613\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1346,7 +1346,13 @@ public AbfsRestOperation read(final String path,\n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     String sasTokenForReuse = appendSASTokenToQuery(path, SASTokenProvider.READ_OPERATION,\n         abfsUriQueryBuilder, cachedSasToken);\n-\n+    // Retrieve the read thread pool metrics from the ABFS counters.\n+    AbfsReadThreadPoolMetrics metrics = getAbfsCounters()\n\nReview Comment:\n   So, these metrics are pushed only when thread pool size changes and whenever toString() method is called whatever we have aggregated till then is pushed and then they are reinitialized. There is no concept of a period for these metrics.\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2556066243\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java:\n##########\n@@ -242,6 +253,18 @@ public AzureIngressHandler getIngressHandler() {\n \n   private volatile boolean switchCompleted = false;\n \n+  /**\n+   * Starts CPU monitoring in the thread pool size manager if it\n+   * is initialized and not already monitoring.\n+   */\n+  private void initializeMonitoringIfNeeded() {\n+    if (poolSizeManager != null && !poolSizeManager.isMonitoringStarted()) {\n+      synchronized (this) {\n+        poolSizeManager.startCPUMonitoring();\n+      }\n+    }\n+  }\n\nReview Comment:\n   Not yet\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -105,14 +110,39 @@ public final class ReadBufferManagerV2 extends ReadBufferManager {\n \n   private static AtomicBoolean isConfigured = new AtomicBoolean(false);\n \n+  /* Metrics collector for monitoring the performance of the ABFS read thread pool.  */\n+  private final AbfsReadThreadPoolMetrics readThreadPoolMetrics;\n+\n+  /* Last recorded CPU time used for computing CPU utilization deltas.  */\n+  private static long lastCpuTime = 0;\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2556076727\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamContext.java:\n##########\n@@ -328,6 +338,10 @@ public AbfsClientHandler getClientHandler() {\n     return clientHandler;\n   }\n \n+  public WriteThreadPoolSizeManager getPoolSizeManager() {\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2556101198\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsCounters.java:\n##########\n@@ -83,5 +83,14 @@ String formString(String prefix, String separator, String suffix,\n \n   AbfsReadFooterMetrics getAbfsReadFooterMetrics();\n \n+  void initializeReadMetrics();\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2556290012\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -1097,4 +1222,210 @@ private void incrementActiveBufferCount() {\n   private void decrementActiveBufferCount() {\n     numberOfActiveBuffers.getAndDecrement();\n   }\n+\n+  /**\n+   * Returns the process ID (PID) of the currently running JVM.\n+   * This method uses {@link ProcessHandle#current()} to obtain the ID of the\n+   * Java process.\n+   *\n+   * @return the PID of the current JVM process\n+   */\n+  public long getJvmProcessId() {\n+    return ProcessHandle.current().pid();\n+  }\n+\n+\n+  /**\n+   * Represents current statistics of the read thread pool and system.\n+   */\n+  public static class ReadThreadPoolStats {\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2556295471\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n+        lastScaleDirection = \"+F\";   // Already full, cannot scale up\n+      } else {\n+        lastScaleDirection = \"I\";    // Normal scale-up\n\nReview Comment:\n   taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n+        lastScaleDirection = \"+F\";   // Already full, cannot scale up\n+      } else {\n+        lastScaleDirection = \"I\";    // Normal scale-up\n+      }\n       // Create new Worker Threads\n-      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n-        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n-        workerRefs.add(worker);\n-        workerPool.submit(worker);\n+      if (\"I\".equals(lastScaleDirection)) {\n\nReview Comment:\n   taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2556297999\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n+        lastScaleDirection = \"+F\";   // Already full, cannot scale up\n+      } else {\n+        lastScaleDirection = \"I\";    // Normal scale-up\n+      }\n       // Create new Worker Threads\n-      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n-        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n-        workerRefs.add(worker);\n-        workerPool.submit(worker);\n+      if (\"I\".equals(lastScaleDirection)) {\n+        for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+          ReadBufferWorker worker = new ReadBufferWorker(i,\n+              getBufferManager(abfsClient));\n+          workerRefs.add(worker);\n+          workerPool.submit(worker);\n+        }\n       }\n+      // Capture the latest thread pool statistics (pool size, CPU, memory, etc.)\n+      ReadThreadPoolStats stats = getCurrentStats(cpuLoad, maxCpuUtilization);\n\nReview Comment:\n   these are needed only when we are scaling up or down not always hence added in blocks where scale up or down is happening\n\n\n\n", "anmolanmol1234 commented on code in PR #8056:\nURL: https://github.com/apache/hadoop/pull/8056#discussion_r2556342075\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n\nReview Comment:\n   taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -831,41 +861,79 @@ private boolean manualEviction(final ReadBuffer buf) {\n    */\n   private void adjustThreadPool() {\n     int currentPoolSize = workerRefs.size();\n-    double cpuLoad = getCpuLoad();\n+    double cpuLoad = getJvmCpuLoad();\n+    if (cpuLoad > maxCpuUtilization) {\n+      maxCpuUtilization = cpuLoad;\n+    }\n     int requiredPoolSize = getRequiredThreadPoolSize();\n     int newThreadPoolSize;\n     printTraceLog(\n         \"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\",\n         cpuLoad, currentPoolSize, requiredPoolSize);\n     if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      lastScaleDirection = \"I\";\n       // Submit more background tasks.\n       newThreadPoolSize = Math.min(maxThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D + threadPoolUpscalePercentage))\n                   / HUNDRED_D));\n+      if (newThreadPoolSize == maxThreadPoolSize) {\n+        lastScaleDirection = \"+F\";   // Already full, cannot scale up\n+      } else {\n+        lastScaleDirection = \"I\";    // Normal scale-up\n+      }\n       // Create new Worker Threads\n-      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n-        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n-        workerRefs.add(worker);\n-        workerPool.submit(worker);\n+      if (\"I\".equals(lastScaleDirection)) {\n+        for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+          ReadBufferWorker worker = new ReadBufferWorker(i,\n+              getBufferManager(abfsClient));\n+          workerRefs.add(worker);\n+          workerPool.submit(worker);\n+        }\n       }\n+      // Capture the latest thread pool statistics (pool size, CPU, memory, etc.)\n+      ReadThreadPoolStats stats = getCurrentStats(cpuLoad, maxCpuUtilization);\n+      // Update the read thread pool metrics with the latest statistics snapshot.\n+      readThreadPoolMetrics.update(stats);\n       printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize,\n           newThreadPoolSize);\n+    } else if (cpuLoad < cpuThreshold && currentPoolSize > requiredPoolSize) {\n+      lastScaleDirection = \"NA\";\n+      // Capture the latest thread pool statistics (pool size, CPU, memory, etc.)\n+      ReadThreadPoolStats stats = getCurrentStats(cpuLoad, maxCpuUtilization);\n+      // Update the read thread pool metrics with the latest statistics snapshot.\n+      readThreadPoolMetrics.update(stats);\n     } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n       newThreadPoolSize = Math.max(minThreadPoolSize,\n           (int) Math.ceil(\n               (currentPoolSize * (HUNDRED_D - threadPoolDownscalePercentage))\n                   / HUNDRED_D));\n-      // Signal the extra workers to stop\n-      while (workerRefs.size() > newThreadPoolSize) {\n-        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n-        worker.stop();\n+      if (newThreadPoolSize == minThreadPoolSize) {\n\nReview Comment:\n   taken\n\n\n\n"], "labels": ["pull-request-available"], "summary": "Introduces new performance metrics in the ABFS driver to monitor and evaluate th", "qna": [{"question": "What is the issue title?", "answer": "ABFS: Add metrics to identify improvements with read and write aggressiveness"}, {"question": "Who reported this issue?", "answer": "Anmol Asrani"}]}
{"key": "HADOOP-19736", "project": "HADOOP", "title": "ABFS: Support for new auth type: User-bound SAS", "status": "Open", "reporter": "Manika Joshi", "created": "2025-10-24T09:31:18.000+0000", "description": "Adding support for new authentication type: user bound SAS", "comments": ["manika137 opened a new pull request, #8051:\nURL: https://github.com/apache/hadoop/pull/8051\n\n   ### Description of PR\r\n   Adding support for new authentication type: user bound SAS\r\n   \r\n   ### How was this patch tested?\r\n   Test suite will be run for the patch\r\n   \r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3442714485\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 45s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 23s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 34s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/blanks-eol.txt) |  The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 36 new + 4 unchanged - 0 fixed = 40 total (was 4)  |\r\n   | -1 :x: |  mvnsite  |   0m 37s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 32s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 2 new + 1472 unchanged - 0 fixed = 1474 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 2 new + 1413 unchanged - 0 fixed = 1415 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 34s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  29m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 41s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 119m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux 88abe69cd96c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 05b52e40f1bc99edc039fc0d2ab5f83d1ceb0da9 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/testReport/ |\r\n   | Max. process+thread count | 779 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3454395745\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   4m 52s |  |  Docker failed to build run-specific yetus/hadoop:tp-4947}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3454455082\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   6m 57s |  |  Docker failed to build run-specific yetus/hadoop:tp-29291}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/3/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3454654532\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   6m  5s |  |  Docker failed to build run-specific yetus/hadoop:tp-27223}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/4/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "bhattmanish98 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2472593921\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java:\n##########\n@@ -80,6 +80,25 @@ public AbfsClientHandler(final URL baseUrl,\n         abfsClientContext);\n   }\n \n+  public AbfsClientHandler(final URL baseUrl,\n\nReview Comment:\n   Java doc missing for the constructor\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -187,7 +187,8 @@ public enum ApiVersion {\n     DEC_12_2019(\"2019-12-12\"),\n     APR_10_2021(\"2021-04-10\"),\n     AUG_03_2023(\"2023-08-03\"),\n-    NOV_04_2024(\"2024-11-04\");\n+    NOV_04_2024(\"2024-11-04\"),\n+    JULY_05_2025(\"2025-07-05\");\n\nReview Comment:\n   We should follow the same format: JUL_05_2025, what do you think?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -174,6 +174,17 @@ public AbfsDfsClient(final URL baseUrl,\n         encryptionContextProvider, abfsClientContext, AbfsServiceType.DFS);\n   }\n \n+  public AbfsDfsClient(final URL baseUrl,\n\nReview Comment:\n   Java doc missing\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -570,6 +570,11 @@ public void signRequest(final AbfsHttpOperation httpOperation, int bytesToSign)\n         // do nothing; the SAS token should already be appended to the query string\n         httpOperation.setMaskForSAS(); //mask sig/oid from url for logs\n         break;\n+      case UserboundSASWithOAuth:\n+        httpOperation.setRequestProperty(HttpHeaderConfigurations.AUTHORIZATION,\n+            client.getAccessToken());\n+        httpOperation.setMaskForSAS(); //mask sig/oid from url for logs\n\nReview Comment:\n   Typo: *sign\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/constants/TestConfigurationKeys.java:\n##########\n@@ -55,6 +55,9 @@ public final class TestConfigurationKeys {\n \n   public static final String FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID = \"fs.azure.test.app.service.principal.object.id\";\n \n+  public static final String FS_AZURE_END_USER_TENANT_ID = \"fs.azure.test.end.user.tenant.id\";\n\nReview Comment:\n   Rename the variable to FS_AZURE_TEST_END_USER_TENANT_ID \n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1741,7 +1741,15 @@ private void initializeClient(URI uri, String fileSystemName,\n     } else if (authType == AuthType.SAS) {\n       LOG.trace(\"Fetching SAS Token Provider\");\n       sasTokenProvider = abfsConfiguration.getSASTokenProvider();\n-    } else {\n+    } else if(authType == AuthType.UserboundSASWithOAuth){\n+      LOG.trace(\"Fetching SAS and OAuth Token Provider for user bound SAS\");\n+      AzureADAuthenticator.init(abfsConfiguration);\n+      tokenProvider = abfsConfiguration.getTokenProvider();\n+      ExtensionHelper.bind(tokenProvider, uri,\n+          abfsConfiguration.getRawConfiguration());\n+      sasTokenProvider = abfsConfiguration.getSASTokenProviderForUserBoundSAS();\n+    }\n\nReview Comment:\n   else can be started in the same line after }, same format we are using at other places as well.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -363,6 +363,21 @@ public AbfsClient(final URL baseUrl,\n     this.sasTokenProvider = sasTokenProvider;\n   }\n \n+  public AbfsClient(final URL baseUrl,\n\nReview Comment:\n   Java doc missing\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/constants/TestConfigurationKeys.java:\n##########\n@@ -55,6 +55,9 @@ public final class TestConfigurationKeys {\n \n   public static final String FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID = \"fs.azure.test.app.service.principal.object.id\";\n \n+  public static final String FS_AZURE_END_USER_TENANT_ID = \"fs.azure.test.end.user.tenant.id\";\n+  public static final String FS_AZURE_END_USER_OBJECT_ID = \"fs.azure.test.end.user.object.id\";\n\nReview Comment:\n   same as above\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/site/markdown/index.md:\n##########\n@@ -303,6 +303,7 @@ driven by them.\n 3. Deployed in-Azure with the Azure VMs providing OAuth 2.0 tokens to the application, \"Managed Instance\".\n 4. Using Shared Access Signature (SAS) tokens provided by a custom implementation of the SASTokenProvider interface.\n 5. By directly configuring a fixed Shared Access Signature (SAS) token in the account configuration settings files.\n+6. Using user-bound SAS auth type, which is requires OAuth 2.0 setup (point 2 above) and SAS setup (point 4 above)\n\nReview Comment:\n   Grammatical mistake: which requires or which is required?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AuthType.java:\n##########\n@@ -24,5 +24,6 @@ public enum AuthType {\n     SharedKey,\n     OAuth,\n     Custom,\n-    SAS\n+    SAS,\n+     UserboundSASWithOAuth\n\nReview Comment:\n   Format issue: There is an extra space before UserboundSASWithOAuth\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1741,7 +1741,15 @@ private void initializeClient(URI uri, String fileSystemName,\n     } else if (authType == AuthType.SAS) {\n       LOG.trace(\"Fetching SAS Token Provider\");\n       sasTokenProvider = abfsConfiguration.getSASTokenProvider();\n-    } else {\n+    } else if(authType == AuthType.UserboundSASWithOAuth){\n\nReview Comment:\n   Missing space between if and (\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1770,7 +1778,12 @@ private void initializeClient(URI uri, String fileSystemName,\n     }\n \n     LOG.trace(\"Initializing AbfsClient for {}\", baseUrl);\n-    if (tokenProvider != null) {\n+    if(tokenProvider != null && sasTokenProvider != null){\n\nReview Comment:\n   Space between if and (\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1770,7 +1778,12 @@ private void initializeClient(URI uri, String fileSystemName,\n     }\n \n     LOG.trace(\"Initializing AbfsClient for {}\", baseUrl);\n-    if (tokenProvider != null) {\n+    if(tokenProvider != null && sasTokenProvider != null){\n+      this.clientHandler = new AbfsClientHandler(baseUrl, creds, abfsConfiguration,\n+          tokenProvider, sasTokenProvider, encryptionContextProvider,\n+          populateAbfsClientContext());\n+    }\n+    else if (tokenProvider != null) {\n\nReview Comment:\n   same as above\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481295321\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/SASGenerator.java:\n##########\n@@ -41,7 +41,8 @@ public abstract class SASGenerator {\n   public enum AuthenticationVersion {\n     Nov18(\"2018-11-09\"),\n     Dec19(\"2019-12-12\"),\n-    Feb20(\"2020-02-10\");\n+    Feb20(\"2020-02-10\"),\n+    July5(\"2025-07-05\");\n\nReview Comment:\n   Same here should be JUL\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481303390\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/DelegationSASGenerator.java:\n##########\n@@ -36,20 +38,26 @@ public class DelegationSASGenerator extends SASGenerator {\n   private final String ske;\n   private final String sks = \"b\";\n   private final String skv;\n+  private final String skdutid;\n+  private final String sduoid;\n \n-  public DelegationSASGenerator(byte[] userDelegationKey, String skoid, String sktid, String skt, String ske, String skv) {\n+  public DelegationSASGenerator(byte[] userDelegationKey, String skoid, String sktid, String skt, String ske, String skv, String skdutid, String sduoid) {\n\nReview Comment:\n   add javadoc for what do all these params signify\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481307275\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/DelegationSASGenerator.java:\n##########\n@@ -117,6 +125,15 @@ public String getDelegationSAS(String accountName, String containerName, String\n     qb.addQuery(\"ske\", ske);\n     qb.addQuery(\"sks\", sks);\n     qb.addQuery(\"skv\", skv);\n+\n+    //skdutid and sduoid are required for user bound SAS only\n+    if(!Objects.equals(skdutid, EMPTY_STRING)){\n\nReview Comment:\n   spaces after if\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481327412\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/DelegationSASGenerator.java:\n##########\n@@ -197,6 +228,7 @@ private String computeSignatureForSAS(String sp, String st, String se, String sv\n \n     String stringToSign = sb.toString();\n     LOG.debug(\"Delegation SAS stringToSign: \" + stringToSign.replace(\"\\n\", \".\"));\n+    System.out.println(\"Delegation SAS stringToSign: \" + stringToSign.replace(\"\\n\", \".\"));\n\nReview Comment:\n   Remove this\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481341233\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java:\n##########\n@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.extensions;\n+\n+import java.io.IOException;\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException;\n+import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.utils.Base64;\n+import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator;\n+import org.apache.hadoop.fs.azurebfs.utils.SASGenerator;\n+import org.apache.hadoop.security.AccessControlException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT;\n+\n+/**\n+ * A mock user-bound SAS token provider implementation.\n+ */\n+\n+public class MockUserBoundSASTokenProvider implements SASTokenProvider {\n+\n+  private DelegationSASGenerator generator;\n+\n+  public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\";\n+  public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\";\n+  public static final String NO_AGENT_PATH = \"NoAgentPath\";\n+\n+  @Override\n+  public void initialize(Configuration configuration, String accountName) throws IOException {\n+    String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID);\n+    String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET);\n+    String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID);\n+    String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID);\n+    String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES));\n+    String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY));\n+    String skv = SASGenerator.AuthenticationVersion.July5.toString();\n+\n+    String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID);\n+    String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID);\n+\n+    byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid);\n+\n+    generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid);\n+  }\n+\n+  // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an\n+  // access token.  See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow.\n+  private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException {\n\nReview Comment:\n   include params in javadoc as well\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481342577\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java:\n##########\n@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.extensions;\n+\n+import java.io.IOException;\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException;\n+import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.utils.Base64;\n+import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator;\n+import org.apache.hadoop.fs.azurebfs.utils.SASGenerator;\n+import org.apache.hadoop.security.AccessControlException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT;\n+\n+/**\n+ * A mock user-bound SAS token provider implementation.\n+ */\n+\n+public class MockUserBoundSASTokenProvider implements SASTokenProvider {\n+\n+  private DelegationSASGenerator generator;\n+\n+  public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\";\n+  public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\";\n+  public static final String NO_AGENT_PATH = \"NoAgentPath\";\n+\n+  @Override\n+  public void initialize(Configuration configuration, String accountName) throws IOException {\n+    String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID);\n+    String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET);\n+    String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID);\n+    String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID);\n+    String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES));\n+    String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY));\n+    String skv = SASGenerator.AuthenticationVersion.July5.toString();\n+\n+    String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID);\n+    String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID);\n+\n+    byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid);\n+\n+    generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid);\n+  }\n+\n+  // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an\n+  // access token.  See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow.\n+  private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException {\n+    String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid);\n\nReview Comment:\n   Add a constant for the string\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481343344\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java:\n##########\n@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.extensions;\n+\n+import java.io.IOException;\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException;\n+import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.utils.Base64;\n+import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator;\n+import org.apache.hadoop.fs.azurebfs.utils.SASGenerator;\n+import org.apache.hadoop.security.AccessControlException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT;\n+\n+/**\n+ * A mock user-bound SAS token provider implementation.\n+ */\n+\n+public class MockUserBoundSASTokenProvider implements SASTokenProvider {\n+\n+  private DelegationSASGenerator generator;\n+\n+  public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\";\n+  public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\";\n+  public static final String NO_AGENT_PATH = \"NoAgentPath\";\n+\n+  @Override\n+  public void initialize(Configuration configuration, String accountName) throws IOException {\n+    String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID);\n+    String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET);\n+    String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID);\n+    String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID);\n+    String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES));\n+    String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY));\n+    String skv = SASGenerator.AuthenticationVersion.July5.toString();\n+\n+    String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID);\n+    String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID);\n+\n+    byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid);\n+\n+    generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid);\n+  }\n+\n+  // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an\n+  // access token.  See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow.\n+  private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException {\n+    String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid);\n+    ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret);\n+    return \"Bearer \" + provider.getToken().getAccessToken();\n+  }\n+\n+  private byte[] getUserDelegationKey(String accountName, String appID, String appSecret,\n\nReview Comment:\n   javadoc\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481344728\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java:\n##########\n@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.extensions;\n+\n+import java.io.IOException;\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException;\n+import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.utils.Base64;\n+import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator;\n+import org.apache.hadoop.fs.azurebfs.utils.SASGenerator;\n+import org.apache.hadoop.security.AccessControlException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT;\n+\n+/**\n+ * A mock user-bound SAS token provider implementation.\n+ */\n+\n+public class MockUserBoundSASTokenProvider implements SASTokenProvider {\n+\n+  private DelegationSASGenerator generator;\n+\n+  public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\";\n+  public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\";\n+  public static final String NO_AGENT_PATH = \"NoAgentPath\";\n+\n+  @Override\n+  public void initialize(Configuration configuration, String accountName) throws IOException {\n+    String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID);\n+    String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET);\n+    String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID);\n+    String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID);\n+    String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES));\n+    String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY));\n+    String skv = SASGenerator.AuthenticationVersion.July5.toString();\n+\n+    String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID);\n+    String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID);\n+\n+    byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid);\n+\n+    generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid);\n+  }\n+\n+  // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an\n+  // access token.  See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow.\n+  private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException {\n+    String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid);\n+    ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret);\n+    return \"Bearer \" + provider.getToken().getAccessToken();\n+  }\n+\n+  private byte[] getUserDelegationKey(String accountName, String appID, String appSecret,\n+      String sktid, String skt, String ske, String skv, String skdutid) throws IOException {\n+\n+    String method = \"POST\";\n\nReview Comment:\n   we have constants for HTTP methods, can be used here\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481347800\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java:\n##########\n@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.extensions;\n+\n+import java.io.IOException;\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException;\n+import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.utils.Base64;\n+import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator;\n+import org.apache.hadoop.fs.azurebfs.utils.SASGenerator;\n+import org.apache.hadoop.security.AccessControlException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT;\n+\n+/**\n+ * A mock user-bound SAS token provider implementation.\n+ */\n+\n+public class MockUserBoundSASTokenProvider implements SASTokenProvider {\n+\n+  private DelegationSASGenerator generator;\n+\n+  public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\";\n+  public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\";\n+  public static final String NO_AGENT_PATH = \"NoAgentPath\";\n+\n+  @Override\n+  public void initialize(Configuration configuration, String accountName) throws IOException {\n+    String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID);\n+    String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET);\n+    String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID);\n+    String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID);\n+    String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES));\n+    String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY));\n+    String skv = SASGenerator.AuthenticationVersion.July5.toString();\n+\n+    String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID);\n+    String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID);\n+\n+    byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid);\n+\n+    generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid);\n+  }\n+\n+  // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an\n+  // access token.  See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow.\n+  private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException {\n+    String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid);\n+    ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret);\n+    return \"Bearer \" + provider.getToken().getAccessToken();\n+  }\n+\n+  private byte[] getUserDelegationKey(String accountName, String appID, String appSecret,\n+      String sktid, String skt, String ske, String skv, String skdutid) throws IOException {\n+\n+    String method = \"POST\";\n+    String account = accountName.substring(0, accountName.indexOf(AbfsHttpConstants.DOT));\n+\n+    final StringBuilder sb = new StringBuilder(128);\n+    sb.append(\"https://\");\n+    sb.append(account);\n+    sb.append(\".blob.core.windows.net/?restype=service&comp=userdelegationkey\");\n\nReview Comment:\n   Try to use constants as much as possible\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2481350011\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java:\n##########\n@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.extensions;\n+\n+import java.io.IOException;\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException;\n+import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.utils.Base64;\n+import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator;\n+import org.apache.hadoop.fs.azurebfs.utils.SASGenerator;\n+import org.apache.hadoop.security.AccessControlException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT;\n+\n+/**\n+ * A mock user-bound SAS token provider implementation.\n+ */\n+\n+public class MockUserBoundSASTokenProvider implements SASTokenProvider {\n+\n+  private DelegationSASGenerator generator;\n+\n+  public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\";\n+  public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\";\n+  public static final String NO_AGENT_PATH = \"NoAgentPath\";\n+\n+  @Override\n+  public void initialize(Configuration configuration, String accountName) throws IOException {\n+    String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID);\n+    String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET);\n+    String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID);\n+    String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID);\n+    String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES));\n+    String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY));\n+    String skv = SASGenerator.AuthenticationVersion.July5.toString();\n+\n+    String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID);\n+    String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID);\n+\n+    byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid);\n+\n+    generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid);\n+  }\n+\n+  // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an\n+  // access token.  See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow.\n+  private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException {\n+    String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid);\n+    ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret);\n+    return \"Bearer \" + provider.getToken().getAccessToken();\n+  }\n+\n+  private byte[] getUserDelegationKey(String accountName, String appID, String appSecret,\n+      String sktid, String skt, String ske, String skv, String skdutid) throws IOException {\n+\n+    String method = \"POST\";\n+    String account = accountName.substring(0, accountName.indexOf(AbfsHttpConstants.DOT));\n+\n+    final StringBuilder sb = new StringBuilder(128);\n+    sb.append(\"https://\");\n+    sb.append(account);\n+    sb.append(\".blob.core.windows.net/?restype=service&comp=userdelegationkey\");\n+\n+    URL url;\n+    try {\n+      url = new URL(sb.toString());\n+    } catch (MalformedURLException ex) {\n+      throw new InvalidUriException(sb.toString());\n+    }\n+\n+    List<AbfsHttpHeader> requestHeaders = new ArrayList<AbfsHttpHeader>();\n+    requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_VERSION, skv));\n+    requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.CONTENT_TYPE, \"application/x-www-form-urlencoded\"));\n+    requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.AUTHORIZATION, getAuthorizationHeader(account, appID, appSecret, sktid)));\n+\n+    final StringBuilder requestBody = new StringBuilder(512);\n+    requestBody.append(\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><KeyInfo><Start>\");\n+    requestBody.append(skt);\n+    requestBody.append(\"</Start><Expiry>\");\n+    requestBody.append(ske);\n+    requestBody.append(\"</Expiry><DelegatedUserTid>\");\n+    requestBody.append(skdutid);\n+    requestBody.append(\"</DelegatedUserTid></KeyInfo>\");\n+\n+//    requestBody.append(\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><KeyInfo><Start>\");\n\nReview Comment:\n   Remove comments\n\n\n\n", "anujmodi2021 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2485253373\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -363,6 +363,21 @@ public AbfsClient(final URL baseUrl,\n     this.sasTokenProvider = sasTokenProvider;\n   }\n \n+  public AbfsClient(final URL baseUrl,\n\nReview Comment:\n   Should we simply combine the 3 constructors to accept both AccessTokenProvider, SASTokenProvider and the caller can set what it has and null as other?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -201,7 +202,7 @@ public String toString() {\n     }\n \n     public static ApiVersion getCurrentVersion() {\n-      return NOV_04_2024;\n+      return JULY_05_2025;\n\nReview Comment:\n   Are we bumping up version for all the requests?\n   Wasn't this version bump only needed for Auth related APIs?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1474,6 +1474,49 @@ public SASTokenProvider getSASTokenProvider() throws AzureBlobFileSystemExceptio\n     }\n   }\n \n+  /**\n+   * Returns the SASTokenProvider implementation to be used to generate user-bound SAS token.<br>\n+   * Custom implementation of {@link SASTokenProvider} under th config\n+   * \"fs.azure.sas.token.provider.type\" needs to be provided.<br>\n+   * @return sasTokenProvider object based on configurations provided\n+   * @throws AzureBlobFileSystemException\n+   */\n+  public SASTokenProvider getSASTokenProviderForUserBoundSAS() throws AzureBlobFileSystemException {\n\nReview Comment:\n   nit: Can be renamed as `getUserBoundSASTokenProvider`\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1474,6 +1474,49 @@ public SASTokenProvider getSASTokenProvider() throws AzureBlobFileSystemExceptio\n     }\n   }\n \n+  /**\n+   * Returns the SASTokenProvider implementation to be used to generate user-bound SAS token.<br>\n+   * Custom implementation of {@link SASTokenProvider} under th config\n+   * \"fs.azure.sas.token.provider.type\" needs to be provided.<br>\n+   * @return sasTokenProvider object based on configurations provided\n+   * @throws AzureBlobFileSystemException\n+   */\n+  public SASTokenProvider getSASTokenProviderForUserBoundSAS() throws AzureBlobFileSystemException {\n+    AuthType authType = getEnum(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.SharedKey);\n+    if (authType != AuthType.UserboundSASWithOAuth) {\n+      throw new SASTokenProviderException(String.format(\n+          \"Invalid auth type: %s is being used, expecting user-bound SAS.\", authType));\n+    }\n+\n+    try {\n+      Class<? extends SASTokenProvider> customSasTokenProviderImplementation =\n+          getTokenProviderClass(authType, FS_AZURE_SAS_TOKEN_PROVIDER_TYPE,\n+              null, SASTokenProvider.class);\n+\n+      if (customSasTokenProviderImplementation == null) {\n\nReview Comment:\n   Similar check for OAuth also needed right?\n   For UBS, Cx must configure OAuth as well?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -174,6 +174,17 @@ public AbfsDfsClient(final URL baseUrl,\n         encryptionContextProvider, abfsClientContext, AbfsServiceType.DFS);\n   }\n \n+  public AbfsDfsClient(final URL baseUrl,\n\nReview Comment:\n   Is this only for DFS Client?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/constants/TestConfigurationKeys.java:\n##########\n@@ -55,6 +55,9 @@ public final class TestConfigurationKeys {\n \n   public static final String FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID = \"fs.azure.test.app.service.principal.object.id\";\n \n+  public static final String FS_AZURE_END_USER_TENANT_ID = \"fs.azure.test.end.user.tenant.id\";\n\nReview Comment:\n   I think TEST should come first. Check for how other test related configs are defined. TestConfigurationKeys.java have them\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1741,7 +1741,15 @@ private void initializeClient(URI uri, String fileSystemName,\n     } else if (authType == AuthType.SAS) {\n       LOG.trace(\"Fetching SAS Token Provider\");\n       sasTokenProvider = abfsConfiguration.getSASTokenProvider();\n-    } else {\n+    } else if(authType == AuthType.UserboundSASWithOAuth){\n\nReview Comment:\n   Here also, it can be combined into a single call with both passed.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java:\n##########\n@@ -154,7 +173,15 @@ private AbfsDfsClient createDfsClient(final URL baseUrl,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n     URL dfsUrl = changeUrlFromBlobToDfs(baseUrl);\n-    if (tokenProvider != null) {\n+    if (tokenProvider != null && sasTokenProvider != null) {\n\nReview Comment:\n   Apart from UDS, can there be a case where caller can send both as not null? Makes ure no flow leads to this and also add a test around it.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java:\n##########\n@@ -80,6 +80,25 @@ public AbfsClientHandler(final URL baseUrl,\n         abfsClientContext);\n   }\n \n+  public AbfsClientHandler(final URL baseUrl,\n\nReview Comment:\n   Same as above. We can combine into a single constructor.\n   But do check if there are any caveats there or a risk of running into NPE\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501698383\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1474,6 +1474,49 @@ public SASTokenProvider getSASTokenProvider() throws AzureBlobFileSystemExceptio\n     }\n   }\n \n+  /**\n+   * Returns the SASTokenProvider implementation to be used to generate user-bound SAS token.<br>\n+   * Custom implementation of {@link SASTokenProvider} under th config\n+   * \"fs.azure.sas.token.provider.type\" needs to be provided.<br>\n+   * @return sasTokenProvider object based on configurations provided\n+   * @throws AzureBlobFileSystemException\n+   */\n+  public SASTokenProvider getSASTokenProviderForUserBoundSAS() throws AzureBlobFileSystemException {\n\nReview Comment:\n   Taken\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501700716\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1474,6 +1474,49 @@ public SASTokenProvider getSASTokenProvider() throws AzureBlobFileSystemExceptio\n     }\n   }\n \n+  /**\n+   * Returns the SASTokenProvider implementation to be used to generate user-bound SAS token.<br>\n+   * Custom implementation of {@link SASTokenProvider} under th config\n+   * \"fs.azure.sas.token.provider.type\" needs to be provided.<br>\n+   * @return sasTokenProvider object based on configurations provided\n+   * @throws AzureBlobFileSystemException\n+   */\n+  public SASTokenProvider getSASTokenProviderForUserBoundSAS() throws AzureBlobFileSystemException {\n+    AuthType authType = getEnum(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.SharedKey);\n+    if (authType != AuthType.UserboundSASWithOAuth) {\n+      throw new SASTokenProviderException(String.format(\n+          \"Invalid auth type: %s is being used, expecting user-bound SAS.\", authType));\n+    }\n+\n+    try {\n+      Class<? extends SASTokenProvider> customSasTokenProviderImplementation =\n+          getTokenProviderClass(authType, FS_AZURE_SAS_TOKEN_PROVIDER_TYPE,\n+              null, SASTokenProvider.class);\n+\n+      if (customSasTokenProviderImplementation == null) {\n\nReview Comment:\n   We were getting OAuth token provider separately. Added a single method to get both now\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501704276\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1741,7 +1741,15 @@ private void initializeClient(URI uri, String fileSystemName,\n     } else if (authType == AuthType.SAS) {\n       LOG.trace(\"Fetching SAS Token Provider\");\n       sasTokenProvider = abfsConfiguration.getSASTokenProvider();\n-    } else {\n+    } else if(authType == AuthType.UserboundSASWithOAuth){\n\nReview Comment:\n   Added a method to get both together\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501704549\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1741,7 +1741,15 @@ private void initializeClient(URI uri, String fileSystemName,\n     } else if (authType == AuthType.SAS) {\n       LOG.trace(\"Fetching SAS Token Provider\");\n       sasTokenProvider = abfsConfiguration.getSASTokenProvider();\n-    } else {\n+    } else if(authType == AuthType.UserboundSASWithOAuth){\n\nReview Comment:\n   Taken\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501705861\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1741,7 +1741,15 @@ private void initializeClient(URI uri, String fileSystemName,\n     } else if (authType == AuthType.SAS) {\n       LOG.trace(\"Fetching SAS Token Provider\");\n       sasTokenProvider = abfsConfiguration.getSASTokenProvider();\n-    } else {\n+    } else if(authType == AuthType.UserboundSASWithOAuth){\n+      LOG.trace(\"Fetching SAS and OAuth Token Provider for user bound SAS\");\n+      AzureADAuthenticator.init(abfsConfiguration);\n+      tokenProvider = abfsConfiguration.getTokenProvider();\n+      ExtensionHelper.bind(tokenProvider, uri,\n+          abfsConfiguration.getRawConfiguration());\n+      sasTokenProvider = abfsConfiguration.getSASTokenProviderForUserBoundSAS();\n+    }\n\nReview Comment:\n   Taken\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501707345\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1770,7 +1778,12 @@ private void initializeClient(URI uri, String fileSystemName,\n     }\n \n     LOG.trace(\"Initializing AbfsClient for {}\", baseUrl);\n-    if (tokenProvider != null) {\n+    if(tokenProvider != null && sasTokenProvider != null){\n\nReview Comment:\n   Taken\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501740559\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -187,7 +187,8 @@ public enum ApiVersion {\n     DEC_12_2019(\"2019-12-12\"),\n     APR_10_2021(\"2021-04-10\"),\n     AUG_03_2023(\"2023-08-03\"),\n-    NOV_04_2024(\"2024-11-04\");\n+    NOV_04_2024(\"2024-11-04\"),\n+    JULY_05_2025(\"2025-07-05\");\n\nReview Comment:\n   Modified\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501744777\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -201,7 +202,7 @@ public String toString() {\n     }\n \n     public static ApiVersion getCurrentVersion() {\n-      return NOV_04_2024;\n+      return JULY_05_2025;\n\nReview Comment:\n   Right- we're already overriding the API version header according to skv param for UBS requests. We dont need it here- removed\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501749548\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -363,6 +363,21 @@ public AbfsClient(final URL baseUrl,\n     this.sasTokenProvider = sasTokenProvider;\n   }\n \n+  public AbfsClient(final URL baseUrl,\n\nReview Comment:\n   Added\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501750149\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -363,6 +363,21 @@ public AbfsClient(final URL baseUrl,\n     this.sasTokenProvider = sasTokenProvider;\n   }\n \n+  public AbfsClient(final URL baseUrl,\n\nReview Comment:\n   Makes sense, added\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501771788\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -570,6 +570,11 @@ public void signRequest(final AbfsHttpOperation httpOperation, int bytesToSign)\n         // do nothing; the SAS token should already be appended to the query string\n         httpOperation.setMaskForSAS(); //mask sig/oid from url for logs\n         break;\n+      case UserboundSASWithOAuth:\n+        httpOperation.setRequestProperty(HttpHeaderConfigurations.AUTHORIZATION,\n+            client.getAccessToken());\n+        httpOperation.setMaskForSAS(); //mask sig/oid from url for logs\n\nReview Comment:\n   sig stands for signature\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501779191\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -174,6 +174,17 @@ public AbfsDfsClient(final URL baseUrl,\n         encryptionContextProvider, abfsClientContext, AbfsServiceType.DFS);\n   }\n \n+  public AbfsDfsClient(final URL baseUrl,\n\nReview Comment:\n   Validated for blob endpoint sometime back only, missed that. Added now\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AuthType.java:\n##########\n@@ -24,5 +24,6 @@ public enum AuthType {\n     SharedKey,\n     OAuth,\n     Custom,\n-    SAS\n+    SAS,\n+     UserboundSASWithOAuth\n\nReview Comment:\n   corrected\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501790448\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/constants/TestConfigurationKeys.java:\n##########\n@@ -55,6 +55,9 @@ public final class TestConfigurationKeys {\n \n   public static final String FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID = \"fs.azure.test.app.service.principal.object.id\";\n \n+  public static final String FS_AZURE_END_USER_TENANT_ID = \"fs.azure.test.end.user.tenant.id\";\n\nReview Comment:\n   Rectified\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501814332\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java:\n##########\n@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.extensions;\n+\n+import java.io.IOException;\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException;\n+import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.utils.Base64;\n+import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator;\n+import org.apache.hadoop.fs.azurebfs.utils.SASGenerator;\n+import org.apache.hadoop.security.AccessControlException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT;\n+\n+/**\n+ * A mock user-bound SAS token provider implementation.\n+ */\n+\n+public class MockUserBoundSASTokenProvider implements SASTokenProvider {\n+\n+  private DelegationSASGenerator generator;\n+\n+  public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\";\n+  public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\";\n+  public static final String NO_AGENT_PATH = \"NoAgentPath\";\n+\n+  @Override\n+  public void initialize(Configuration configuration, String accountName) throws IOException {\n+    String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID);\n+    String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET);\n+    String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID);\n+    String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID);\n+    String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES));\n+    String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY));\n+    String skv = SASGenerator.AuthenticationVersion.July5.toString();\n+\n+    String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID);\n+    String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID);\n+\n+    byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid);\n+\n+    generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid);\n+  }\n+\n+  // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an\n+  // access token.  See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow.\n+  private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException {\n+    String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid);\n+    ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret);\n+    return \"Bearer \" + provider.getToken().getAccessToken();\n+  }\n+\n+  private byte[] getUserDelegationKey(String accountName, String appID, String appSecret,\n\nReview Comment:\n   Added\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501828490\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java:\n##########\n@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.extensions;\n+\n+import java.io.IOException;\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException;\n+import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.utils.Base64;\n+import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator;\n+import org.apache.hadoop.fs.azurebfs.utils.SASGenerator;\n+import org.apache.hadoop.security.AccessControlException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT;\n+\n+/**\n+ * A mock user-bound SAS token provider implementation.\n+ */\n+\n+public class MockUserBoundSASTokenProvider implements SASTokenProvider {\n+\n+  private DelegationSASGenerator generator;\n+\n+  public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\";\n+  public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\";\n+  public static final String NO_AGENT_PATH = \"NoAgentPath\";\n+\n+  @Override\n+  public void initialize(Configuration configuration, String accountName) throws IOException {\n+    String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID);\n+    String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET);\n+    String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID);\n+    String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID);\n+    String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES));\n+    String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY));\n+    String skv = SASGenerator.AuthenticationVersion.July5.toString();\n+\n+    String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID);\n+    String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID);\n+\n+    byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid);\n+\n+    generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid);\n+  }\n+\n+  // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an\n+  // access token.  See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow.\n+  private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException {\n+    String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid);\n+    ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret);\n+    return \"Bearer \" + provider.getToken().getAccessToken();\n+  }\n+\n+  private byte[] getUserDelegationKey(String accountName, String appID, String appSecret,\n+      String sktid, String skt, String ske, String skv, String skdutid) throws IOException {\n+\n+    String method = \"POST\";\n+    String account = accountName.substring(0, accountName.indexOf(AbfsHttpConstants.DOT));\n+\n+    final StringBuilder sb = new StringBuilder(128);\n+    sb.append(\"https://\");\n+    sb.append(account);\n+    sb.append(\".blob.core.windows.net/?restype=service&comp=userdelegationkey\");\n\nReview Comment:\n   Added\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2501843102\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/DelegationSASGenerator.java:\n##########\n@@ -36,20 +38,26 @@ public class DelegationSASGenerator extends SASGenerator {\n   private final String ske;\n   private final String sks = \"b\";\n   private final String skv;\n+  private final String skdutid;\n+  private final String sduoid;\n \n-  public DelegationSASGenerator(byte[] userDelegationKey, String skoid, String sktid, String skt, String ske, String skv) {\n+  public DelegationSASGenerator(byte[] userDelegationKey, String skoid, String sktid, String skt, String ske, String skv, String skdutid, String sduoid) {\n\nReview Comment:\n   Added\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2502170839\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java:\n##########\n@@ -154,7 +173,15 @@ private AbfsDfsClient createDfsClient(final URL baseUrl,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n     URL dfsUrl = changeUrlFromBlobToDfs(baseUrl);\n-    if (tokenProvider != null) {\n+    if (tokenProvider != null && sasTokenProvider != null) {\n\nReview Comment:\n   We are initialising both the token providers only for UBS auth type. Added a test for token provider expectations (null or non-null) according to the auth type\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2502235346\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java:\n##########\n@@ -154,7 +156,15 @@ private AbfsDfsClient createDfsClient(final URL baseUrl,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n     URL dfsUrl = changeUrlFromBlobToDfs(baseUrl);\n-    if (tokenProvider != null) {\n+    if (tokenProvider != null && sasTokenProvider != null) {\n\nReview Comment:\n   All these conditions can again be removed by creating a single constructor for AbfsDfsClient and AbfsBlobClient that sends both access token provider and sas token provider (like we did for client handler and parent constructors)\r\n   But would it be better to have it this way to separate out the logging here?\n\n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3510501447\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 47s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 26s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/6/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  26m 41s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 38s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/6/artifact/out/blanks-eol.txt) |  The patch has 4 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/6/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 15 new + 7 unchanged - 0 fixed = 22 total (was 7)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/6/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 5 new + 1541 unchanged - 3 fixed = 1546 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/6/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 3 new + 1436 unchanged - 1 fixed = 1439 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   1m 25s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/6/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 174 unchanged - 4 fixed = 176 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  6s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m  7s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:[line 370] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:[line 77] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux e36176f6f95c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 58fc8b3cb952b4264e515c1df7d0e3a15e58084b |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/6/testReport/ |\r\n   | Max. process+thread count | 763 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/6/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3511083332\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 57s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  33m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 25s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/7/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 23s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  26m 45s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/7/artifact/out/blanks-eol.txt) |  The patch has 4 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 8 new + 7 unchanged - 0 fixed = 15 total (was 7)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 39s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 5 new + 1541 unchanged - 3 fixed = 1546 total (was 1544)  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 3 new + 1436 unchanged - 1 fixed = 1439 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   1m 24s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/7/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 174 unchanged - 4 fixed = 176 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  5s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 103m  3s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:[line 370] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:[line 77] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux fe83e047b7bd 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 09139dc1eabc3fcc5c8bb552cc29e3760ca6b7d2 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/7/testReport/ |\r\n   | Max. process+thread count | 612 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/7/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3515498649\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 57s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  33m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 47s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 27s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/8/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  27m 14s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 8 new + 7 unchanged - 0 fixed = 15 total (was 7)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 31s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/8/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 3 new + 1540 unchanged - 4 fixed = 1543 total (was 1544)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 1436 unchanged - 1 fixed = 1436 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   1m 24s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/8/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 174 unchanged - 4 fixed = 176 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  9s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 103m 59s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:[line 370] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:[line 77] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux d9eed73ecaae 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 85f1a30ccdc0cfffab87bd68eb35ee058d2eb8d4 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/8/testReport/ |\r\n   | Max. process+thread count | 612 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/8/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2513736983\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java:\n##########\n@@ -154,7 +156,15 @@ private AbfsDfsClient createDfsClient(final URL baseUrl,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n     URL dfsUrl = changeUrlFromBlobToDfs(baseUrl);\n-    if (tokenProvider != null) {\n+    if (tokenProvider != null && sasTokenProvider != null) {\n\nReview Comment:\n   Logging as well can be combined. in log we will see null value for the one which is not supposed to be used and we will know.\n   Better to combine everywhere IMO.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -201,7 +201,21 @@ public AbfsBlobClient(final URL baseUrl,\n       final SASTokenProvider sasTokenProvider,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n-    super(baseUrl, sharedKeyCredentials, abfsConfiguration, sasTokenProvider,\n+    super(baseUrl, sharedKeyCredentials, abfsConfiguration, null, sasTokenProvider,\n+        encryptionContextProvider, abfsClientContext, AbfsServiceType.BLOB);\n+    this.azureAtomicRenameDirSet = new HashSet<>(Arrays.asList(\n+        abfsConfiguration.getAzureAtomicRenameDirs()\n+            .split(AbfsHttpConstants.COMMA)));\n+  }\n+\n+  public AbfsBlobClient(final URL baseUrl,\n\nReview Comment:\n   Similarly here also we can do how we did for AbfsClient.\n   We have 3 constructors here and the only diff they have is the token provider.\n   Let's combine them and caller can choose what to send\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -363,6 +363,21 @@ public AbfsClient(final URL baseUrl,\n     this.sasTokenProvider = sasTokenProvider;\n   }\n \n+  public AbfsClient(final URL baseUrl,\n\nReview Comment:\n   Similar we should do for AbfsBlobClient and AbfsDfsClient\n\n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3521669143\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 58s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 24s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/9/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  26m 43s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/9/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 7 unchanged - 0 fixed = 8 total (was 7)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 39s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 31s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/9/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 3 new + 1540 unchanged - 4 fixed = 1543 total (was 1544)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 1436 unchanged - 1 fixed = 1436 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   1m 24s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/9/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 174 unchanged - 4 fixed = 176 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  7s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 103m 27s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:[line 370] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:[line 77] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux b44c133d85bb 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ec2c76c9333b047e38ece590f12afe849f02e718 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/9/testReport/ |\r\n   | Max. process+thread count | 610 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/9/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3542163233\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  33m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 25s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/10/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  26m 38s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 31s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/10/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 3 new + 1540 unchanged - 4 fixed = 1543 total (was 1544)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 1436 unchanged - 1 fixed = 1436 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   1m 24s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/10/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 174 unchanged - 4 fixed = 176 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  4s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m 27s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:[line 370] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:[line 77] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux d12d8de249de 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 700cce36b991e2214be228f3438a3fc15a254425 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/10/testReport/ |\r\n   | Max. process+thread count | 611 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/10/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3542222339\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  20m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 32s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 18s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 18s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 18s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 19s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 3 new + 1538 unchanged - 6 fixed = 1541 total (was 1544)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 1434 unchanged - 3 fixed = 1434 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   0m 20s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  15m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 22s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  56m 45s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux 96ec8e1c0568 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0bbbd364a0a0080def19cd0ebb9db2959d27cc8d |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/testReport/ |\r\n   | Max. process+thread count | 663 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/11/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2537531694\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1610,6 +1612,70 @@ public SASTokenProvider getSASTokenProvider() throws AzureBlobFileSystemExceptio\n     }\n   }\n \n+  /**\n+   * Returns the SASTokenProvider implementation to be used to generate user-bound SAS token.\n+   * Custom implementation of {@link SASTokenProvider} under th config\n+   * \"fs.azure.sas.token.provider.type\" needs to be provided.\n+   * @param authType authentication type\n+   * @return sasTokenProvider object based on configurations provided\n+   * @throws AzureBlobFileSystemException is user-bound SAS token provider initialization fails\n+   */\n+  public SASTokenProvider getUserBoundSASTokenProvider(AuthType authType)\n+      throws AzureBlobFileSystemException {\n+\n+    try {\n+      Class<? extends SASTokenProvider> customSasTokenProviderImplementation =\n+          getTokenProviderClass(authType, FS_AZURE_SAS_TOKEN_PROVIDER_TYPE,\n+              null, SASTokenProvider.class);\n+\n+      if (customSasTokenProviderImplementation == null) {\n+        throw new SASTokenProviderException(String.format(\n+            \"\\\"%s\\\" must be set for user-bound SAS auth type.\",\n+            FS_AZURE_SAS_TOKEN_PROVIDER_TYPE));\n+      }\n+\n+        SASTokenProvider sasTokenProvider = ReflectionUtils.newInstance(\n+            customSasTokenProviderImplementation, rawConfig);\n+        if (sasTokenProvider == null) {\n+          throw new SASTokenProviderException(String.format(\n+              \"Failed to initialize %s\", customSasTokenProviderImplementation));\n+        }\n+        LOG.trace(\"Initializing {}\", customSasTokenProviderImplementation.getName());\n+        sasTokenProvider.initialize(rawConfig, accountName);\n+        LOG.trace(\"{} init complete\", customSasTokenProviderImplementation.getName());\n+        return sasTokenProvider;\n+    } catch (SASTokenProviderException e) {\n+      throw e;\n+    } catch (Exception e) {\n+      throw new SASTokenProviderException(\n+          \"Unable to load user-bound SAS token provider class: \" + e, e);\n\nReview Comment:\n   Why e 2 times ?\n\n\n\n", "anmolanmol1234 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2537688385\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemUserBoundSAS.java:\n##########\n@@ -0,0 +1,432 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Field;\n+import java.nio.file.AccessDeniedException;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.format.DateTimeFormatter;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ListResultEntrySchema;\n+import org.apache.hadoop.fs.azurebfs.extensions.MockInvalidSASTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.extensions.MockUserBoundSASTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.extensions.SASTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.oauth2.AzureADToken;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsBlobClient;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n+import org.apache.hadoop.fs.azurebfs.services.AuthType;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SERVICE_PRINCIPAL_OBJECT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_END_USER_OBJECT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_END_USER_TENANT_ID;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n+\n+/**\n+ * Integration tests for AzureBlobFileSystem using User-Bound SAS and OAuth.\n+ * Covers scenarios for token provider configuration, SAS token validity, and basic file operations.\n+ */\n+public class ITestAzureBlobFileSystemUserBoundSAS\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static Path testPath = new Path(\"/test.txt\");\n+\n+  private static final String TEST_OBJECT_ID = \"123456789\";\n+\n+  private static final String INVALID_OAUTH_TOKEN_VALUE = \"InvalidOAuthTokenValue\";\n+\n+  /**\n+   * Constructor. Ensures tests run with SharedKey authentication.\n+   * @throws Exception if auth type is not SharedKey\n+   */\n+  protected ITestAzureBlobFileSystemUserBoundSAS() throws Exception {\n+    assumeThat(this.getAuthType()).isEqualTo(AuthType.SharedKey);\n+  }\n+\n+  /**\n+   * Sets up the test environment and configures the AbfsConfiguration for user-bound SAS tests.\n+   * @throws Exception if setup fails\n+   */\n+  @BeforeEach\n+  @Override\n+  public void setup() throws Exception {\n+    AbfsConfiguration abfsConfig = this.getConfiguration();\n+    String accountName = getAccountName();\n+\n+    Boolean isHNSEnabled = abfsConfig.getBoolean(\n+        TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false);\n+\n+    if (!isHNSEnabled) {\n+      assumeBlobServiceType();\n+    }\n+\n+    createFilesystemForUserBoundSASTests();\n+    super.setup();\n+\n+    // Set all required configs on the raw configuration\n+    abfsConfig.set(\n+        FS_AZURE_BLOB_FS_CLIENT_SERVICE_PRINCIPAL_OBJECT_ID + \".\" + accountName,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID));\n+    abfsConfig.set(FS_AZURE_BLOB_FS_CLIENT_SERVICE_PRINCIPAL_OBJECT_ID,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID));\n+    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID + \".\" + accountName,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID));\n+    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID));\n+    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET + \".\" + accountName,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET));\n+    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET));\n+    abfsConfig.set(FS_AZURE_TEST_END_USER_TENANT_ID,\n+        abfsConfig.get(FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID));\n+    abfsConfig.set(FS_AZURE_TEST_END_USER_OBJECT_ID,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID));\n+    abfsConfig.set(FS_AZURE_SAS_TOKEN_PROVIDER_TYPE,\n+        MockUserBoundSASTokenProvider.class.getName());\n+  }\n+\n+\n+  /**\n+   * Injects a mock AccessTokenProvider into the AbfsClient of the given filesystem.\n+   * @param fs AzureBlobFileSystem instance\n+   * @param mockProvider AccessTokenProvider to inject\n+   * @throws Exception if reflection fails\n+   */\n+  private void injectMockTokenProvider(AzureBlobFileSystem fs,\n+      AccessTokenProvider mockProvider) throws Exception {\n+    Field abfsStoreField = AzureBlobFileSystem.class.getDeclaredField(\n+        \"abfsStore\");\n+    abfsStoreField.setAccessible(true);\n+    AzureBlobFileSystemStore store\n+        = (AzureBlobFileSystemStore) abfsStoreField.get(fs);\n+\n+    Field abfsClientField = AzureBlobFileSystemStore.class.getDeclaredField(\n+        \"client\");\n+    abfsClientField.setAccessible(true);\n+    AbfsClient client = (AbfsClient) abfsClientField.get(store);\n+\n+    Field tokenProviderField = AbfsClient.class.getDeclaredField(\n+        \"tokenProvider\");\n+    tokenProviderField.setAccessible(true);\n+    tokenProviderField.set(client, mockProvider);\n+  }\n+\n+  /**\n+   * Helper to create a new AzureBlobFileSystem instance for tests.\n+   * @return AzureBlobFileSystem instance\n+   * @throws RuntimeException if creation fails\n+   */\n+  private AzureBlobFileSystem createTestFileSystem() throws RuntimeException {\n+    try {\n+      return (AzureBlobFileSystem) FileSystem.newInstance(\n+          getRawConfiguration());\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /**\n+   * Test that file creation fails when the end user object ID does not match the service principal object ID.\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testShouldFailWhenSduoidMismatchesServicePrincipalId()\n+      throws Exception {\n+    this.getConfiguration()\n+        .set(FS_AZURE_TEST_END_USER_OBJECT_ID, TEST_OBJECT_ID);\n+    AzureBlobFileSystem testFs = createTestFileSystem();\n+    intercept(AccessDeniedException.class,\n+        () -> {\n+          testFs.create(testPath);\n+        });\n+  }\n+\n+  /**\n+   * Verifies that both OAuth token provider and user-bound SAS token provider are configured and usable.\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testOAuthTokenProviderAndSASTokenFlow() throws Exception {\n+    AzureBlobFileSystem testFs = createTestFileSystem();\n+\n+    AbfsConfiguration abfsConfiguration = testFs.getAbfsStore()\n+        .getAbfsConfiguration();\n+\n+    // Verify AbfsConfiguration has an OAuth token provider configured\n+    AccessTokenProvider tokenProvider = abfsConfiguration.getTokenProvider();\n+    assertNotNull(tokenProvider,\n+        \"AccessTokenProvider must be configured for UserboundSASWithOAuth\");\n+\n+    // Acquire an OAuth token and assert it is non-empty\n+    AzureADToken token = tokenProvider.getToken();\n+    assertNotNull(token, \"OAuth token must not be null\");\n+    assertNotNull(token.getAccessToken(),\n+        \"OAuth access token must not be null\");\n+    assertFalse(token.getAccessToken().isEmpty(),\n+        \"OAuth access token must not be empty\");\n+\n+    // Verify AbfsConfiguration has an SASTokenProvider configured\n+    SASTokenProvider sasProvider\n+        = abfsConfiguration.getUserBoundSASTokenProvider(\n+        AuthType.UserboundSASWithOAuth);\n+    assertNotNull(sasProvider,\n+        \"SASTokenProvider for user-bound SAS must be configured\");\n+    assertInstanceOf(MockUserBoundSASTokenProvider.class, sasProvider,\n+        \"Expected MockUserBoundSASTokenProvider to be used for tests\");\n+\n+    // Request a SAS token and assert we get a non-empty result\n+    String sasToken = sasProvider.getSASToken(\n+        \"abfsdrivercanaryhns.dfs.core.windows.net\", \"userbound\", \"/\",\n+        SASTokenProvider.GET_PROPERTIES_OPERATION);\n+    assertNotNull(sasToken, \"SAS token must not be null\");\n+    assertFalse(sasToken.isEmpty(), \"SAS token must not be empty\");\n+  }\n+\n+  /*\n+   * Tests listing and deleting files under an implicit directory\n+   */\n+  @Test\n+  public void testOperationsForImplicitPaths() throws Exception {\n\nReview Comment:\n   In the setup we don't assume non hns for all cases, shouldn't we add that assumption here ?\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2540475191\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1610,6 +1612,70 @@ public SASTokenProvider getSASTokenProvider() throws AzureBlobFileSystemExceptio\n     }\n   }\n \n+  /**\n+   * Returns the SASTokenProvider implementation to be used to generate user-bound SAS token.\n+   * Custom implementation of {@link SASTokenProvider} under th config\n+   * \"fs.azure.sas.token.provider.type\" needs to be provided.\n+   * @param authType authentication type\n+   * @return sasTokenProvider object based on configurations provided\n+   * @throws AzureBlobFileSystemException is user-bound SAS token provider initialization fails\n+   */\n+  public SASTokenProvider getUserBoundSASTokenProvider(AuthType authType)\n+      throws AzureBlobFileSystemException {\n+\n+    try {\n+      Class<? extends SASTokenProvider> customSasTokenProviderImplementation =\n+          getTokenProviderClass(authType, FS_AZURE_SAS_TOKEN_PROVIDER_TYPE,\n+              null, SASTokenProvider.class);\n+\n+      if (customSasTokenProviderImplementation == null) {\n+        throw new SASTokenProviderException(String.format(\n+            \"\\\"%s\\\" must be set for user-bound SAS auth type.\",\n+            FS_AZURE_SAS_TOKEN_PROVIDER_TYPE));\n+      }\n+\n+        SASTokenProvider sasTokenProvider = ReflectionUtils.newInstance(\n+            customSasTokenProviderImplementation, rawConfig);\n+        if (sasTokenProvider == null) {\n+          throw new SASTokenProviderException(String.format(\n+              \"Failed to initialize %s\", customSasTokenProviderImplementation));\n+        }\n+        LOG.trace(\"Initializing {}\", customSasTokenProviderImplementation.getName());\n+        sasTokenProvider.initialize(rawConfig, accountName);\n+        LOG.trace(\"{} init complete\", customSasTokenProviderImplementation.getName());\n+        return sasTokenProvider;\n+    } catch (SASTokenProviderException e) {\n+      throw e;\n+    } catch (Exception e) {\n+      throw new SASTokenProviderException(\n+          \"Unable to load user-bound SAS token provider class: \" + e, e);\n\nReview Comment:\n   e is included in the message string and also passed as the cause in the exception constructor\n\n\n\n", "manika137 commented on code in PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#discussion_r2540494549\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemUserBoundSAS.java:\n##########\n@@ -0,0 +1,432 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Field;\n+import java.nio.file.AccessDeniedException;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.format.DateTimeFormatter;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ListResultEntrySchema;\n+import org.apache.hadoop.fs.azurebfs.extensions.MockInvalidSASTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.extensions.MockUserBoundSASTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.extensions.SASTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider;\n+import org.apache.hadoop.fs.azurebfs.oauth2.AzureADToken;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsBlobClient;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n+import org.apache.hadoop.fs.azurebfs.services.AuthType;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SERVICE_PRINCIPAL_OBJECT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_END_USER_OBJECT_ID;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_END_USER_TENANT_ID;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n+\n+/**\n+ * Integration tests for AzureBlobFileSystem using User-Bound SAS and OAuth.\n+ * Covers scenarios for token provider configuration, SAS token validity, and basic file operations.\n+ */\n+public class ITestAzureBlobFileSystemUserBoundSAS\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static Path testPath = new Path(\"/test.txt\");\n+\n+  private static final String TEST_OBJECT_ID = \"123456789\";\n+\n+  private static final String INVALID_OAUTH_TOKEN_VALUE = \"InvalidOAuthTokenValue\";\n+\n+  /**\n+   * Constructor. Ensures tests run with SharedKey authentication.\n+   * @throws Exception if auth type is not SharedKey\n+   */\n+  protected ITestAzureBlobFileSystemUserBoundSAS() throws Exception {\n+    assumeThat(this.getAuthType()).isEqualTo(AuthType.SharedKey);\n+  }\n+\n+  /**\n+   * Sets up the test environment and configures the AbfsConfiguration for user-bound SAS tests.\n+   * @throws Exception if setup fails\n+   */\n+  @BeforeEach\n+  @Override\n+  public void setup() throws Exception {\n+    AbfsConfiguration abfsConfig = this.getConfiguration();\n+    String accountName = getAccountName();\n+\n+    Boolean isHNSEnabled = abfsConfig.getBoolean(\n+        TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false);\n+\n+    if (!isHNSEnabled) {\n+      assumeBlobServiceType();\n+    }\n+\n+    createFilesystemForUserBoundSASTests();\n+    super.setup();\n+\n+    // Set all required configs on the raw configuration\n+    abfsConfig.set(\n+        FS_AZURE_BLOB_FS_CLIENT_SERVICE_PRINCIPAL_OBJECT_ID + \".\" + accountName,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID));\n+    abfsConfig.set(FS_AZURE_BLOB_FS_CLIENT_SERVICE_PRINCIPAL_OBJECT_ID,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID));\n+    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID + \".\" + accountName,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID));\n+    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID));\n+    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET + \".\" + accountName,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET));\n+    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET));\n+    abfsConfig.set(FS_AZURE_TEST_END_USER_TENANT_ID,\n+        abfsConfig.get(FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID));\n+    abfsConfig.set(FS_AZURE_TEST_END_USER_OBJECT_ID,\n+        abfsConfig.get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID));\n+    abfsConfig.set(FS_AZURE_SAS_TOKEN_PROVIDER_TYPE,\n+        MockUserBoundSASTokenProvider.class.getName());\n+  }\n+\n+\n+  /**\n+   * Injects a mock AccessTokenProvider into the AbfsClient of the given filesystem.\n+   * @param fs AzureBlobFileSystem instance\n+   * @param mockProvider AccessTokenProvider to inject\n+   * @throws Exception if reflection fails\n+   */\n+  private void injectMockTokenProvider(AzureBlobFileSystem fs,\n+      AccessTokenProvider mockProvider) throws Exception {\n+    Field abfsStoreField = AzureBlobFileSystem.class.getDeclaredField(\n+        \"abfsStore\");\n+    abfsStoreField.setAccessible(true);\n+    AzureBlobFileSystemStore store\n+        = (AzureBlobFileSystemStore) abfsStoreField.get(fs);\n+\n+    Field abfsClientField = AzureBlobFileSystemStore.class.getDeclaredField(\n+        \"client\");\n+    abfsClientField.setAccessible(true);\n+    AbfsClient client = (AbfsClient) abfsClientField.get(store);\n+\n+    Field tokenProviderField = AbfsClient.class.getDeclaredField(\n+        \"tokenProvider\");\n+    tokenProviderField.setAccessible(true);\n+    tokenProviderField.set(client, mockProvider);\n+  }\n+\n+  /**\n+   * Helper to create a new AzureBlobFileSystem instance for tests.\n+   * @return AzureBlobFileSystem instance\n+   * @throws RuntimeException if creation fails\n+   */\n+  private AzureBlobFileSystem createTestFileSystem() throws RuntimeException {\n+    try {\n+      return (AzureBlobFileSystem) FileSystem.newInstance(\n+          getRawConfiguration());\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /**\n+   * Test that file creation fails when the end user object ID does not match the service principal object ID.\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testShouldFailWhenSduoidMismatchesServicePrincipalId()\n+      throws Exception {\n+    this.getConfiguration()\n+        .set(FS_AZURE_TEST_END_USER_OBJECT_ID, TEST_OBJECT_ID);\n+    AzureBlobFileSystem testFs = createTestFileSystem();\n+    intercept(AccessDeniedException.class,\n+        () -> {\n+          testFs.create(testPath);\n+        });\n+  }\n+\n+  /**\n+   * Verifies that both OAuth token provider and user-bound SAS token provider are configured and usable.\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testOAuthTokenProviderAndSASTokenFlow() throws Exception {\n+    AzureBlobFileSystem testFs = createTestFileSystem();\n+\n+    AbfsConfiguration abfsConfiguration = testFs.getAbfsStore()\n+        .getAbfsConfiguration();\n+\n+    // Verify AbfsConfiguration has an OAuth token provider configured\n+    AccessTokenProvider tokenProvider = abfsConfiguration.getTokenProvider();\n+    assertNotNull(tokenProvider,\n+        \"AccessTokenProvider must be configured for UserboundSASWithOAuth\");\n+\n+    // Acquire an OAuth token and assert it is non-empty\n+    AzureADToken token = tokenProvider.getToken();\n+    assertNotNull(token, \"OAuth token must not be null\");\n+    assertNotNull(token.getAccessToken(),\n+        \"OAuth access token must not be null\");\n+    assertFalse(token.getAccessToken().isEmpty(),\n+        \"OAuth access token must not be empty\");\n+\n+    // Verify AbfsConfiguration has an SASTokenProvider configured\n+    SASTokenProvider sasProvider\n+        = abfsConfiguration.getUserBoundSASTokenProvider(\n+        AuthType.UserboundSASWithOAuth);\n+    assertNotNull(sasProvider,\n+        \"SASTokenProvider for user-bound SAS must be configured\");\n+    assertInstanceOf(MockUserBoundSASTokenProvider.class, sasProvider,\n+        \"Expected MockUserBoundSASTokenProvider to be used for tests\");\n+\n+    // Request a SAS token and assert we get a non-empty result\n+    String sasToken = sasProvider.getSASToken(\n+        \"abfsdrivercanaryhns.dfs.core.windows.net\", \"userbound\", \"/\",\n+        SASTokenProvider.GET_PROPERTIES_OPERATION);\n+    assertNotNull(sasToken, \"SAS token must not be null\");\n+    assertFalse(sasToken.isEmpty(), \"SAS token must not be empty\");\n+  }\n+\n+  /*\n+   * Tests listing and deleting files under an implicit directory\n+   */\n+  @Test\n+  public void testOperationsForImplicitPaths() throws Exception {\n\nReview Comment:\n   In setup, if non-hns, we're assuming it to be Blob endpoint. It won't run for FNS-DFS\r\n   In this test, we're only running for Blob endpoint as well\n\n\n\n", "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3551021016\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  33m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 47s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 23s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/12/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  26m 54s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/12/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 7 unchanged - 0 fixed = 8 total (was 7)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 39s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/12/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 3 new + 1538 unchanged - 6 fixed = 1541 total (was 1544)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 1434 unchanged - 3 fixed = 1434 total (was 1437)  |\r\n   | -1 :x: |  spotbugs  |   1m 25s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/12/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 174 unchanged - 4 fixed = 176 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  5s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 104m  3s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClient(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext, AbfsServiceType)  At AbfsClient.java:[line 370] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:new org.apache.hadoop.fs.azurebfs.services.AbfsClientHandler(URL, SharedKeyCredentials, AbfsConfiguration, AccessTokenProvider, SASTokenProvider, EncryptionContextProvider, AbfsClientContext)  At AbfsClientHandler.java:[line 77] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/12/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux 5f9d0c270222 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d3f710804d3ad6e9b5f84dbfeccc7d1ffed282da |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/12/testReport/ |\r\n   | Max. process+thread count | 711 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/12/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "Adding support for new authentication type: user bound SAS", "qna": [{"question": "What is the issue title?", "answer": "ABFS: Support for new auth type: User-bound SAS"}, {"question": "Who reported this issue?", "answer": "Manika Joshi"}]}
{"key": "HADOOP-19735", "project": "HADOOP", "title": "ABFS: Adding request priority for prefetches", "status": "Open", "reporter": "Manika Joshi", "created": "2025-10-24T04:46:13.000+0000", "description": "Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server during throttling", "comments": ["anujmodi2021 commented on code in PR #8050:\nURL: https://github.com/apache/hadoop/pull/8050#discussion_r2488425804\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -393,6 +393,14 @@ public final class FileSystemConfigurations {\n \n   public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true;\n \n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_PREFETCH_REQUEST_PRIORITY = true;\n+\n+  // The default traffic request priority is 3 (from service)\n+  // The lowest priority a request can get is 7\n+  public static final int DEFAULT_FS_AZURE_REQUEST_PRIORITY_VALUE = 7;\n\nReview Comment:\n   Not vey sure about this.\n   Can 7 be used in any other case as well?\n   \n   For now may be okay to use prefetch keyword.\n\n\n\n", "anujmodi2021 commented on code in PR #8050:\nURL: https://github.com/apache/hadoop/pull/8050#discussion_r2488427272\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -638,6 +638,15 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MAX_RETRY_COUNT)\n   private int tailLatencyMaxRetryCount;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_PREFETCH_REQUEST_PRIORITY,\n+      DefaultValue = DEFAULT_FS_AZURE_ENABLE_PREFETCH_REQUEST_PRIORITY)\n+  private boolean enablePrefetchRequestPriority;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_PREFETCH_REQUEST_PRIORITY_VALUE,\n+      MinValue = DEFAULT_FS_AZURE_STANDARD_MIN_REQUEST_PRIORITY_VALUE,\n+      DefaultValue = DEFAULT_FS_AZURE_REQUEST_PRIORITY_VALUE)\n\nReview Comment:\n   Can be something general, FS_AZURE_REQUEST_PRIORITY_LOW\n\n\n\n", "manika137 commented on code in PR #8050:\nURL: https://github.com/apache/hadoop/pull/8050#discussion_r2489045520\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -638,6 +638,15 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MAX_RETRY_COUNT)\n   private int tailLatencyMaxRetryCount;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_PREFETCH_REQUEST_PRIORITY,\n+      DefaultValue = DEFAULT_FS_AZURE_ENABLE_PREFETCH_REQUEST_PRIORITY)\n+  private boolean enablePrefetchRequestPriority;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_PREFETCH_REQUEST_PRIORITY_VALUE,\n+      MinValue = DEFAULT_FS_AZURE_STANDARD_MIN_REQUEST_PRIORITY_VALUE,\n+      DefaultValue = DEFAULT_FS_AZURE_REQUEST_PRIORITY_VALUE)\n\nReview Comment:\n   Adding general terms for both to be used for future cases:\r\n   \r\n   DEFAULT_FS_AZURE_LOWEST_REQUEST_PRIORITY_VALUE- Value as 7- this can be used as default when we want the request to have lowest priority\r\n   DEFAULT_FS_AZURE_STANDARD_REQUEST_PRIORITY_VALUE- Value as 3- this is the standard req priority value and will be set as minimum for reqs where we want lowest priorities and maximum  for reqs where we want highest priorities\r\n   \n\n\n\n", "manika137 commented on code in PR #8050:\nURL: https://github.com/apache/hadoop/pull/8050#discussion_r2489059758\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -638,6 +638,15 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MAX_RETRY_COUNT)\n   private int tailLatencyMaxRetryCount;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_PREFETCH_REQUEST_PRIORITY,\n+      DefaultValue = DEFAULT_FS_AZURE_ENABLE_PREFETCH_REQUEST_PRIORITY)\n+  private boolean enablePrefetchRequestPriority;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_PREFETCH_REQUEST_PRIORITY_VALUE,\n+      MinValue = DEFAULT_FS_AZURE_STANDARD_MIN_REQUEST_PRIORITY_VALUE,\n+      DefaultValue = DEFAULT_FS_AZURE_REQUEST_PRIORITY_VALUE)\n\nReview Comment:\n   We can have one: DEFAULT_FS_AZURE_HIGHEST_REQUEST_PRIORITY_VALUE- value as 0 later when reqd\n\n\n\n", "manika137 commented on code in PR #8050:\nURL: https://github.com/apache/hadoop/pull/8050#discussion_r2489381491\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -899,6 +908,91 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs,\n     assertReadTypeInClientRequestId(fs, numOfReadCalls, totalReadCalls, readType);\n   }\n \n+  /*\n+   * Test to verify that both conditions of prefetch read and respective config\n+   * enabled needs to be true for the priority header to be added\n+   */\n+  @Test\n+  public void testPrefetchReadAddsPriorityHeaderWithDifferentConfigs()\n+      throws Exception {\n+    Configuration configuration1 = new Configuration(getRawConfiguration());\n+    configuration1.set(FS_AZURE_ENABLE_PREFETCH_REQUEST_PRIORITY, \"true\");\n+\n+    Configuration configuration2 = new Configuration(getRawConfiguration());\n+    //use the default value for the config: false\n+    configuration2.unset(FS_AZURE_ENABLE_PREFETCH_REQUEST_PRIORITY);\n+\n+    TracingContext tracingContext1 = mock(TracingContext.class);\n+    when(tracingContext1.getReadType()).thenReturn(PREFETCH_READ);\n+\n+    //Prefetch Read with config enabled\n+    executePrefetchReadTest(tracingContext1, configuration1, true);\n+    //Prefetch Read with config disabled\n+    executePrefetchReadTest(tracingContext1, configuration2, false);\n+\n+    when(tracingContext1.getReadType()).thenReturn(DIRECT_READ);\n+\n+    //Non-prefetch read with config disabled\n+    executePrefetchReadTest(tracingContext1, configuration2, false);\n+    //Non-prefetch read with config enabled\n+    executePrefetchReadTest(tracingContext1, configuration1, false);\n+  }\n+\n+  private void executePrefetchReadTest(TracingContext tracingContext,\n\nReview Comment:\n   Added\n\n\n\n", "hadoop-yetus commented on PR #8050:\nURL: https://github.com/apache/hadoop/pull/8050#issuecomment-3485072499\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8050/7/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 46s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8050/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 6 new + 1538 unchanged - 0 fixed = 1544 total (was 1538)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8050/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 6 new + 1431 unchanged - 0 fixed = 1437 total (was 1431)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 48s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 17s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  61m 14s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8050/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8050 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c955f15f4c60 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e77c870ec6a02d2ae564e1efa698cb3f59cd754f |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8050/7/testReport/ |\r\n   | Max. process+thread count | 618 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8050/7/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 merged PR #8050:\nURL: https://github.com/apache/hadoop/pull/8050\n\n\n"], "labels": ["pull-request-available"], "summary": "Adding low traffic request priority (behind a config flag) for prefetches to red", "qna": [{"question": "What is the issue title?", "answer": "ABFS: Adding request priority for prefetches"}, {"question": "Who reported this issue?", "answer": "Manika Joshi"}]}
{"key": "HADOOP-19734", "project": "HADOOP", "title": "S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\"", "status": "Resolved", "reporter": "Steve Loughran", "created": "2025-10-23T15:42:38.000+0000", "description": "\r\nExperienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests.\r\n\r\nHypothesis: a transient failure meant the server receiving the POST calls to complete the uploads was mistakenly reporting no upload IDs.\r\n\r\nOutcome: all active write operations failed, without any retry attempts. This can lose data and fail jobs, even though the store may recover.\r\n\r\nProposed. The multipart uploads, especially block output stream, retry on this error; treat it as a connectivity issue. ", "comments": ["\r\n{code}\r\n[ERROR]   ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1)                                                                                                                                                                                   \r\n[ERROR]   ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit                                                                                                                                                      \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)                                                                                                                     \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                                                                                                  \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src  \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src      \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src            \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src          \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1)                                                                                                           \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                   \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                             \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                 \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                       \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                     \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1)                                                                                                                                                                                       \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src                                                                                                                     \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src     \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src         \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src               \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src             \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1)                                                                                                                   \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src                                                                                                                 \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src     \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src           \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src         \r\n[ERROR]   ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1)                                                                                        \r\n[ERROR]   ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                                                             \r\n[ERROR]   ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src     \r\n[ERROR]   ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                   \r\n[INFO] \r\n[ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13\r\n[INFO] \r\n{code}\r\n\r\nThis has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h\r\n\r\nWhen these uploads fail we do leave incomplete uploads in progress:\r\n{code}\r\nListing uploads under path \"\"\r\njob-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98\r\njob-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I\r\njob-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4\r\njob-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV\r\njob-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh\r\njob-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC\r\njob-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw--\r\njob-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq\r\ntest/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA--\r\ntest/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA--\r\nTotal 10 uploads found.\r\n{code}\r\n\r\nMost interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. \r\n\r\nThe attempt to complete failed.\r\n{code}\r\n[ERROR] org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads -- Time elapsed: 2.783 s <<< ERROR!\r\norg.apache.hadoop.fs.s3a.AWSBadRequestException: Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1)\r\n        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:265)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)\r\n        at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:318)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:370)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.lambda$complete$3(S3ABlockOutputStream.java:1227)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:493)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:1225)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$1500(S3ABlockOutputStream.java:876)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:545)\r\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\r\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\n        at org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.createFileWithFlags(ITestS3APutIfMatchAndIfNoneMatch.java:190)\r\n        at org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads(ITestS3APutIfMatchAndIfNoneMatch.java:380)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:113)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:61)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.retryPolicyDisallowedRetryException(RetryableStageHelper.java:168)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:73)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\n        at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\r\n        at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\n        at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\r\n        at software.amazon.awssdk.services.s3.DefaultS3Client.completeMultipartUpload(DefaultS3Client.java:801)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$completeMultipartUpload$1(DelegatingS3Client.java:611)\r\n        at software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.completeMultipartUpload(DelegatingS3Client.java:611)\r\n        at org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.completeMultipartUpload(S3AStoreImpl.java:906)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelperCallbacksImpl.completeMultipartUpload(S3AFileSystem.java:1953)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:324)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)\r\n        ... 18 more\r\n\r\n{code}\r\n\r\nYet the uploads list afterwards finds it\r\n{code}\r\njob-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4\r\n{code}\r\n", "And stack on a write failure. \r\n{code}\r\n[ERROR] org.apache.hadoop.fs.s3a.scale.ITestS3AHugeFilesArrayBlocks.test_010_CreateHugeFile -- Time elapsed: 2.870 s <<< ERROR!\r\norg.apache.hadoop.fs.s3a.AWSBadRequestException: Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)\r\n        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:265)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)\r\n        at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:318)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:370)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.lambda$complete$3(S3ABlockOutputStream.java:1227)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:493)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:1225)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$1500(S3ABlockOutputStream.java:876)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:545)\r\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\r\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\n        at org.apache.hadoop.fs.s3a.scale.AbstractSTestS3AHugeFiles.test_010_CreateHugeFile(AbstractSTestS3AHugeFiles.java:276)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:113)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:61)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.retryPolicyDisallowedRetryException(RetryableStageHelper.java:168)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:73)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\n        at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\r\n        at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\n        at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\r\n        at software.amazon.awssdk.services.s3.DefaultS3Client.completeMultipartUpload(DefaultS3Client.java:801)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$completeMultipartUpload$1(DelegatingS3Client.java:611)\r\n        at software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.completeMultipartUpload(DelegatingS3Client.java:611)\r\n        at org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.completeMultipartUpload(S3AStoreImpl.java:906)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelperCallbacksImpl.completeMultipartUpload(S3AFileSystem.java:1953)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:324)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)\r\n        ... 17 more\r\n\r\n{code}\r\n\r\nwe'd have to map 400 + the error text to a \"MultipartUploadCompleteFailed\" exception and add a policy for it, leaving other 400s as unrecoverable.", "+ any tracking in block output stream should record when the POST to initiate the MPU was issued. That way if an error still surfaces but the output stream has been open for three days, we have a good cause \"stream open too long\"", "\r\nthis is actually me making a mess of checksum config\r\n\r\nif the sdk checksum clalculation is set to \"always\" then the user MUST choose a checksum algorithm for s3 uploads (proposed: CRC32). \r\n\r\nI\"m going to leave checksum calculation off by default for performance and compatibility"], "labels": [], "summary": "\r\nExperienced transient failure in test run of https://github", "qna": [{"question": "What is the issue title?", "answer": "S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\""}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19733", "project": "HADOOP", "title": "S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`", "status": "Open", "reporter": "Brandon", "created": "2025-10-22T19:56:55.000+0000", "description": "HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found.\r\n\r\nI think the cause is:\r\n * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration`\r\n * However, code paths to load AWS credential provider call `S3AUtils.getInstanceFromReflection`, which uses the classloader that loaded the S3AUtils class. That's likely to be the built-in application classloader, which won't be able to load classes in a Spark job JAR.\r\n\r\nAnd the fix seems small:\r\n * Change `S3AUtils.getInstanceFromReflection` to load classes using the `Configuration`'s classloader. Luckily we already have the Configuration in this method.", "comments": ["I haven't contributed to Hadoop or other Apache projects before, but this approachable for a first contribution. I'll open a PR.", "brandonvin opened a new pull request, #8048:\nURL: https://github.com/apache/hadoop/pull/8048\n\n   \u2026lassloader\r\n   \r\n   \r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Follow-up to [HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) and [HADOOP-19733](https://issues.apache.org/jira/browse/HADOOP-19733) before it.\r\n   \r\n   With `fs.s3a.classloader.isolation` set to `false` in a Spark application, it was still impossible to load a credentials provider class from the Spark application jar.\r\n   \r\n   `fs.s3a.classloader.isolation` works by saving a reference to the intended classloader in the `Configuration`.\r\n   \r\n   However, loading credentials providers goes through\r\n   `S3AUtils#getInstanceFromReflection`, which always used the classloader that loaded `S3AUtils`.\r\n   \r\n   With this patch, credentials providers will be loaded using the `Configuration`'s classloader.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Unit tests in `org.apache.hadoop.fs.s3a.ITestS3AFileSystemIsolatedClassloader`.\r\n   \r\n   Manual testing in a Spark application.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "brandonvin commented on code in PR #8048:\nURL: https://github.com/apache/hadoop/pull/8048#discussion_r2453905622\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java:\n##########\n@@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map<String, String> confToSet, Consumer<FileS\n     }\n   }\n \n-  private Map<String, String> mapOf() {\n-    return new HashMap<>();\n-  }\n-\n-  private Map<String, String> mapOf(String key, String value) {\n-    HashMap<String, String> m = new HashMap<>();\n-    m.put(key, value);\n-    return m;\n-  }\n\nReview Comment:\n   Since I added test cases that set 2 key-value pairs, I switched to `Map.of` instead of extending these. Not sure if there was a reason to avoid `Map.of` here.\n\n\n\n", "hadoop-yetus commented on PR #8048:\nURL: https://github.com/apache/hadoop/pull/8048#issuecomment-3435132657\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) |  hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) |  hadoop-tools/hadoop-aws: The patch generated 31 new + 4 unchanged - 0 fixed = 35 total (was 4)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-aws.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/new-spotbugs-hadoop-tools_hadoop-aws.html) |  hadoop-tools/hadoop-aws generated 2 new + 188 unchanged - 0 fixed = 190 total (was 188)  |\r\n   | +1 :green_heart: |  shadedclient  |  15m  9s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   1m 57s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  63m 28s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-aws |\r\n   |  |  Nullcheck of conf at line 655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String)  At S3AUtils.java:655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String)  At S3AUtils.java:[line 645] |\r\n   |  |  Non-virtual method call in org.apache.hadoop.fs.s3a.auth.SignerFactory.createSigner(String, String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String)  At SignerFactory.java:String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String)  At SignerFactory.java:[line 125] |\r\n   | Failed junit tests | hadoop.fs.s3a.auth.TestSignerManager |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8048 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 573c49df2825 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 032c335082f24aef12ee3e002ae1cfd9c5f40507 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/testReport/ |\r\n   | Max. process+thread count | 610 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8048:\nURL: https://github.com/apache/hadoop/pull/8048#issuecomment-3435205771\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) |  hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 50s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) |  hadoop-tools/hadoop-aws: The patch generated 12 new + 4 unchanged - 0 fixed = 16 total (was 4)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/new-spotbugs-hadoop-tools_hadoop-aws.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-aws.html) |  hadoop-tools/hadoop-aws generated 2 new + 188 unchanged - 0 fixed = 190 total (was 188)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m  0s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  63m 47s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-aws |\r\n   |  |  Nullcheck of conf at line 655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String)  At S3AUtils.java:655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String)  At S3AUtils.java:[line 645] |\r\n   |  |  Non-virtual method call in org.apache.hadoop.fs.s3a.auth.SignerFactory.createSigner(String, String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String)  At SignerFactory.java:String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String)  At SignerFactory.java:[line 125] |\r\n   | Failed junit tests | hadoop.fs.s3a.auth.TestSignerManager |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8048 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 55f7cbac0d20 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 249aef5213fa039d252e7f7ae03c060b6c87d94f |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/testReport/ |\r\n   | Max. process+thread count | 616 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on code in PR #8048:\nURL: https://github.com/apache/hadoop/pull/8048#discussion_r2455036391\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java:\n##########\n@@ -37,10 +46,33 @@\n  */\n public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase {\n \n+  private static String customClassName = \"custom.class.name\";\n+\n+  private static class CustomCredentialsProvider implements AwsCredentialsProvider {\n+\n+      public CustomCredentialsProvider() {\n+      }\n+\n+      @Override\n+      public AwsCredentials resolveCredentials() {\n+          return null;\n+      }\n+\n+  }\n+\n   private static class CustomClassLoader extends ClassLoader {\n   }\n \n-  private final ClassLoader customClassLoader = new CustomClassLoader();\n+  private final ClassLoader customClassLoader = spy(new CustomClassLoader());\n+  {\n+    try {\n\nReview Comment:\n   this is a nice way to simulate classloader pain.\r\n   \n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java:\n##########\n@@ -28,6 +29,14 @@\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.s3a.impl.InstantiationIOException;\n+\n+import software.amazon.awssdk.auth.credentials.AwsCredentials;\n\nReview Comment:\n   nit: put the amazon imports in the same group as the junit ones\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java:\n##########\n@@ -100,11 +122,26 @@ public void defaultIsolatedClassloader() throws IOException {\n               .isEqualTo(fs.getClass().getClassLoader())\n               .describedAs(\"the classloader that loaded the fs\");\n     });\n+\n+    Throwable thrown = Assertions.catchThrowable(() -> {\n\nReview Comment:\n   Use our `LambdaTestUtils.intercept()`; it's like the spark one and does the casting checks\r\n   \r\n   ```\r\n   InstantiationIOException ex = intercept(InstantiationIOException.class, () -> { assert...})\r\n   ```\r\n   we have a `assertExceptionContains` to look at the inner stuff, but the assert of L136 is fine.\r\n   \r\n   \n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java:\n##########\n@@ -115,11 +152,31 @@ public void isolatedClassloader() throws IOException {\n               .isEqualTo(fs.getClass().getClassLoader())\n               .describedAs(\"the classloader that loaded the fs\");\n     });\n+\n+    Throwable thrown = Assertions.catchThrowable(() -> {\n\nReview Comment:\n   again `intercept()` and cut the assert at L163\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java:\n##########\n@@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map<String, String> confToSet, Consumer<FileS\n     }\n   }\n \n-  private Map<String, String> mapOf() {\n-    return new HashMap<>();\n-  }\n-\n-  private Map<String, String> mapOf(String key, String value) {\n-    HashMap<String, String> m = new HashMap<>();\n-    m.put(key, value);\n-    return m;\n-  }\n\nReview Comment:\n   It's because we only switched to java17 yesterday! And in trunk only.\r\n   \r\n   If you want to see this change in Hadoop 3.4.3 it'll still need to be java8 code, so this needs to be restored. Otherwise: trunk/3.5.0 only\n\n\n\n", "Ok, will also update the custom signer loading to use the configuration, for consistency.", "brandonvin commented on code in PR #8048:\nURL: https://github.com/apache/hadoop/pull/8048#discussion_r2456994430\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java:\n##########\n@@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map<String, String> confToSet, Consumer<FileS\n     }\n   }\n \n-  private Map<String, String> mapOf() {\n-    return new HashMap<>();\n-  }\n-\n-  private Map<String, String> mapOf(String key, String value) {\n-    HashMap<String, String> m = new HashMap<>();\n-    m.put(key, value);\n-    return m;\n-  }\n\nReview Comment:\n   Thanks, makes sense!\n\n\n\n"], "labels": ["pull-request-available"], "summary": "HADOOP-18993 added the option `fs", "qna": [{"question": "What is the issue title?", "answer": "S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`"}, {"question": "Who reported this issue?", "answer": "Brandon"}]}
{"key": "HADOOP-19732", "project": "HADOOP", "title": "Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)", "status": "Resolved", "reporter": "Karthik Palanisamy", "created": "2025-10-21T18:43:38.000+0000", "description": "The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone.\r\n\r\nAfter the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will take time, and during this period, other jobs are still catching up with the new realm configuration. However, the namenode down disrupts all of them.\r\n\r\n\u00a0\r\n{code:java}\r\nERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception\r\njava.lang.IllegalArgumentException: Illegal principal name hive/xxxxx@yyyy.COM\r\norg.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:51)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:43)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1417)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1401)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.getUser(AbstractDelegationTokenIdentifier.java:80)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.getUser(DelegationTokenIdentifier.java:81)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.toString(DelegationTokenIdentifier.java:91)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.String.valueOf(String.java:2994)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.StringBuilder.append(StringBuilder.java:137)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.formatTokenId(AbstractDelegationTokenSecretManager.java:58)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.logExpireTokens(AbstractDelegationTokenSecretManager.java:642)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:635)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:51)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:694)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.authentication.util.KerberosName.getShortName(KerberosName.java:429)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:48)\r\n\u00a0 \u00a0 \u00a0 \u00a0 ... 14 more {code}\r\n\u00a0", "comments": ["looks a duplicate of HDFS-17138.\r\n\r\n[~kpalanisamy] please set the hadoop version you saw it with. If it is a version without HDFS-17138 -please upgrade.\r\n\r\nclosing as a duplicate. If it surfaces on branches with HDFS-17138, re-open", "You\u2019re right [~stevel@apache.org].\u00a0My user version is 3.1.1, so it is missing this HDFS-17138 fix, which clearly addresses my user scenarios. Thanks for checking so quickly. I should have checked it, but unfortunately I taken your time :)\u00a0"], "labels": [], "summary": "The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, ", "qna": [{"question": "What is the issue title?", "answer": "Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)"}, {"question": "Who reported this issue?", "answer": "Karthik Palanisamy"}]}
{"key": "HADOOP-19731", "project": "HADOOP", "title": "Fix SpotBugs warnings introduced after SpotBugs version upgrade.", "status": "In Progress", "reporter": "Shilun Fan", "created": "2025-10-19T09:57:26.000+0000", "description": "Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged.\r\nWe plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.", "comments": ["Hi [~slfan1989]\u00a0\r\nThanks for tracking this.\r\n\r\nWe do have a bunch of PRs open that are facing issue reported here.\r\nWhat are the expectations here? Do we need to address all warnings with the PR itself or they can be ignored and taken later as part of this Jira?\r\n\r\nI think later would be better. It will help keep the changes in PR limited to what is being done and will ease the review process.", "I agree with your point. We\u2019ll work on submitting a common PR that includes a SpotBugs rule to temporarily suppress the new static analysis warnings and restore the state back to the 4.2.0 level.", "Sounds awesome.\r\nThanks for all the efforts.", "zhtttylz opened a new pull request, #8053:\nURL: https://github.com/apache/hadoop/pull/8053\n\n   ### Description of PR\r\n   HADOOP-19731. Fix SpotBugs warnings introduced after SpotBugs version upgrade.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran `mvn -Dspotbugs.skip=false spotbugs:spotbugs` on affected modules and verified the build no longer fails on SpotBugs warnings. No functional code changes, config-only.\r\n   \r\n   ### For code changes:\r\n   \r\n   - Add a project-wide baseline at dev-support/findbugs-exclude-global.xml. \r\n   - Consolidate SpotBugs plugin config in affected module POMs to consistently include local excludes and the new global baseline. \r\n   - Wire the global baseline from hadoop-project-dist/pom.xml; introduce a root path property to reference the repository root. \n\n\n", "slfan1989 commented on PR #8053:\nURL: https://github.com/apache/hadoop/pull/8053#issuecomment-3470749232\n\n   @zhtttylz Thank you for following up on this issue.\r\n   \n\n\n", "hadoop-yetus commented on PR #8053:\nURL: https://github.com/apache/hadoop/pull/8053#issuecomment-3471069512\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 39s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  15m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 19s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  mvnsite  |   6m 23s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 21s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 44s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  71m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 24s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  25m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  1s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   3m 41s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 15s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 38s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 589m 16s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 48s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   | 734m 52s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler |\r\n   |   | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.hdfs.TestDecommission |\r\n   |   | hadoop.hdfs.tools.TestDFSAdmin |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8053 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient |\r\n   | uname | Linux 2197de4c49c9 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c46c6afc99a346474f5b255749c5a86ae4de90bc |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/testReport/ |\r\n   | Max. process+thread count | 4391 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project-dist hadoop-common-project/hadoop-minikdc hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-nfs hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-mapreduce-project hadoop-tools/hadoop-streaming hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-rumen hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-datajoin hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-aliyun hadoop-tools/hadoop-sls hadoop-tools/hadoop-fs2img hadoop-tools/hadoop-gcp hadoop-tools/hadoop-benchmark hadoop-cloud-storage-project/hadoop-cos hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-tos . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8053:\nURL: https://github.com/apache/hadoop/pull/8053#issuecomment-3484495232\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |  14m 34s |  |  Docker failed to build run-specific yetus/hadoop:tp-29464}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8053 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/4/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #8053:\nURL: https://github.com/apache/hadoop/pull/8053#issuecomment-3522409635\n\n   @steveloughran @szetszwo We propose adding a global configuration to temporarily suppress the warnings generated by the new rules introduced in SpotBugs 4.9.7. Would this approach be acceptable to you?\n\n\n", "szetszwo commented on PR #8053:\nURL: https://github.com/apache/hadoop/pull/8053#issuecomment-3522708552\n\n   > ...  adding a global configuration to temporarily suppress the warnings generated by the new rules introduced in SpotBugs 4.9.7.  ...\r\n   \r\n   @slfan1989, @zhtttylz  , it sounds good.  Thanks a lot for fixing the warnings!\n\n\n", "steveloughran commented on PR #8053:\nURL: https://github.com/apache/hadoop/pull/8053#issuecomment-3562296634\n\n   suppressing the new reports would be mostly good (they are overreactions/false alarms), we may lose some real issues on the way.\r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "Following the upgrade to SpotBugs {*}4", "qna": [{"question": "What is the issue title?", "answer": "Fix SpotBugs warnings introduced after SpotBugs version upgrade."}, {"question": "Who reported this issue?", "answer": "Shilun Fan"}]}
{"key": "HADOOP-19730", "project": "HADOOP", "title": "upgrade bouncycastle to 1.82 due to CVE-2025-8916", "status": "Resolved", "reporter": "PJ Fanning", "created": "2025-10-19T09:09:36.000+0000", "description": "https://github.com/advisories/GHSA-4cx2-fc23-5wg6\r\n\r\nThought it was tidier to upgrade to latest version even if the fix was a while ago.", "comments": ["pjfanning opened a new pull request, #8039:\nURL: https://github.com/apache/hadoop/pull/8039\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   HADOOP-19730\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8039:\nURL: https://github.com/apache/hadoop/pull/8039#issuecomment-3420366545\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 51s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 12s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 28s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  mvnsite  |   8m 58s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 36s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 36s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  27m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 49s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 37s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   7m  1s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 42s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  45m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 807m 40s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1064m 10s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList |\r\n   |   | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |\r\n   |   | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes |\r\n   |   | hadoop.hdfs.TestRollingUpgrade |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   |   | hadoop.yarn.service.TestYarnNativeServices |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8039 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs |\r\n   | uname | Linux 66cf96c27f49 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 695a0a30232b143ec8837d6a6648344ffd4efec0 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/testReport/ |\r\n   | Max. process+thread count | 4498 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #8039:\nURL: https://github.com/apache/hadoop/pull/8039\n\n\n", "slfan1989 commented on PR #8039:\nURL: https://github.com/apache/hadoop/pull/8039#issuecomment-3424343993\n\n   @pjfanning Thanks for the contribution! Merged into trunk. Could we also open a PR for branch-3.4?\n\n\n", "pjfanning opened a new pull request, #8047:\nURL: https://github.com/apache/hadoop/pull/8047\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   backport #6976\r\n   \r\n   * HADOOP-19730. Upgrade Bouncycastle to 1.82 due to CVE-2025-8916\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8047:\nURL: https://github.com/apache/hadoop/pull/8047#issuecomment-3438232725\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  12m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   2m 54s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  39m 30s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |  18m 44s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  17m 41s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  22m  5s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 20s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 36s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  50m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  29m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 51s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  18m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 11s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  52m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 720m 55s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 49s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1024m 55s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.gridmix.TestGridmixSubmission |\r\n   |   | hadoop.mapred.gridmix.TestLoadJob |\r\n   |   | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8047 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs |\r\n   | uname | Linux d69a46a8ee67 5.15.0-160-generic #170-Ubuntu SMP Wed Oct 1 10:06:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / c8b8fb4e82d33a470f10a447f4799cc872fb3c01 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/testReport/ |\r\n   | Max. process+thread count | 3660 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #8047:\nURL: https://github.com/apache/hadoop/pull/8047\n\n\n", "slfan1989 commented on PR #8047:\nURL: https://github.com/apache/hadoop/pull/8047#issuecomment-3449617778\n\n   @pjfanning Thanks for the contribution! Merged into trunk.\n\n\n"], "labels": ["pull-request-available"], "summary": "https://github", "qna": [{"question": "What is the issue title?", "answer": "upgrade bouncycastle to 1.82 due to CVE-2025-8916"}, {"question": "Who reported this issue?", "answer": "PJ Fanning"}]}
{"key": "HADOOP-19729", "project": "HADOOP", "title": "ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively", "status": "Open", "reporter": "Anuj Modi", "created": "2025-10-17T05:41:02.000+0000", "description": "It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be bad for perf.\r\n\r\nIn this effort we try to identify such connections and close them so that new good connetions can be established and perf can be improved. There are two parts of this effort.\r\n # Identifying Tailing Requests: This involves profiling all the network calls and getting percentiles value optimally. By default we consider p99 as the tail latency and all the future requests taking more than tail latency will be considere as Tailing requests.\r\n\r\n # Proactively Killing Socket Connections: With Apache client, we can now kill the socket connection and fail the tailing request. Such failures will not be thrown back to user and retried immediately without any sleep but from another socket connection.", "comments": ["hadoop-yetus commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3448411577\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  11m  8s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 53s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 43 new + 3 unchanged - 0 fixed = 46 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 34 new + 1472 unchanged - 0 fixed = 1506 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 33 new + 1413 unchanged - 0 fixed = 1446 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 177 unchanged - 1 fixed = 181 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  70m  0s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:[line 533] |\r\n   |  |  new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:[line 55] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:[line 81] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8043 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux d98c9c1604f2 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7dcac93eec4dc5a48d643ae81372c581b6c3bebf |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/testReport/ |\r\n   | Max. process+thread count | 614 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3449602331\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 43 new + 3 unchanged - 0 fixed = 46 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 35 new + 1472 unchanged - 0 fixed = 1507 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 34 new + 1413 unchanged - 0 fixed = 1447 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 177 unchanged - 1 fixed = 181 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  60m 12s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:[line 533] |\r\n   |  |  new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:[line 55] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:[line 81] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8043 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 9b5c6baa74d5 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / de244d215362fca4d8ba16b3d01a9f39a3ff0e81 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/testReport/ |\r\n   | Max. process+thread count | 637 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2464810768\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1824,4 +1860,41 @@ public int getBlobRenameDirConsumptionParallelism() {\n   public int getBlobDeleteDirConsumptionParallelism() {\n     return blobDeleteDirConsumptionParallelism;\n   }\n+\n+  public boolean isTailLatencyTrackerEnabled() {\n+    return isTailLatencyTrackerEnabled;\n+  }\n+\n+  public boolean isTailLatencyRequestTimeoutEnabled() {\n+    return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled\n\nReview Comment:\n   first check should be for isTailLatencyTrackerEnabled\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2464820798\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1824,4 +1860,41 @@ public int getBlobRenameDirConsumptionParallelism() {\n   public int getBlobDeleteDirConsumptionParallelism() {\n     return blobDeleteDirConsumptionParallelism;\n   }\n+\n+  public boolean isTailLatencyTrackerEnabled() {\n+    return isTailLatencyTrackerEnabled;\n+  }\n+\n+  public boolean isTailLatencyRequestTimeoutEnabled() {\n+    return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled\n+        && getPreferredHttpOperationType().equals(HttpOperationType.APACHE_HTTP_CLIENT);\n+  }\n+\n+  public int getTailLatencyPercentile() {\n+    return tailLatencyPercentile;\n+  }\n+\n+  public int getTailLatencyMinDeviation() {\n+    return tailLatencyMinDeviation;\n+  }\n+\n+  public int getTailLatencyMinSampleSize() {\n+    return tailLatencyMinSampleSize;\n+  }\n+\n+  public int getTailLatencyAnalysisWindowInMillis() {\n+    return tailLatencyAnalysisWindowInMillis;\n+  }\n+\n+  public int getTailLatencyPercentileComputationIntervalInMillis() {\n\nReview Comment:\n   Name should be shortened\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465174540\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -266,5 +266,15 @@ public final class FileSystemConfigurations {\n \n   public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true;\n \n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false;\n\nReview Comment:\n   Do we not want this feature to be enabled by default ?\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465238391\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java:\n##########\n@@ -143,6 +173,51 @@ public HttpResponse execute(HttpRequestBase httpRequest,\n     return httpClient.execute(httpRequest, abfsHttpClientContext);\n   }\n \n+  /**\n+   * Executes the HTTP request with a deadline. If the request does not complete\n+   * within the deadline, it is aborted and an IOException is thrown.\n+   *\n+   * @param httpRequest HTTP request to execute.\n+   * @param abfsHttpClientContext HttpClient context.\n+   * @param connectTimeout Connection timeout.\n+   * @param readTimeout Read timeout.\n+   * @param deadlineMillis Deadline in milliseconds.\n+   *\n+   * @return HTTP response.\n+   * @throws IOException network error or deadline exceeded.\n+   */\n+  public HttpResponse executeWithDeadline(HttpRequestBase httpRequest,\n+      final AbfsManagedHttpClientContext abfsHttpClientContext,\n+      final int connectTimeout,\n+      final int readTimeout,\n+      final long deadlineMillis) throws IOException {\n+    RequestConfig.Builder requestConfigBuilder = RequestConfig\n+        .custom()\n+        .setConnectTimeout(connectTimeout)\n+        .setSocketTimeout(readTimeout);\n+    httpRequest.setConfig(requestConfigBuilder.build());\n+    ExecutorService executor = Executors.newSingleThreadExecutor();\n+    Future<HttpResponse> future = executor.submit(() ->\n+        httpClient.execute(httpRequest, abfsHttpClientContext)\n+    );\n+\n+    try {\n+      return future.get(deadlineMillis, TimeUnit.MILLISECONDS);\n+    } catch (TimeoutException e) {\n+      /* Deadline exceeded, abort the request.\n+       * This will also kill the underlying socket exception in the HttpClient.\n+       * Connection will be marker stale and won't be returned back to KAC for reuse.\n\nReview Comment:\n   nit: typo marked\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465280967\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/RetryPolicyConstants.java:\n##########\n@@ -32,4 +32,8 @@ private RetryPolicyConstants() {\n    * Constant for Static Retry Policy Abbreviation. {@value}\n    */\n   public static final String STATIC_RETRY_POLICY_ABBREVIATION = \"S\";\n+  /**\n+   * Constant for Static Retry Policy Abbreviation. {@value}\n+   */\n+  public static final String TAIL_LATENCY_TIMEOUT_RETRY_POLICY_ABBREVIATION = \"T\";\n\nReview Comment:\n   Can we make it TL ?\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465316539\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n\nReview Comment:\n   Should be numberOfSegments in exception\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465371723\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n\nReview Comment:\n   We can return here itself\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465396005\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n+          operationType, getCurrentTotalCount());\n+    } else {\n+      rotateLock.lock();\n+      try {\n+        tmpForMerge.reset();\n+        for (int i = 0; i < numSegments; i++) {\n+          Histogram h = completedSegments[i];\n+          if (h != null && h.getTotalCount() > 0) {\n+            tmpForMerge.add(h);\n+          }\n+        }\n+\n+        if (tmpForMerge.getTotalCount() == 0) return;\n+\n+        tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile);\n+        p50 = tmpForMerge.getValueAtPercentile(50);\n+        p90 = tmpForMerge.getValueAtPercentile(90);\n+        p99 = tmpForMerge.getValueAtPercentile(99);\n+        deviation = (int) ((tailLatency - p50)/p50 * 100);\n\nReview Comment:\n   Chances of division by zero error\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465418474\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n+          operationType, getCurrentTotalCount());\n+    } else {\n+      rotateLock.lock();\n+      try {\n+        tmpForMerge.reset();\n+        for (int i = 0; i < numSegments; i++) {\n+          Histogram h = completedSegments[i];\n+          if (h != null && h.getTotalCount() > 0) {\n+            tmpForMerge.add(h);\n+          }\n+        }\n+\n+        if (tmpForMerge.getTotalCount() == 0) return;\n+\n+        tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile);\n+        p50 = tmpForMerge.getValueAtPercentile(50);\n+        p90 = tmpForMerge.getValueAtPercentile(90);\n+        p99 = tmpForMerge.getValueAtPercentile(99);\n+        deviation = (int) ((tailLatency - p50)/p50 * 100);\n+      } finally {\n+        rotateLock.unlock();\n+      }\n+    }\n+    LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\",\n+        operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount());\n+  }\n+\n+  private long alignToSegmentDuration(long timeMs) {\n+    return timeMs - (timeMs % timeSegmentDurationMillis);\n+  }\n+\n+  /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */\n+  public void rotateIfNeeded() {\n+    LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType);\n+    long now = System.currentTimeMillis();\n+    long expectedStart = alignToSegmentDuration(now);\n+    if (expectedStart == currentSegmentStartMillis) {\n+      LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart);\n+      return; // still current\n+    }\n+\n+    rotateLock.lock();\n+    try {\n+      // Re-check inside lock\n+      now = System.currentTimeMillis();\n+      expectedStart = alignToSegmentDuration(now);\n+      if (expectedStart == currentSegmentStartMillis) return;\n+\n+      // Finalize the current bucket:\n+      // Pull any remaining deltas from active recorder and add to currentAccumulation\n+      tmpForDelta.reset();\n+      activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta);\n+      currentSegmentAccumulation.add(tmpForDelta);\n+\n+      if (currentSegmentAccumulation.getTotalCount() <= 0) {\n\nReview Comment:\n   Is less than 0 possible for total count ? It is increment always right \n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465418474\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n+          operationType, getCurrentTotalCount());\n+    } else {\n+      rotateLock.lock();\n+      try {\n+        tmpForMerge.reset();\n+        for (int i = 0; i < numSegments; i++) {\n+          Histogram h = completedSegments[i];\n+          if (h != null && h.getTotalCount() > 0) {\n+            tmpForMerge.add(h);\n+          }\n+        }\n+\n+        if (tmpForMerge.getTotalCount() == 0) return;\n+\n+        tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile);\n+        p50 = tmpForMerge.getValueAtPercentile(50);\n+        p90 = tmpForMerge.getValueAtPercentile(90);\n+        p99 = tmpForMerge.getValueAtPercentile(99);\n+        deviation = (int) ((tailLatency - p50)/p50 * 100);\n+      } finally {\n+        rotateLock.unlock();\n+      }\n+    }\n+    LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\",\n+        operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount());\n+  }\n+\n+  private long alignToSegmentDuration(long timeMs) {\n+    return timeMs - (timeMs % timeSegmentDurationMillis);\n+  }\n+\n+  /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */\n+  public void rotateIfNeeded() {\n+    LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType);\n+    long now = System.currentTimeMillis();\n+    long expectedStart = alignToSegmentDuration(now);\n+    if (expectedStart == currentSegmentStartMillis) {\n+      LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart);\n+      return; // still current\n+    }\n+\n+    rotateLock.lock();\n+    try {\n+      // Re-check inside lock\n+      now = System.currentTimeMillis();\n+      expectedStart = alignToSegmentDuration(now);\n+      if (expectedStart == currentSegmentStartMillis) return;\n+\n+      // Finalize the current bucket:\n+      // Pull any remaining deltas from active recorder and add to currentAccumulation\n+      tmpForDelta.reset();\n+      activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta);\n+      currentSegmentAccumulation.add(tmpForDelta);\n+\n+      if (currentSegmentAccumulation.getTotalCount() <= 0) {\n\nReview Comment:\n   Is less than 0 possible for total count? It is incremented always right \n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465446936\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n+          operationType, getCurrentTotalCount());\n+    } else {\n+      rotateLock.lock();\n+      try {\n+        tmpForMerge.reset();\n+        for (int i = 0; i < numSegments; i++) {\n+          Histogram h = completedSegments[i];\n+          if (h != null && h.getTotalCount() > 0) {\n+            tmpForMerge.add(h);\n+          }\n+        }\n+\n+        if (tmpForMerge.getTotalCount() == 0) return;\n+\n+        tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile);\n+        p50 = tmpForMerge.getValueAtPercentile(50);\n+        p90 = tmpForMerge.getValueAtPercentile(90);\n+        p99 = tmpForMerge.getValueAtPercentile(99);\n+        deviation = (int) ((tailLatency - p50)/p50 * 100);\n+      } finally {\n+        rotateLock.unlock();\n+      }\n+    }\n+    LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\",\n+        operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount());\n+  }\n+\n+  private long alignToSegmentDuration(long timeMs) {\n+    return timeMs - (timeMs % timeSegmentDurationMillis);\n+  }\n+\n+  /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */\n+  public void rotateIfNeeded() {\n+    LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType);\n+    long now = System.currentTimeMillis();\n+    long expectedStart = alignToSegmentDuration(now);\n+    if (expectedStart == currentSegmentStartMillis) {\n+      LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart);\n+      return; // still current\n+    }\n+\n+    rotateLock.lock();\n+    try {\n+      // Re-check inside lock\n+      now = System.currentTimeMillis();\n+      expectedStart = alignToSegmentDuration(now);\n+      if (expectedStart == currentSegmentStartMillis) return;\n+\n+      // Finalize the current bucket:\n+      // Pull any remaining deltas from active recorder and add to currentAccumulation\n+      tmpForDelta.reset();\n+      activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta);\n+      currentSegmentAccumulation.add(tmpForDelta);\n+\n+      if (currentSegmentAccumulation.getTotalCount() <= 0) {\n+        currentSegmentStartMillis = alignToSegmentDuration(System.currentTimeMillis());\n+        LOG.debug(\"[{}] No data recorded in current time segment at {}. Skipping Rotation. Current Index is {}.\",\n+            operationType, currentSegmentStartMillis, currentIndex.get());\n+        return;\n+      }\n+\n+      LOG.debug(\"[{}] Rotating current segment with total count {} into slot {}\",\n+          operationType, currentSegmentAccumulation.getTotalCount(), currentIndex.get());\n+\n+      // Place the finished currentAccumulation into the ring buffer slot ahead.\n+      int currentIdx = (currentIndex.getAndIncrement()) % numSegments;\n+      // Next slot is now going to be eradicated. Remove its count from total.\n+      currentTotalCount.set(currentTotalCount.get() - (completedSegments[currentIdx] == null ? 0 : completedSegments[currentIdx].getTotalCount()));\n+      // Store an immutable snapshot (make sure we don't mutate the instance after storing)\n+      completedSegments[currentIdx] = currentSegmentAccumulation;\n\nReview Comment:\n   how are we making sure that this is immutable after this point ? completedSegments[currentIdx] = currentSegmentAccumulation.copy(); ideally we should create a deep copy of the histogram data so that even if currentSegmentAccumulation is reused or reset for the next segment, the data in completedSegments remains unchanged.\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465452348\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n+          operationType, getCurrentTotalCount());\n+    } else {\n+      rotateLock.lock();\n+      try {\n+        tmpForMerge.reset();\n+        for (int i = 0; i < numSegments; i++) {\n+          Histogram h = completedSegments[i];\n+          if (h != null && h.getTotalCount() > 0) {\n+            tmpForMerge.add(h);\n+          }\n+        }\n+\n+        if (tmpForMerge.getTotalCount() == 0) return;\n+\n+        tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile);\n+        p50 = tmpForMerge.getValueAtPercentile(50);\n+        p90 = tmpForMerge.getValueAtPercentile(90);\n+        p99 = tmpForMerge.getValueAtPercentile(99);\n+        deviation = (int) ((tailLatency - p50)/p50 * 100);\n+      } finally {\n+        rotateLock.unlock();\n+      }\n+    }\n+    LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\",\n+        operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount());\n+  }\n+\n+  private long alignToSegmentDuration(long timeMs) {\n+    return timeMs - (timeMs % timeSegmentDurationMillis);\n+  }\n+\n+  /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */\n+  public void rotateIfNeeded() {\n+    LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType);\n+    long now = System.currentTimeMillis();\n\nReview Comment:\n   what is the use of the variable now ? We can directly use System.currentTimeMillis(); in expectedStart as we are doing later in line 195\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465506034\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -114,6 +116,7 @@ public class AbfsRestOperation {\n    */\n   private String failureReason;\n   private AbfsRetryPolicy retryPolicy;\n+  private boolean shouldTailLatencyTimeout = true;\n\nReview Comment:\n   Can be renamed to enableTailLatencyTimeout\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465510352\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -531,6 +544,12 @@ private boolean executeHttpOperation(final int retryCount,\n       if (shouldUpdateCSTMetrics(statusCode) && !wasKnownExceptionThrown) {\n         intercept.updateMetrics(operationType, httpOperation);\n       }\n+\n+      // Update Tail Latency Tracker only for successful requests.\n+      if (tailLatencyTracker != null && statusCode <  HttpURLConnection.HTTP_MULT_CHOICE) {\n\nReview Comment:\n   Will get updated for -1 status code as well, should be checked between 200 to 300\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465561509\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestSlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,162 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestSlidingWindowHdrHistogram {\n+\n+  @Test\n+  public void testSlidingWindowHdrHistogram() throws Exception {\n+    SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram(\n\nReview Comment:\n   add comment for which value represents what\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2465576049\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestSlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,162 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestSlidingWindowHdrHistogram {\n+\n+  @Test\n+  public void testSlidingWindowHdrHistogram() throws Exception {\n+    SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram(\n+        100,\n+        5,\n+        7,\n+        99,\n+        0,\n+        100,\n+        3,\n+        AbfsRestOperationType.GetPathStatus);\n+\n+    // Verify that the histogram is created successfully with default values and\n+    // do not report any percentiles\n+    assertThat(histogram).isNotNull();\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(0);\n+    assertThat(histogram.getCurrentIndex()).isEqualTo(0);\n+    assertThat(histogram.getP50()).isEqualTo(0.0);\n+    assertThat(histogram.getTailLatency()).isEqualTo(0.0);\n+\n+    // Verify that recording values works as expected\n+    addAndRotate(histogram, 10, 5); // Add 5 values of 10\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(5);\n+\n+    // Verify that percentiles are not computed with insufficient samples\n+    assertThat(histogram.getP50()).isEqualTo(0.0);\n+    assertThat(histogram.getTailLatency()).isEqualTo(0.0);\n+\n+    // Record more values to exceed the minimum sample size\n+    addAndRotate(histogram, 20, 5); // Add 5 values of 20\n+\n+    // Verify that percentiles are now computed but tail Latency is still not reported\n+    assertThat(histogram.getP50()).isGreaterThan(0.0);\n+    assertThat(histogram.getTailLatency()).isEqualTo(0.0);\n+\n+    // Record more values and rotate histogram to fill whole analysis window\n+    addAndRotate(histogram, 30, 5); // Add 5 values of 30\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(15);\n+\n+    // Verify that analysis window is not full until full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isFalse();\n+\n+    addAndRotate(histogram, 60, 5); // Add 5 values of 60\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(20);\n+\n+    // Verify that analysis window is not full until full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isFalse();\n+\n+    // Verify that rotation is skipped if nothing new recorded and hence window not filled\n+    addAndRotate(histogram, 100, 0); // No new values added\n+    assertThat(histogram.isAnalysisWindowFilled()).isFalse();\n+\n+    // Verify that rotation does not happen if analysis window is not filled\n+    histogram.rotateIfNeeded();\n+    assertThat(histogram.isAnalysisWindowFilled()).isFalse();\n+\n+    addAndRotate(histogram, 80, 5); // Add 5 values of 80\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(25);\n+\n+    // Verify that analysis window is full after full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isTrue();\n+\n+    // Verify that percentiles and tail latency are computed\n+    assertThat(histogram.getP50()).isGreaterThan(0.0);\n+    assertThat(histogram.getTailLatency()).isGreaterThan(0.0);\n+\n+    // Verify that sliding window works. Old values should be evicted\n+    addAndRotate(histogram, 90, 3); // Add 3 values of 90\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(23);\n+    assertThat(histogram.isAnalysisWindowFilled()).isTrue();\n+  }\n+\n+  @Test\n+  public void testMinDeviationRequirementNotMet() throws Exception {\n+    SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram(\n+        100,\n+        5,\n+        7,\n+        99,\n+        100,\n+        100,\n+        3,\n+        AbfsRestOperationType.GetPathStatus);\n+\n+    // Add values with low deviation\n+    addAndRotate(histogram, 50, 5); // Add 5 values of 50\n+    addAndRotate(histogram, 51, 5); // Add 5 values of 52\n+    addAndRotate(histogram, 52, 5); // Add 5 values of 51\n+    addAndRotate(histogram, 80, 5); // Add 5 values of 53\n+    addAndRotate(histogram, 90, 5); // Add 5 values of 50\n+\n+    // Verify that analysis window is full after full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isTrue();\n+\n+    // Verify that percentiles are not computed due to low deviation\n+    assertThat(histogram.getP50()).isGreaterThan(0.0);\n+    assertThat(histogram.getTailLatency()).isEqualTo(0.0);\n+  }\n+\n+  @Test\n+  public void testMinDeviationRequirementMet() throws Exception {\n+    SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram(\n+        100,\n+        5,\n+        7,\n+        99,\n+        50,\n+        100,\n+        3,\n+        AbfsRestOperationType.GetPathStatus);\n+\n+    // Add values with low deviation\n+    addAndRotate(histogram, 50, 5); // Add 5 values of 50\n+    addAndRotate(histogram, 51, 5); // Add 5 values of 52\n+    addAndRotate(histogram, 52, 5); // Add 5 values of 51\n+    addAndRotate(histogram, 80, 5); // Add 5 values of 53\n+    addAndRotate(histogram, 90, 5); // Add 5 values of 50\n+\n+    // Verify that analysis window is full after full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isTrue();\n+\n+    // Verify that percentiles are not computed due to low deviation\n\nReview Comment:\n   nit: should be computed ?\n\n\n\n", "bhattmanish98 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2469106859\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -266,5 +266,15 @@ public final class FileSystemConfigurations {\n \n   public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true;\n \n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false;\n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT = false;\n+  public static final int DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE = 99;\n\nReview Comment:\n   shouldn't it be float/double instead of int? Tomorrow we can change default percentile to 99.9 or 99.99.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TailLatencyRequestTimeoutException.java:\n##########\n@@ -0,0 +1,39 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.contracts.exceptions;\n+\n+import java.util.concurrent.TimeoutException;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_TAIL_LATENCY_REQUEST_TIMEOUT;\n+\n+/**\n+ * Thrown when a request takes more time than the current reported tail latency.\n+ */\n+public class TailLatencyRequestTimeoutException extends AzureBlobFileSystemException {\n+\n+  /**\n+   * Constructs a TailLatencyRequestTimeoutException with TimeoutException as the cause.\n+   */\n+  public TailLatencyRequestTimeoutException(TimeoutException innerException) {\n\nReview Comment:\n   @param missing in the java doc\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java:\n##########\n@@ -132,6 +138,30 @@ public void close() throws IOException {\n    * @throws IOException network error.\n    */\n   public HttpResponse execute(HttpRequestBase httpRequest,\n+      final AbfsManagedHttpClientContext abfsHttpClientContext,\n+      final int connectTimeout,\n+      final int readTimeout,\n+      final long tailLatencyTimeout) throws IOException {\n+    if (tailLatencyTimeout <= 0) {\n+      return executeWithoutDeadline(httpRequest, abfsHttpClientContext,\n+          connectTimeout, readTimeout);\n+    }\n+    return executeWithDeadline(httpRequest, abfsHttpClientContext,\n+        connectTimeout, readTimeout, tailLatencyTimeout);\n+  }\n+\n+  /**\n+   * Executes the HTTP request.\n+   *\n+   * @param httpRequest HTTP request to execute.\n+   * @param abfsHttpClientContext HttpClient context.\n+   * @param connectTimeout Connection timeout.\n+   * @param readTimeout Read timeout.\n+   *\n+   * @return HTTP response.\n+   * @throws IOException network error.\n+   */\n+  public HttpResponse executeWithoutDeadline(HttpRequestBase httpRequest,\n\nReview Comment:\n   executeWithoutDeadline and executeWithDeadline can be private methods.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TailLatencyRequestTimeoutException.java:\n##########\n@@ -0,0 +1,39 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.contracts.exceptions;\n+\n+import java.util.concurrent.TimeoutException;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_TAIL_LATENCY_REQUEST_TIMEOUT;\n+\n+/**\n+ * Thrown when a request takes more time than the current reported tail latency.\n+ */\n+public class TailLatencyRequestTimeoutException extends AzureBlobFileSystemException {\n+\n+  /**\n+   * Constructs a TailLatencyRequestTimeoutException with TimeoutException as the cause.\n+   */\n+  public TailLatencyRequestTimeoutException(TimeoutException innerException) {\n+    super(ERR_TAIL_LATENCY_REQUEST_TIMEOUT, innerException);\n+  }\n+\n+  public TailLatencyRequestTimeoutException() {\n\nReview Comment:\n   Java doc missing for this\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n\nReview Comment:\n   We can rename this variable to something which is more relevant.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int HISTOGRAM_MAX_VALUE = 60_000;\n+  private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3;\n+  private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram>\n+      operationLatencyMap = new HashMap<>();\n+  private final AbfsConfiguration configuration;\n+\n+  /**\n+   * Constructor to initialize the latency tracker with configuration.\n+   * @param abfsConfiguration Configuration settings for latency tracking.\n+   */\n+  public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) {\n+    this.configuration = abfsConfiguration;\n+    ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Histogram-Rotator-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+    long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis()\n+        / configuration.getTailLatencyAnalysisWindowGranularity();\n+    histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms,\n+        rotationInterval, rotationInterval, TimeUnit.MILLISECONDS);\n+\n+\n+    ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor(\n\nReview Comment:\n   We should close this thread pool and one below once the use is done or at least during filesystem close.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -611,10 +630,30 @@ AbfsJdkHttpOperation createAbfsHttpOperation() throws IOException {\n \n   @VisibleForTesting\n   AbfsAHCHttpOperation createAbfsAHCHttpOperation() throws IOException {\n+    long tailLatency = getTailLatencyTimeoutIfEnabled();\n     return new AbfsAHCHttpOperation(url, method, requestHeaders,\n         Duration.ofMillis(client.getAbfsConfiguration().getHttpConnectionTimeout()),\n         Duration.ofMillis(client.getAbfsConfiguration().getHttpReadTimeout()),\n-        client.getAbfsApacheHttpClient(), client);\n+        tailLatency, client.getAbfsApacheHttpClient(), client);\n+  }\n+\n+  /**\n+   * Get Tail Latency Timeout value if profiling is enabled, timeout is enabled\n+   * and retries due to tail latency request timeout is allowed.\n+   * @return tail latency timeout value else return zero.\n+   */\n+  long getTailLatencyTimeoutIfEnabled() {\n+    if (isTailLatencyTimeoutEnabled() && shouldTailLatencyTimeout) {\n+      return (long) tailLatencyTracker.getTailLatency(this.operationType);\n+    }\n+    return ZERO;\n+  }\n+\n+  boolean isTailLatencyTimeoutEnabled() {\n\nReview Comment:\n   Java doc missing\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470451730\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1824,4 +1860,41 @@ public int getBlobRenameDirConsumptionParallelism() {\n   public int getBlobDeleteDirConsumptionParallelism() {\n     return blobDeleteDirConsumptionParallelism;\n   }\n+\n+  public boolean isTailLatencyTrackerEnabled() {\n+    return isTailLatencyTrackerEnabled;\n+  }\n+\n+  public boolean isTailLatencyRequestTimeoutEnabled() {\n+    return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled\n\nReview Comment:\n   Make sense.\r\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470457723\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1824,4 +1860,41 @@ public int getBlobRenameDirConsumptionParallelism() {\n   public int getBlobDeleteDirConsumptionParallelism() {\n     return blobDeleteDirConsumptionParallelism;\n   }\n+\n+  public boolean isTailLatencyTrackerEnabled() {\n+    return isTailLatencyTrackerEnabled;\n+  }\n+\n+  public boolean isTailLatencyRequestTimeoutEnabled() {\n+    return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled\n+        && getPreferredHttpOperationType().equals(HttpOperationType.APACHE_HTTP_CLIENT);\n+  }\n+\n+  public int getTailLatencyPercentile() {\n+    return tailLatencyPercentile;\n+  }\n+\n+  public int getTailLatencyMinDeviation() {\n+    return tailLatencyMinDeviation;\n+  }\n+\n+  public int getTailLatencyMinSampleSize() {\n+    return tailLatencyMinSampleSize;\n+  }\n+\n+  public int getTailLatencyAnalysisWindowInMillis() {\n+    return tailLatencyAnalysisWindowInMillis;\n+  }\n+\n+  public int getTailLatencyPercentileComputationIntervalInMillis() {\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470459989\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -266,5 +266,15 @@ public final class FileSystemConfigurations {\n \n   public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true;\n \n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false;\n\nReview Comment:\n   There is no value add currently to just enable profling as we are not consuming it anywhere.\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470461231\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java:\n##########\n@@ -143,6 +173,51 @@ public HttpResponse execute(HttpRequestBase httpRequest,\n     return httpClient.execute(httpRequest, abfsHttpClientContext);\n   }\n \n+  /**\n+   * Executes the HTTP request with a deadline. If the request does not complete\n+   * within the deadline, it is aborted and an IOException is thrown.\n+   *\n+   * @param httpRequest HTTP request to execute.\n+   * @param abfsHttpClientContext HttpClient context.\n+   * @param connectTimeout Connection timeout.\n+   * @param readTimeout Read timeout.\n+   * @param deadlineMillis Deadline in milliseconds.\n+   *\n+   * @return HTTP response.\n+   * @throws IOException network error or deadline exceeded.\n+   */\n+  public HttpResponse executeWithDeadline(HttpRequestBase httpRequest,\n+      final AbfsManagedHttpClientContext abfsHttpClientContext,\n+      final int connectTimeout,\n+      final int readTimeout,\n+      final long deadlineMillis) throws IOException {\n+    RequestConfig.Builder requestConfigBuilder = RequestConfig\n+        .custom()\n+        .setConnectTimeout(connectTimeout)\n+        .setSocketTimeout(readTimeout);\n+    httpRequest.setConfig(requestConfigBuilder.build());\n+    ExecutorService executor = Executors.newSingleThreadExecutor();\n+    Future<HttpResponse> future = executor.submit(() ->\n+        httpClient.execute(httpRequest, abfsHttpClientContext)\n+    );\n+\n+    try {\n+      return future.get(deadlineMillis, TimeUnit.MILLISECONDS);\n+    } catch (TimeoutException e) {\n+      /* Deadline exceeded, abort the request.\n+       * This will also kill the underlying socket exception in the HttpClient.\n+       * Connection will be marker stale and won't be returned back to KAC for reuse.\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470463895\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/RetryPolicyConstants.java:\n##########\n@@ -32,4 +32,8 @@ private RetryPolicyConstants() {\n    * Constant for Static Retry Policy Abbreviation. {@value}\n    */\n   public static final String STATIC_RETRY_POLICY_ABBREVIATION = \"S\";\n+  /**\n+   * Constant for Static Retry Policy Abbreviation. {@value}\n+   */\n+  public static final String TAIL_LATENCY_TIMEOUT_RETRY_POLICY_ABBREVIATION = \"T\";\n\nReview Comment:\n   Other Retry policy abbreviations are already single character. Keeping it likewise\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470465366\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470469035\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470478310\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n+          operationType, getCurrentTotalCount());\n+    } else {\n+      rotateLock.lock();\n+      try {\n+        tmpForMerge.reset();\n+        for (int i = 0; i < numSegments; i++) {\n+          Histogram h = completedSegments[i];\n+          if (h != null && h.getTotalCount() > 0) {\n+            tmpForMerge.add(h);\n+          }\n+        }\n+\n+        if (tmpForMerge.getTotalCount() == 0) return;\n+\n+        tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile);\n+        p50 = tmpForMerge.getValueAtPercentile(50);\n+        p90 = tmpForMerge.getValueAtPercentile(90);\n+        p99 = tmpForMerge.getValueAtPercentile(99);\n+        deviation = (int) ((tailLatency - p50)/p50 * 100);\n\nReview Comment:\n   Nice catch.\r\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470481704\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n+          operationType, getCurrentTotalCount());\n+    } else {\n+      rotateLock.lock();\n+      try {\n+        tmpForMerge.reset();\n+        for (int i = 0; i < numSegments; i++) {\n+          Histogram h = completedSegments[i];\n+          if (h != null && h.getTotalCount() > 0) {\n+            tmpForMerge.add(h);\n+          }\n+        }\n+\n+        if (tmpForMerge.getTotalCount() == 0) return;\n+\n+        tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile);\n+        p50 = tmpForMerge.getValueAtPercentile(50);\n+        p90 = tmpForMerge.getValueAtPercentile(90);\n+        p99 = tmpForMerge.getValueAtPercentile(99);\n+        deviation = (int) ((tailLatency - p50)/p50 * 100);\n+      } finally {\n+        rotateLock.unlock();\n+      }\n+    }\n+    LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\",\n+        operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount());\n+  }\n+\n+  private long alignToSegmentDuration(long timeMs) {\n+    return timeMs - (timeMs % timeSegmentDurationMillis);\n+  }\n+\n+  /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */\n+  public void rotateIfNeeded() {\n+    LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType);\n+    long now = System.currentTimeMillis();\n+    long expectedStart = alignToSegmentDuration(now);\n+    if (expectedStart == currentSegmentStartMillis) {\n+      LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart);\n+      return; // still current\n+    }\n+\n+    rotateLock.lock();\n+    try {\n+      // Re-check inside lock\n+      now = System.currentTimeMillis();\n+      expectedStart = alignToSegmentDuration(now);\n+      if (expectedStart == currentSegmentStartMillis) return;\n+\n+      // Finalize the current bucket:\n+      // Pull any remaining deltas from active recorder and add to currentAccumulation\n+      tmpForDelta.reset();\n+      activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta);\n+      currentSegmentAccumulation.add(tmpForDelta);\n+\n+      if (currentSegmentAccumulation.getTotalCount() <= 0) {\n\nReview Comment:\n   Yeah this is primarily for equal to 0\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470498123\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n+          operationType, getCurrentTotalCount());\n+    } else {\n+      rotateLock.lock();\n+      try {\n+        tmpForMerge.reset();\n+        for (int i = 0; i < numSegments; i++) {\n+          Histogram h = completedSegments[i];\n+          if (h != null && h.getTotalCount() > 0) {\n+            tmpForMerge.add(h);\n+          }\n+        }\n+\n+        if (tmpForMerge.getTotalCount() == 0) return;\n+\n+        tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile);\n+        p50 = tmpForMerge.getValueAtPercentile(50);\n+        p90 = tmpForMerge.getValueAtPercentile(90);\n+        p99 = tmpForMerge.getValueAtPercentile(99);\n+        deviation = (int) ((tailLatency - p50)/p50 * 100);\n+      } finally {\n+        rotateLock.unlock();\n+      }\n+    }\n+    LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\",\n+        operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount());\n+  }\n+\n+  private long alignToSegmentDuration(long timeMs) {\n+    return timeMs - (timeMs % timeSegmentDurationMillis);\n+  }\n+\n+  /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */\n+  public void rotateIfNeeded() {\n+    LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType);\n+    long now = System.currentTimeMillis();\n+    long expectedStart = alignToSegmentDuration(now);\n+    if (expectedStart == currentSegmentStartMillis) {\n+      LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart);\n+      return; // still current\n+    }\n+\n+    rotateLock.lock();\n+    try {\n+      // Re-check inside lock\n+      now = System.currentTimeMillis();\n+      expectedStart = alignToSegmentDuration(now);\n+      if (expectedStart == currentSegmentStartMillis) return;\n+\n+      // Finalize the current bucket:\n+      // Pull any remaining deltas from active recorder and add to currentAccumulation\n+      tmpForDelta.reset();\n+      activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta);\n+      currentSegmentAccumulation.add(tmpForDelta);\n+\n+      if (currentSegmentAccumulation.getTotalCount() <= 0) {\n+        currentSegmentStartMillis = alignToSegmentDuration(System.currentTimeMillis());\n+        LOG.debug(\"[{}] No data recorded in current time segment at {}. Skipping Rotation. Current Index is {}.\",\n+            operationType, currentSegmentStartMillis, currentIndex.get());\n+        return;\n+      }\n+\n+      LOG.debug(\"[{}] Rotating current segment with total count {} into slot {}\",\n+          operationType, currentSegmentAccumulation.getTotalCount(), currentIndex.get());\n+\n+      // Place the finished currentAccumulation into the ring buffer slot ahead.\n+      int currentIdx = (currentIndex.getAndIncrement()) % numSegments;\n+      // Next slot is now going to be eradicated. Remove its count from total.\n+      currentTotalCount.set(currentTotalCount.get() - (completedSegments[currentIdx] == null ? 0 : completedSegments[currentIdx].getTotalCount()));\n+      // Store an immutable snapshot (make sure we don't mutate the instance after storing)\n+      completedSegments[currentIdx] = currentSegmentAccumulation;\n\nReview Comment:\n   This is happening by reference. The reference earlier held by `currentSegmentAccumulation` is now saved into `completedSegments[currentIdx]`. And a new reference is created  and saved into `currentSegmentAccumulation`\r\n   \n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470505964\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.HdrHistogram.Histogram;\n+import org.HdrHistogram.Recorder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+public class SlidingWindowHdrHistogram {\n+  private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class);\n+\n+  // Configuration\n+  private final long windowSizeMillis;          // Total analysis window\n+  private final long timeSegmentDurationMillis;       // Subdivision on analysis window\n+  private final int numSegments;\n+  private final long highestTrackableValue;\n+  private final int significantFigures;\n+\n+  // Ring buffer of immutable snapshots for completed time segments\n+  private final Histogram[] completedSegments;\n+  private final AtomicInteger currentIndex = new AtomicInteger(0);\n+\n+  // Active Time Segment\n+  private volatile Recorder activeSegmentRecorder;\n+  private Histogram currentSegmentAccumulation;\n+  private volatile long currentSegmentStartMillis;\n+  private final AtomicLong currentTotalCount = new AtomicLong(0L);\n+\n+  // Synchronization\n+  // Writers never take locks. Readers (queries) and rotation use this lock\n+  // to mutate currentAccumulation and ring-buffer pointers safely.\n+  private final ReentrantLock rotateLock = new ReentrantLock();\n+\n+  // Reusable temp histograms to minimize allocations\n+  private Histogram tmpForDelta;\n+  private Histogram tmpForMerge;\n+\n+  private final AbfsRestOperationType operationType;\n+\n+  private boolean isAnalysisWindowFilled = false;\n+  private int minSampleSize;\n+  private int tailLatencyPercentile;\n+  private int tailLatencyMinDeviation;\n+\n+  private double p50 = 0.0;\n+  private double p90 = 0.0;\n+  private double p99 = 0.0;\n+  private double tailLatency = 0.0;\n+  private int deviation = 0;\n+\n+  public SlidingWindowHdrHistogram(long windowSizeMillis,\n+      int numberOfSegments,\n+      int minSampleSize,\n+      int tailLatencyPercentile,\n+      int tailLatencyMinDeviation,\n+      long highestTrackableValue,\n+      int significantFigures,\n+      final AbfsRestOperationType operationType) {\n+    if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\");\n+    if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\");\n+    if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\");\n+    if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\");\n+\n+    this.windowSizeMillis = windowSizeMillis;\n+    this.numSegments = numberOfSegments;\n+    this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments;\n+    this.highestTrackableValue = highestTrackableValue;\n+    this.significantFigures = significantFigures;\n+    this.operationType = operationType;\n+    this.minSampleSize = minSampleSize;\n+    this.tailLatencyPercentile = tailLatencyPercentile;\n+    this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms\n+\n+    this.completedSegments = new Histogram[numSegments];\n+    long now = System.currentTimeMillis();\n+    this.currentSegmentStartMillis = alignToSegmentDuration(now);\n+    currentIndex.set(0);\n+    this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures);\n+    this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures);\n+    this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures);\n+\n+    LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments);\n+  }\n+\n+  /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */\n+  public void recordValue(long value) {\n+    if (value < 0 || value > highestTrackableValue) {\n+      LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\",\n+          operationType, value, highestTrackableValue);\n+      return;\n+    }\n+    activeSegmentRecorder.recordValue(value);\n+    currentTotalCount.incrementAndGet();\n+    LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\",\n+        operationType, value, currentTotalCount.get());\n+  }\n+\n+  /** Get any percentile over the current sliding window. */\n+  public void computeLatency() {\n+    if (getCurrentTotalCount() < minSampleSize) {\n+      LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\",\n+          operationType, getCurrentTotalCount());\n+    } else {\n+      rotateLock.lock();\n+      try {\n+        tmpForMerge.reset();\n+        for (int i = 0; i < numSegments; i++) {\n+          Histogram h = completedSegments[i];\n+          if (h != null && h.getTotalCount() > 0) {\n+            tmpForMerge.add(h);\n+          }\n+        }\n+\n+        if (tmpForMerge.getTotalCount() == 0) return;\n+\n+        tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile);\n+        p50 = tmpForMerge.getValueAtPercentile(50);\n+        p90 = tmpForMerge.getValueAtPercentile(90);\n+        p99 = tmpForMerge.getValueAtPercentile(99);\n+        deviation = (int) ((tailLatency - p50)/p50 * 100);\n+      } finally {\n+        rotateLock.unlock();\n+      }\n+    }\n+    LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\",\n+        operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount());\n+  }\n+\n+  private long alignToSegmentDuration(long timeMs) {\n+    return timeMs - (timeMs % timeSegmentDurationMillis);\n+  }\n+\n+  /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */\n+  public void rotateIfNeeded() {\n+    LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType);\n+    long now = System.currentTimeMillis();\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470514912\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -114,6 +116,7 @@ public class AbfsRestOperation {\n    */\n   private String failureReason;\n   private AbfsRetryPolicy retryPolicy;\n+  private boolean shouldTailLatencyTimeout = true;\n\nReview Comment:\n   That might be misleading.\r\n   This variable is not a flag for this feature. Even when feature is enabled, we might have this as false.\r\n   \r\n   This is to indicate that all the retried due to TailLatencyTimeout are exhausted and even though the feature is still enabled, for the next retry we should not Timeout due to tail latency\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470519920\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -114,6 +116,7 @@ public class AbfsRestOperation {\n    */\n   private String failureReason;\n   private AbfsRetryPolicy retryPolicy;\n+  private boolean shouldTailLatencyTimeout = true;\n\nReview Comment:\n   Added javadoc\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470527273\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -531,6 +544,12 @@ private boolean executeHttpOperation(final int retryCount,\n       if (shouldUpdateCSTMetrics(statusCode) && !wasKnownExceptionThrown) {\n         intercept.updateMetrics(operationType, httpOperation);\n       }\n+\n+      // Update Tail Latency Tracker only for successful requests.\n+      if (tailLatencyTracker != null && statusCode <  HttpURLConnection.HTTP_MULT_CHOICE) {\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470543064\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestSlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,162 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestSlidingWindowHdrHistogram {\n+\n+  @Test\n+  public void testSlidingWindowHdrHistogram() throws Exception {\n+    SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram(\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestSlidingWindowHdrHistogram.java:\n##########\n@@ -0,0 +1,162 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestSlidingWindowHdrHistogram {\n+\n+  @Test\n+  public void testSlidingWindowHdrHistogram() throws Exception {\n+    SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram(\n+        100,\n+        5,\n+        7,\n+        99,\n+        0,\n+        100,\n+        3,\n+        AbfsRestOperationType.GetPathStatus);\n+\n+    // Verify that the histogram is created successfully with default values and\n+    // do not report any percentiles\n+    assertThat(histogram).isNotNull();\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(0);\n+    assertThat(histogram.getCurrentIndex()).isEqualTo(0);\n+    assertThat(histogram.getP50()).isEqualTo(0.0);\n+    assertThat(histogram.getTailLatency()).isEqualTo(0.0);\n+\n+    // Verify that recording values works as expected\n+    addAndRotate(histogram, 10, 5); // Add 5 values of 10\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(5);\n+\n+    // Verify that percentiles are not computed with insufficient samples\n+    assertThat(histogram.getP50()).isEqualTo(0.0);\n+    assertThat(histogram.getTailLatency()).isEqualTo(0.0);\n+\n+    // Record more values to exceed the minimum sample size\n+    addAndRotate(histogram, 20, 5); // Add 5 values of 20\n+\n+    // Verify that percentiles are now computed but tail Latency is still not reported\n+    assertThat(histogram.getP50()).isGreaterThan(0.0);\n+    assertThat(histogram.getTailLatency()).isEqualTo(0.0);\n+\n+    // Record more values and rotate histogram to fill whole analysis window\n+    addAndRotate(histogram, 30, 5); // Add 5 values of 30\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(15);\n+\n+    // Verify that analysis window is not full until full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isFalse();\n+\n+    addAndRotate(histogram, 60, 5); // Add 5 values of 60\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(20);\n+\n+    // Verify that analysis window is not full until full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isFalse();\n+\n+    // Verify that rotation is skipped if nothing new recorded and hence window not filled\n+    addAndRotate(histogram, 100, 0); // No new values added\n+    assertThat(histogram.isAnalysisWindowFilled()).isFalse();\n+\n+    // Verify that rotation does not happen if analysis window is not filled\n+    histogram.rotateIfNeeded();\n+    assertThat(histogram.isAnalysisWindowFilled()).isFalse();\n+\n+    addAndRotate(histogram, 80, 5); // Add 5 values of 80\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(25);\n+\n+    // Verify that analysis window is full after full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isTrue();\n+\n+    // Verify that percentiles and tail latency are computed\n+    assertThat(histogram.getP50()).isGreaterThan(0.0);\n+    assertThat(histogram.getTailLatency()).isGreaterThan(0.0);\n+\n+    // Verify that sliding window works. Old values should be evicted\n+    addAndRotate(histogram, 90, 3); // Add 3 values of 90\n+    assertThat(histogram.getCurrentTotalCount()).isEqualTo(23);\n+    assertThat(histogram.isAnalysisWindowFilled()).isTrue();\n+  }\n+\n+  @Test\n+  public void testMinDeviationRequirementNotMet() throws Exception {\n+    SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram(\n+        100,\n+        5,\n+        7,\n+        99,\n+        100,\n+        100,\n+        3,\n+        AbfsRestOperationType.GetPathStatus);\n+\n+    // Add values with low deviation\n+    addAndRotate(histogram, 50, 5); // Add 5 values of 50\n+    addAndRotate(histogram, 51, 5); // Add 5 values of 52\n+    addAndRotate(histogram, 52, 5); // Add 5 values of 51\n+    addAndRotate(histogram, 80, 5); // Add 5 values of 53\n+    addAndRotate(histogram, 90, 5); // Add 5 values of 50\n+\n+    // Verify that analysis window is full after full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isTrue();\n+\n+    // Verify that percentiles are not computed due to low deviation\n+    assertThat(histogram.getP50()).isGreaterThan(0.0);\n+    assertThat(histogram.getTailLatency()).isEqualTo(0.0);\n+  }\n+\n+  @Test\n+  public void testMinDeviationRequirementMet() throws Exception {\n+    SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram(\n+        100,\n+        5,\n+        7,\n+        99,\n+        50,\n+        100,\n+        3,\n+        AbfsRestOperationType.GetPathStatus);\n+\n+    // Add values with low deviation\n+    addAndRotate(histogram, 50, 5); // Add 5 values of 50\n+    addAndRotate(histogram, 51, 5); // Add 5 values of 52\n+    addAndRotate(histogram, 52, 5); // Add 5 values of 51\n+    addAndRotate(histogram, 80, 5); // Add 5 values of 53\n+    addAndRotate(histogram, 90, 5); // Add 5 values of 50\n+\n+    // Verify that analysis window is full after full rotation.\n+    assertThat(histogram.isAnalysisWindowFilled()).isTrue();\n+\n+    // Verify that percentiles are not computed due to low deviation\n\nReview Comment:\n   Taken\n\n\n\n", "manika137 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470552760\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int HISTOGRAM_MAX_VALUE = 60_000;\n+  private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3;\n+  private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram>\n+      operationLatencyMap = new HashMap<>();\n+  private final AbfsConfiguration configuration;\n+\n+  /**\n+   * Constructor to initialize the latency tracker with configuration.\n+   * @param abfsConfiguration Configuration settings for latency tracking.\n+   */\n+  public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) {\n+    this.configuration = abfsConfiguration;\n+    ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Histogram-Rotator-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+    long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis()\n\nReview Comment:\n   the division could be by 0 if someone sets window granularity as 0\n\n\n\n", "manika137 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470562303\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -519,6 +519,42 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY)\n   private boolean enableCreateIdempotency;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER,\n+      DefaultValue = DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER)\n+  private boolean isTailLatencyTrackerEnabled;\n+\n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT,\n+      DefaultValue = DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT)\n+  private boolean isTailLatencyRequestTimeoutEnabled;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_PERCENTILE,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE)\n+  private int tailLatencyPercentile;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_MIN_DEVIATION,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MIN_DEVIATION)\n+  private int tailLatencyMinDeviation;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_MIN_SAMPLE_SIZE,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MIN_SAMPLE_SIZE)\n+  private int tailLatencyMinSampleSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_MILLIS,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_MILLIS)\n+  private int tailLatencyAnalysisWindowInMillis;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_GRANULARITY,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_GRANULARITY)\n\nReview Comment:\n   should we have a min, max value for window size above and window granularity?\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470579619\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -266,5 +266,15 @@ public final class FileSystemConfigurations {\n \n   public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true;\n \n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false;\n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT = false;\n+  public static final int DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE = 99;\n\nReview Comment:\n   Nice suggestion. Will take it up.\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470580745\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TailLatencyRequestTimeoutException.java:\n##########\n@@ -0,0 +1,39 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.contracts.exceptions;\n+\n+import java.util.concurrent.TimeoutException;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_TAIL_LATENCY_REQUEST_TIMEOUT;\n+\n+/**\n+ * Thrown when a request takes more time than the current reported tail latency.\n+ */\n+public class TailLatencyRequestTimeoutException extends AzureBlobFileSystemException {\n+\n+  /**\n+   * Constructs a TailLatencyRequestTimeoutException with TimeoutException as the cause.\n+   */\n+  public TailLatencyRequestTimeoutException(TimeoutException innerException) {\n\nReview Comment:\n   Added\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TailLatencyRequestTimeoutException.java:\n##########\n@@ -0,0 +1,39 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.contracts.exceptions;\n+\n+import java.util.concurrent.TimeoutException;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_TAIL_LATENCY_REQUEST_TIMEOUT;\n+\n+/**\n+ * Thrown when a request takes more time than the current reported tail latency.\n+ */\n+public class TailLatencyRequestTimeoutException extends AzureBlobFileSystemException {\n+\n+  /**\n+   * Constructs a TailLatencyRequestTimeoutException with TimeoutException as the cause.\n+   */\n+  public TailLatencyRequestTimeoutException(TimeoutException innerException) {\n+    super(ERR_TAIL_LATENCY_REQUEST_TIMEOUT, innerException);\n+  }\n+\n+  public TailLatencyRequestTimeoutException() {\n\nReview Comment:\n   Added\n\n\n\n", "manika137 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470581573\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int HISTOGRAM_MAX_VALUE = 60_000;\n+  private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3;\n+  private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram>\n+      operationLatencyMap = new HashMap<>();\n+  private final AbfsConfiguration configuration;\n+\n+  /**\n+   * Constructor to initialize the latency tracker with configuration.\n+   * @param abfsConfiguration Configuration settings for latency tracking.\n+   */\n+  public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) {\n+    this.configuration = abfsConfiguration;\n+    ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Histogram-Rotator-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+    long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis()\n+        / configuration.getTailLatencyAnalysisWindowGranularity();\n+    histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms,\n+        rotationInterval, rotationInterval, TimeUnit.MILLISECONDS);\n+\n+\n+    ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Tail-Latency-Computation-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+\n+    long computationalInterval = configuration.getTailLatencyPercentileComputationIntervalInMillis();\n+    tailLatencyComputationThread.scheduleAtFixedRate(this::computePercentiles,\n+        computationalInterval, computationalInterval, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Rotates all histograms to ensure they reflect the most recent latency data.\n+   * This method is called periodically based on the configured rotation interval.\n+   */\n+  private void rotateHistograms() {\n+    for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) {\n+      histogram.rotateIfNeeded();\n+    }\n+  }\n+\n+  /**\n+   * Computes the tail latency percentiles for all operation types.\n+   * This method is called periodically based on the configured computation interval.\n+   */\n+  private void computePercentiles() {\n+    for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) {\n+      histogram.computeLatency();\n+    }\n+  }\n+\n+  /**\n+   * Creates a singleton object of the {@link SlidingWindowHdrHistogram}.\n+   * which is shared across all filesystem instances.\n+   * @param abfsConfiguration configuration set.\n+   * @return singleton object of intercept.\n+   */\n+  static AbfsTailLatencyTracker initializeSingleton(AbfsConfiguration abfsConfiguration) {\n+    if (singleton == null) {\n+      LOCK.lock();\n+      try {\n+        if (singleton == null) {\n+          singleton = new AbfsTailLatencyTracker(abfsConfiguration);\n\nReview Comment:\n   we could log the initialization with the granularity etc configs here\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470582171\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java:\n##########\n@@ -132,6 +138,30 @@ public void close() throws IOException {\n    * @throws IOException network error.\n    */\n   public HttpResponse execute(HttpRequestBase httpRequest,\n+      final AbfsManagedHttpClientContext abfsHttpClientContext,\n+      final int connectTimeout,\n+      final int readTimeout,\n+      final long tailLatencyTimeout) throws IOException {\n+    if (tailLatencyTimeout <= 0) {\n+      return executeWithoutDeadline(httpRequest, abfsHttpClientContext,\n+          connectTimeout, readTimeout);\n+    }\n+    return executeWithDeadline(httpRequest, abfsHttpClientContext,\n+        connectTimeout, readTimeout, tailLatencyTimeout);\n+  }\n+\n+  /**\n+   * Executes the HTTP request.\n+   *\n+   * @param httpRequest HTTP request to execute.\n+   * @param abfsHttpClientContext HttpClient context.\n+   * @param connectTimeout Connection timeout.\n+   * @param readTimeout Read timeout.\n+   *\n+   * @return HTTP response.\n+   * @throws IOException network error.\n+   */\n+  public HttpResponse executeWithoutDeadline(HttpRequestBase httpRequest,\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470583329\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -611,10 +630,30 @@ AbfsJdkHttpOperation createAbfsHttpOperation() throws IOException {\n \n   @VisibleForTesting\n   AbfsAHCHttpOperation createAbfsAHCHttpOperation() throws IOException {\n+    long tailLatency = getTailLatencyTimeoutIfEnabled();\n     return new AbfsAHCHttpOperation(url, method, requestHeaders,\n         Duration.ofMillis(client.getAbfsConfiguration().getHttpConnectionTimeout()),\n         Duration.ofMillis(client.getAbfsConfiguration().getHttpReadTimeout()),\n-        client.getAbfsApacheHttpClient(), client);\n+        tailLatency, client.getAbfsApacheHttpClient(), client);\n+  }\n+\n+  /**\n+   * Get Tail Latency Timeout value if profiling is enabled, timeout is enabled\n+   * and retries due to tail latency request timeout is allowed.\n+   * @return tail latency timeout value else return zero.\n+   */\n+  long getTailLatencyTimeoutIfEnabled() {\n+    if (isTailLatencyTimeoutEnabled() && shouldTailLatencyTimeout) {\n+      return (long) tailLatencyTracker.getTailLatency(this.operationType);\n+    }\n+    return ZERO;\n+  }\n+\n+  boolean isTailLatencyTimeoutEnabled() {\n\nReview Comment:\n   Added\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470586126\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470589733\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int HISTOGRAM_MAX_VALUE = 60_000;\n+  private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3;\n+  private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram>\n+      operationLatencyMap = new HashMap<>();\n+  private final AbfsConfiguration configuration;\n+\n+  /**\n+   * Constructor to initialize the latency tracker with configuration.\n+   * @param abfsConfiguration Configuration settings for latency tracking.\n+   */\n+  public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) {\n+    this.configuration = abfsConfiguration;\n+    ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Histogram-Rotator-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+    long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis()\n+        / configuration.getTailLatencyAnalysisWindowGranularity();\n+    histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms,\n+        rotationInterval, rotationInterval, TimeUnit.MILLISECONDS);\n+\n+\n+    ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor(\n\nReview Comment:\n   Latency tracker are per account basis. They are shared across all filesystem in a single JVM. These are daemon threads and will be killed when JVM gets killed\n\n\n\n", "manika137 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470589786\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -531,6 +544,12 @@ private boolean executeHttpOperation(final int retryCount,\n       if (shouldUpdateCSTMetrics(statusCode) && !wasKnownExceptionThrown) {\n         intercept.updateMetrics(operationType, httpOperation);\n       }\n+\n+      // Update Tail Latency Tracker only for successful requests.\n+      if (tailLatencyTracker != null && statusCode <  HttpURLConnection.HTTP_MULT_CHOICE) {\n+        tailLatencyTracker.updateLatency(operationType,\n\nReview Comment:\n   can 2 threads call updateLatency() for the same operation type simultaneously and create histograms?\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470595674\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int HISTOGRAM_MAX_VALUE = 60_000;\n+  private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3;\n+  private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram>\n+      operationLatencyMap = new HashMap<>();\n+  private final AbfsConfiguration configuration;\n+\n+  /**\n+   * Constructor to initialize the latency tracker with configuration.\n+   * @param abfsConfiguration Configuration settings for latency tracking.\n+   */\n+  public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) {\n+    this.configuration = abfsConfiguration;\n+    ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Histogram-Rotator-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+    long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis()\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470598024\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -519,6 +519,42 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY)\n   private boolean enableCreateIdempotency;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER,\n+      DefaultValue = DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER)\n+  private boolean isTailLatencyTrackerEnabled;\n+\n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT,\n+      DefaultValue = DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT)\n+  private boolean isTailLatencyRequestTimeoutEnabled;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_PERCENTILE,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE)\n+  private int tailLatencyPercentile;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_MIN_DEVIATION,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MIN_DEVIATION)\n+  private int tailLatencyMinDeviation;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_MIN_SAMPLE_SIZE,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MIN_SAMPLE_SIZE)\n+  private int tailLatencyMinSampleSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_MILLIS,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_MILLIS)\n+  private int tailLatencyAnalysisWindowInMillis;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_GRANULARITY,\n+      DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_GRANULARITY)\n\nReview Comment:\n   Added min value for granularity.\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470599110\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int HISTOGRAM_MAX_VALUE = 60_000;\n+  private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3;\n+  private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram>\n+      operationLatencyMap = new HashMap<>();\n+  private final AbfsConfiguration configuration;\n+\n+  /**\n+   * Constructor to initialize the latency tracker with configuration.\n+   * @param abfsConfiguration Configuration settings for latency tracking.\n+   */\n+  public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) {\n+    this.configuration = abfsConfiguration;\n+    ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Histogram-Rotator-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+    long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis()\n+        / configuration.getTailLatencyAnalysisWindowGranularity();\n+    histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms,\n+        rotationInterval, rotationInterval, TimeUnit.MILLISECONDS);\n+\n+\n+    ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Tail-Latency-Computation-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+\n+    long computationalInterval = configuration.getTailLatencyPercentileComputationIntervalInMillis();\n+    tailLatencyComputationThread.scheduleAtFixedRate(this::computePercentiles,\n+        computationalInterval, computationalInterval, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Rotates all histograms to ensure they reflect the most recent latency data.\n+   * This method is called periodically based on the configured rotation interval.\n+   */\n+  private void rotateHistograms() {\n+    for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) {\n+      histogram.rotateIfNeeded();\n+    }\n+  }\n+\n+  /**\n+   * Computes the tail latency percentiles for all operation types.\n+   * This method is called periodically based on the configured computation interval.\n+   */\n+  private void computePercentiles() {\n+    for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) {\n+      histogram.computeLatency();\n+    }\n+  }\n+\n+  /**\n+   * Creates a singleton object of the {@link SlidingWindowHdrHistogram}.\n+   * which is shared across all filesystem instances.\n+   * @param abfsConfiguration configuration set.\n+   * @return singleton object of intercept.\n+   */\n+  static AbfsTailLatencyTracker initializeSingleton(AbfsConfiguration abfsConfiguration) {\n+    if (singleton == null) {\n+      LOCK.lock();\n+      try {\n+        if (singleton == null) {\n+          singleton = new AbfsTailLatencyTracker(abfsConfiguration);\n\nReview Comment:\n   Already in SlidingWindowHdrHistogram class\n\n\n\n", "manika137 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470608205\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int HISTOGRAM_MAX_VALUE = 60_000;\n+  private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3;\n+  private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram>\n+      operationLatencyMap = new HashMap<>();\n+  private final AbfsConfiguration configuration;\n+\n+  /**\n+   * Constructor to initialize the latency tracker with configuration.\n+   * @param abfsConfiguration Configuration settings for latency tracking.\n+   */\n+  public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) {\n+    this.configuration = abfsConfiguration;\n+    ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Histogram-Rotator-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+    long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis()\n+        / configuration.getTailLatencyAnalysisWindowGranularity();\n+    histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms,\n+        rotationInterval, rotationInterval, TimeUnit.MILLISECONDS);\n+\n+\n+    ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Tail-Latency-Computation-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+\n+    long computationalInterval = configuration.getTailLatencyPercentileComputationIntervalInMillis();\n+    tailLatencyComputationThread.scheduleAtFixedRate(this::computePercentiles,\n+        computationalInterval, computationalInterval, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Rotates all histograms to ensure they reflect the most recent latency data.\n+   * This method is called periodically based on the configured rotation interval.\n+   */\n+  private void rotateHistograms() {\n+    for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) {\n+      histogram.rotateIfNeeded();\n+    }\n+  }\n+\n+  /**\n+   * Computes the tail latency percentiles for all operation types.\n+   * This method is called periodically based on the configured computation interval.\n+   */\n+  private void computePercentiles() {\n+    for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) {\n+      histogram.computeLatency();\n+    }\n+  }\n+\n+  /**\n+   * Creates a singleton object of the {@link SlidingWindowHdrHistogram}.\n+   * which is shared across all filesystem instances.\n+   * @param abfsConfiguration configuration set.\n+   * @return singleton object of intercept.\n+   */\n+  static AbfsTailLatencyTracker initializeSingleton(AbfsConfiguration abfsConfiguration) {\n+    if (singleton == null) {\n+      LOCK.lock();\n+      try {\n+        if (singleton == null) {\n+          singleton = new AbfsTailLatencyTracker(abfsConfiguration);\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return singleton;\n+  }\n+\n+  /**\n+   * Updates the latency for a specific operation type.\n+   * @param latency Latency value to be recorded.\n+   * @param operationType Only applicable for read and write operations.\n+   */\n+  public void updateLatency(final AbfsRestOperationType operationType,\n+      final long latency) {\n+    SlidingWindowHdrHistogram histogram = operationLatencyMap.get(operationType);\n+    if (histogram == null) {\n+      LOG.debug(\"Creating new histogram for operation: {}\", operationType);\n+      histogram = new SlidingWindowHdrHistogram(\n+          configuration.getTailLatencyAnalysisWindowInMillis(),\n+          configuration.getTailLatencyAnalysisWindowGranularity(),\n+          configuration.getTailLatencyMinSampleSize(),\n+          configuration.getTailLatencyPercentile(),\n+          configuration.getTailLatencyMinDeviation(),\n+          HISTOGRAM_MAX_VALUE, HISTOGRAM_SIGNIFICANT_FIGURES, operationType);\n+      operationLatencyMap.put(operationType, histogram);\n+    } else {\n+      LOG.debug(\"Using existing histogram for operation: {}\",  operationType);\n+    }\n+    histogram.recordValue(latency);\n+    LOG.debug(\"Updated latency for operation: {} with latency: {}\",\n+        operationType, latency);\n+  }\n+\n+  /**\n+   * Gets the tail latency for a specific operation type.\n+   * @param operationType Only applicable for read and write operations.\n\nReview Comment:\n   why only for read, write operations? \r\n   we are not making the operationType check inside the method\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470616013\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -531,6 +544,12 @@ private boolean executeHttpOperation(final int retryCount,\n       if (shouldUpdateCSTMetrics(statusCode) && !wasKnownExceptionThrown) {\n         intercept.updateMetrics(operationType, httpOperation);\n       }\n+\n+      // Update Tail Latency Tracker only for successful requests.\n+      if (tailLatencyTracker != null && statusCode <  HttpURLConnection.HTTP_MULT_CHOICE) {\n+        tailLatencyTracker.updateLatency(operationType,\n\nReview Comment:\n   Nice catch.\r\n   Added Lock while creation.\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2470619056\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+\n+/**\n+ * Account Specific Latency Tracker.\n+ * This class tracks the latency of various operations like read, write etc for a single account.\n+ * It maintains a sliding window histogram for each operation type to analyze latency patterns over time.\n+ */\n+public class AbfsTailLatencyTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      AbfsTailLatencyTracker.class);\n+  private static AbfsTailLatencyTracker singleton;\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int HISTOGRAM_MAX_VALUE = 60_000;\n+  private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3;\n+  private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram>\n+      operationLatencyMap = new HashMap<>();\n+  private final AbfsConfiguration configuration;\n+\n+  /**\n+   * Constructor to initialize the latency tracker with configuration.\n+   * @param abfsConfiguration Configuration settings for latency tracking.\n+   */\n+  public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) {\n+    this.configuration = abfsConfiguration;\n+    ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Histogram-Rotator-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+    long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis()\n+        / configuration.getTailLatencyAnalysisWindowGranularity();\n+    histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms,\n+        rotationInterval, rotationInterval, TimeUnit.MILLISECONDS);\n+\n+\n+    ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor(\n+        r -> {\n+          Thread t = new Thread(r, \"Tail-Latency-Computation-Thread\");\n+          t.setDaemon(true);\n+          return t;\n+        });\n+\n+    long computationalInterval = configuration.getTailLatencyPercentileComputationIntervalInMillis();\n+    tailLatencyComputationThread.scheduleAtFixedRate(this::computePercentiles,\n+        computationalInterval, computationalInterval, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Rotates all histograms to ensure they reflect the most recent latency data.\n+   * This method is called periodically based on the configured rotation interval.\n+   */\n+  private void rotateHistograms() {\n+    for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) {\n+      histogram.rotateIfNeeded();\n+    }\n+  }\n+\n+  /**\n+   * Computes the tail latency percentiles for all operation types.\n+   * This method is called periodically based on the configured computation interval.\n+   */\n+  private void computePercentiles() {\n+    for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) {\n+      histogram.computeLatency();\n+    }\n+  }\n+\n+  /**\n+   * Creates a singleton object of the {@link SlidingWindowHdrHistogram}.\n+   * which is shared across all filesystem instances.\n+   * @param abfsConfiguration configuration set.\n+   * @return singleton object of intercept.\n+   */\n+  static AbfsTailLatencyTracker initializeSingleton(AbfsConfiguration abfsConfiguration) {\n+    if (singleton == null) {\n+      LOCK.lock();\n+      try {\n+        if (singleton == null) {\n+          singleton = new AbfsTailLatencyTracker(abfsConfiguration);\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return singleton;\n+  }\n+\n+  /**\n+   * Updates the latency for a specific operation type.\n+   * @param latency Latency value to be recorded.\n+   * @param operationType Only applicable for read and write operations.\n+   */\n+  public void updateLatency(final AbfsRestOperationType operationType,\n+      final long latency) {\n+    SlidingWindowHdrHistogram histogram = operationLatencyMap.get(operationType);\n+    if (histogram == null) {\n+      LOG.debug(\"Creating new histogram for operation: {}\", operationType);\n+      histogram = new SlidingWindowHdrHistogram(\n+          configuration.getTailLatencyAnalysisWindowInMillis(),\n+          configuration.getTailLatencyAnalysisWindowGranularity(),\n+          configuration.getTailLatencyMinSampleSize(),\n+          configuration.getTailLatencyPercentile(),\n+          configuration.getTailLatencyMinDeviation(),\n+          HISTOGRAM_MAX_VALUE, HISTOGRAM_SIGNIFICANT_FIGURES, operationType);\n+      operationLatencyMap.put(operationType, histogram);\n+    } else {\n+      LOG.debug(\"Using existing histogram for operation: {}\",  operationType);\n+    }\n+    histogram.recordValue(latency);\n+    LOG.debug(\"Updated latency for operation: {} with latency: {}\",\n+        operationType, latency);\n+  }\n+\n+  /**\n+   * Gets the tail latency for a specific operation type.\n+   * @param operationType Only applicable for read and write operations.\n\nReview Comment:\n   Updated\n\n\n\n", "hadoop-yetus commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3458139061\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 44 new + 3 unchanged - 0 fixed = 47 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 34 new + 1472 unchanged - 0 fixed = 1506 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 33 new + 1413 unchanged - 0 fixed = 1446 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 44s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 5 new + 177 unchanged - 1 fixed = 182 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  60m 40s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:[line 539] |\r\n   |  |  new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:[line 55] |\r\n   |  |  Possible null pointer dereference of histogram in org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker.updateLatency(AbfsRestOperationType, long)  Dereferenced at AbfsTailLatencyTracker.java:histogram in org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker.updateLatency(AbfsRestOperationType, long)  Dereferenced at AbfsTailLatencyTracker.java:[line 149] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:[line 81] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8043 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux c4d12ec6b6d0 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3ca8f94cbd461d8904d9c655d446c2927dac0cd5 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/testReport/ |\r\n   | Max. process+thread count | 611 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "bhattmanish98 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2477231657\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -359,7 +369,7 @@ void completeExecute(TracingContext tracingContext)\n   @VisibleForTesting\n   void updateBackoffMetrics(int retryCount, int statusCode) {\n     if (abfsBackoffMetrics != null) {\n-      if (statusCode < HttpURLConnection.HTTP_OK\n\nReview Comment:\n   This change can we reverted.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -453,7 +463,7 @@ private boolean executeHttpOperation(final int retryCount,\n       }\n         incrementCounter(AbfsStatistic.GET_RESPONSES, 1);\n       //Only increment bytesReceived counter when the status code is 2XX.\n-      if (httpOperation.getStatusCode() >= HttpURLConnection.HTTP_OK\n\nReview Comment:\n   same as above\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,75 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_NETWORKING_LIBRARY;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class ITestAbfsTailLatencyTracker extends AbstractAbfsIntegrationTest {\n+\n+  protected ITestAbfsTailLatencyTracker() throws Exception {\n+  }\n+\n+  @Test\n+  public void testTailLatencyTimeoutEnabled() throws Exception {\n\nReview Comment:\n   Java doc missing. Please add it to all newly added test cases\n\n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2480181549\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -266,5 +266,15 @@ public final class FileSystemConfigurations {\n \n   public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true;\n \n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false;\n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT = false;\n+  public static final int DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE = 99;\n\nReview Comment:\n   I am not finding an easy way to make this change. Will add a work item for this improvement and take it up in follow up items.\n\n\n\n", "hadoop-yetus commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3471759836\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 44s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 177 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 58s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 25 new + 3 unchanged - 0 fixed = 28 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 32 new + 1518 unchanged - 0 fixed = 1550 total (was 1518)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 31 new + 1412 unchanged - 0 fixed = 1443 total (was 1412)  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 176 unchanged - 1 fixed = 180 total (was 177)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  59m 35s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:[line 539] |\r\n   |  |  new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:[line 55] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:[line 95] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8043 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 4e580718cb8c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a57f2afea1b3be581f31a6595e498ec2b2a42e3a |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/testReport/ |\r\n   | Max. process+thread count | 633 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2480654128\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -453,7 +463,7 @@ private boolean executeHttpOperation(final int retryCount,\n       }\n         incrementCounter(AbfsStatistic.GET_RESPONSES, 1);\n       //Only increment bytesReceived counter when the status code is 2XX.\n-      if (httpOperation.getStatusCode() >= HttpURLConnection.HTTP_OK\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -359,7 +369,7 @@ void completeExecute(TracingContext tracingContext)\n   @VisibleForTesting\n   void updateBackoffMetrics(int retryCount, int statusCode) {\n     if (abfsBackoffMetrics != null) {\n-      if (statusCode < HttpURLConnection.HTTP_OK\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsTailLatencyTracker.java:\n##########\n@@ -0,0 +1,75 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_NETWORKING_LIBRARY;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class ITestAbfsTailLatencyTracker extends AbstractAbfsIntegrationTest {\n+\n+  protected ITestAbfsTailLatencyTracker() throws Exception {\n+  }\n+\n+  @Test\n+  public void testTailLatencyTimeoutEnabled() throws Exception {\n\nReview Comment:\n   Taken\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2480943824\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1949,7 +1950,7 @@ public boolean isTailLatencyTrackerEnabled() {\n   }\n \n   public boolean isTailLatencyRequestTimeoutEnabled() {\n-    return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled\n+    return isTailLatencyTrackerEnabled && isTailLatencyRequestTimeoutEnabled\n\nReview Comment:\n   javadocs for warnings can be added\n\n\n\n", "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2481457271\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java:\n##########\n@@ -131,11 +139,11 @@ public void updateLatency(final AbfsRestOperationType operationType,\n         if (operationLatencyMap.get(operationType) == null) {\n           LOG.debug(\"Creating new histogram for operation: {}\", operationType);\n           histogram = new SlidingWindowHdrHistogram(\n-              configuration.getTailLatencyAnalysisWindowInMillis(),\n-              configuration.getTailLatencyAnalysisWindowGranularity(),\n-              configuration.getTailLatencyMinSampleSize(),\n-              configuration.getTailLatencyPercentile(),\n-              configuration.getTailLatencyMinDeviation(),\n+              talLatencyAnalysisWindowInMillis,\n\nReview Comment:\n   nit: spelling of tail\n\n\n\n", "hadoop-yetus commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3473362007\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 177 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 58s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 21 new + 1517 unchanged - 1 fixed = 1538 total (was 1518)  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 19 new + 1412 unchanged - 0 fixed = 1431 total (was 1412)  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 176 unchanged - 1 fixed = 178 total (was 177)  |\r\n   | +1 :green_heart: |  shadedclient  |  15m 45s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  63m  1s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:[line 539] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8043 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 7c5c2cb63961 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 56129accd046c6ae237402d7f64894da6ba44046 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/testReport/ |\r\n   | Max. process+thread count | 639 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3473551869\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  33m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 38s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 25s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 177 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 31s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 21 new + 1517 unchanged - 1 fixed = 1538 total (was 1518)  |\r\n   | -1 :x: |  javadoc  |   0m 28s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 19 new + 1412 unchanged - 0 fixed = 1431 total (was 1412)  |\r\n   | -1 :x: |  spotbugs  |   1m 23s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 176 unchanged - 1 fixed = 178 total (was 177)  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  4s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 105m 11s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:[line 539] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8043 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 86ea63abc892 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 56129accd046c6ae237402d7f64894da6ba44046 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/testReport/ |\r\n   | Max. process+thread count | 634 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3478667095\n\n   ------------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 875, Failures: 0, Errors: 0, Skipped: 214\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 878, Failures: 0, Errors: 0, Skipped: 166\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 717, Failures: 0, Errors: 0, Skipped: 279\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 875, Failures: 0, Errors: 0, Skipped: 225\r\n   [WARNING] Tests run: 126, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 724, Failures: 0, Errors: 0, Skipped: 137\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 281\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 721, Failures: 0, Errors: 0, Skipped: 149\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 716, Failures: 0, Errors: 0, Skipped: 195\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 749, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 217, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 278\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   Time taken: 275 mins 19 secs.\r\n   \n\n\n", "anujmodi2021 merged PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043\n\n\n"], "labels": ["pull-request-available"], "summary": "It has been observed that certain requests taking more time than expected to com", "qna": [{"question": "What is the issue title?", "answer": "ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively"}, {"question": "Who reported this issue?", "answer": "Anuj Modi"}]}
{"key": "HADOOP-19728", "project": "HADOOP", "title": "S3A: add ipv6 support", "status": "Open", "reporter": "Steve Loughran", "created": "2025-10-15T13:16:53.000+0000", "description": "Support IPv6 with a flag to enable/disable dual stack endpoints\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/ipv6-access.html", "comments": [], "labels": [], "summary": "Support IPv6 with a flag to enable/disable dual stack endpoints\r\n\r\nhttps://docs", "qna": [{"question": "What is the issue title?", "answer": "S3A: add ipv6 support"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19727", "project": "HADOOP", "title": "Release hadoop-thirdparty 1.5.0", "status": "Resolved", "reporter": "Steve Loughran", "created": "2025-10-14T14:34:32.000+0000", "description": "\r\nRelease hadoop-thirdparty 1.5.0", "comments": [], "labels": [], "summary": "\r\nRelease hadoop-thirdparty 1", "qna": [{"question": "What is the issue title?", "answer": "Release hadoop-thirdparty 1.5.0"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19726", "project": "HADOOP", "title": "Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module", "status": "Resolved", "reporter": "Shilun Fan", "created": "2025-10-12T23:47:15.000+0000", "description": "Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution:\r\n{code:java}\r\njava.lang.IllegalStateException: Failed to set environment variable\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120)\tat org.apache.hadoop.fs.tosfs.object.ObjectStorageTestBase.setUp(ObjectStorageTestBase.java:59)\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not \"opens java.util\" to unnamed module @69eee410\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)\tat java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)\tat java.base/java.lang.reflect.Field.setAccessible(Field.java:172)\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:116)\t... 4 more\r\n {code}\r\nThis error occurs due to the module system restrictions in JDK 17, where reflection cannot access private fields in the java.util.Collections$UnmodifiableMap class.\r\n\r\n\u00a0\r\n\r\nTo resolve this issue, JDK 17 compile options have been added to ensure the maven-surefire-plugin works correctly in a JDK 17 environment. This PR adds the necessary compile options for maven-surefire-plugin to support JDK 17, fixing the error and ensuring that unit tests can run smoothly.", "comments": ["slfan1989 opened a new pull request, #8029:\nURL: https://github.com/apache/hadoop/pull/8029\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19726. [JDK17] Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   CI\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029#issuecomment-3395542255\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 39s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 23s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  javadoc  |   0m 12s | [/patch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/patch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tos in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  shadedclient  |   1m 41s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 13s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  55m  3s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8029 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux dd7bc96b56b5 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f188198cdec1613ae955ea31795a8cb1ca496139 |\r\n   | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/testReport/ |\r\n   | Max. process+thread count | 576 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029#issuecomment-3411382684\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 55s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 26s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/artifact/out/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  58m 21s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream |\r\n   |   | hadoop.fs.tosfs.object.TestObjectRangeInputStream |\r\n   |   | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider |\r\n   |   | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain |\r\n   |   | hadoop.fs.tosfs.object.TestObjectOutputStream |\r\n   |   | hadoop.fs.tosfs.commit.TestMagicOutputStream |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8029 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 3f25ded462da 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2bd00ae481cc8a6aeb977fef2700e684236375a4 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/testReport/ |\r\n   | Max. process+thread count | 616 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029#issuecomment-3411690102\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 35s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html) |  hadoop-cloud-storage-project/hadoop-tos in trunk has 56 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 27s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/artifact/out/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  60m 57s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream |\r\n   |   | hadoop.fs.tosfs.object.TestObjectRangeInputStream |\r\n   |   | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider |\r\n   |   | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain |\r\n   |   | hadoop.fs.tosfs.object.TestObjectOutputStream |\r\n   |   | hadoop.fs.tosfs.commit.TestMagicOutputStream |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8029 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 9e3548929018 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9ba2bc5bf9cda429475a820bfcf689479e7fdc2d |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/testReport/ |\r\n   | Max. process+thread count | 639 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029#issuecomment-3412145538\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 44s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 34s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/4/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html) |  hadoop-cloud-storage-project/hadoop-tos in trunk has 56 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  3s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 58s |  |  hadoop-tos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  60m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8029 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 6a680a7543dc 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1b29c9884dbd1895ea8116fe1c0cf495ce03d39f |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/4/testReport/ |\r\n   | Max. process+thread count | 643 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/4/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029#issuecomment-3413251437\n\n   @wojiaodoubao Could you please help review this PR again? Thanks a lot!\n\n\n", "slfan1989 commented on PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029#issuecomment-3418122640\n\n   I plan to merge this PR, as the unit test errors in TOS have been resolved. If further optimization is needed later, we can submit a separate PR for improvements.\r\n   \r\n   cc: @steveloughran @wojiaodoubao \n\n\n", "slfan1989 merged PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029\n\n\n"], "labels": ["pull-request-available"], "summary": "Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options co", "qna": [{"question": "What is the issue title?", "answer": "Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module"}, {"question": "Who reported this issue?", "answer": "Shilun Fan"}]}
{"key": "HADOOP-19725", "project": "HADOOP", "title": "Upgrade SpotBugs Version to Support JDK 17 Compilation", "status": "Resolved", "reporter": "Shilun Fan", "created": "2025-10-12T23:27:49.000+0000", "description": "The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17.", "comments": ["slfan1989 opened a new pull request, #8028:\nURL: https://github.com/apache/hadoop/pull/8028\n\n   ### Description of PR\r\n   \r\n   JIRA: [JDK17] Upgrade SpotBugs Version to Support JDK 17 Compilation.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3397979702\n\n   @cnauroth @szetszwo After upgrading to JDK 17, I found that spotbug could not run properly because the current version does not support JDK 17. To resolve this issue, I upgraded the versions of the two related plugins. The changes have been tested locally, and the results are as expected.\n\n\n", "hadoop-yetus commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3398540465\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 58s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  mvnsite  |   9m 55s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 51s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  | 124m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  28m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 17s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 43s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   7m  2s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m  7s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 45s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  53m 32s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 853m 28s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 44s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1109m 31s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8028 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux da08cd8994df 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3b47bd160a2122ed84df1730e54cbfafa3ab60b8 |\r\n   | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/testReport/ |\r\n   | Max. process+thread count | 3529 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3399438331\n\n   I have completed the investigation of the mvnsite build issues and confirmed that the problem is related to the JDIFF module. We plan to submit a separate PR to fix and optimize this issue. \n\n\n", "slfan1989 commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3401292116\n\n   @steveloughran Could you please review this PR? Thank you very much!\n\n\n", "slfan1989 commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3405489643\n\n   @Hexiaoqiao Could you please review this PR? Thank you very much!\n\n\n", "szetszwo commented on code in PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#discussion_r2433007716\n\n\n##########\npom.xml:\n##########\n@@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x\n     <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version>\n     <checkstyle.version>8.29</checkstyle.version>\n     <dependency-check-maven.version>7.1.1</dependency-check-maven.version>\n-    <spotbugs.version>4.2.2</spotbugs.version>\n-    <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version>\n+    <spotbugs.version>4.8.3</spotbugs.version>\n+    <spotbugs-maven-plugin.version>4.7.3.6</spotbugs-maven-plugin.version>\n\nReview Comment:\n   Similarly, why not 4.9.7.0 (or 4.8.6.7)?\n\n\n\n##########\npom.xml:\n##########\n@@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x\n     <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version>\n     <checkstyle.version>8.29</checkstyle.version>\n     <dependency-check-maven.version>7.1.1</dependency-check-maven.version>\n-    <spotbugs.version>4.2.2</spotbugs.version>\n-    <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version>\n+    <spotbugs.version>4.8.3</spotbugs.version>\n\nReview Comment:\n   Why not 4.9.7 (or 4.8.6)?\n   \n   https://mvnrepository.com/artifact/com.github.spotbugs/spotbugs\n\n\n\n", "slfan1989 commented on code in PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#discussion_r2433103901\n\n\n##########\npom.xml:\n##########\n@@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x\n     <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version>\n     <checkstyle.version>8.29</checkstyle.version>\n     <dependency-check-maven.version>7.1.1</dependency-check-maven.version>\n-    <spotbugs.version>4.2.2</spotbugs.version>\n-    <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version>\n+    <spotbugs.version>4.8.3</spotbugs.version>\n\nReview Comment:\n   Thank you for reviewing! You made a very good point \u2014 I\u2019ll update this PR accordingly.\n\n\n\n", "hadoop-yetus commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3409917424\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  37m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 40s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  mvnsite  |  10m 13s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 11s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 47s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  | 125m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  28m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 11s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 58s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   7m  5s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m  8s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 40s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  53m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 792m 24s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 46s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1035m 29s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8028 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux c50dc2503f2d 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c7ac33f5f774a890079fd002e27829cc7b38c67d |\r\n   | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/testReport/ |\r\n   | Max. process+thread count | 3553 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3411057566\n\n   > Thanks @slfan1989 . LGTM. +1. I think it is smooth after check spotbug release changes and other apache projects upgrade feedbacks. TBH, I am not check it with new JDK version carefully.\r\n   \r\n   @Hexiaoqiao Many thanks for reviewing the code! The new Sputbug plugin has been tested on JDK 17 and JDK 21 and works properly.\n\n\n", "slfan1989 merged PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028\n\n\n", "slfan1989 commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3411067429\n\n   @szetszwo @steveloughran @Hexiaoqiao @zhtttylz Thank you very much for reviewing the code!\n\n\n", "pan3793 commented on code in PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#discussion_r2490333425\n\n\n##########\npom.xml:\n##########\n@@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x\n     <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version>\n     <checkstyle.version>8.29</checkstyle.version>\n     <dependency-check-maven.version>7.1.1</dependency-check-maven.version>\n-    <spotbugs.version>4.2.2</spotbugs.version>\n-    <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version>\n+    <spotbugs.version>4.8.3</spotbugs.version>\n\nReview Comment:\n   spotbugs 4.9 onwards requires JDK 11+, are we ready to drop Java 8 support on trunk?\r\n   \r\n   currently, trunk GitHub Actions site jobs fail due to this upgrade.\r\n   \r\n   @szetszwo @slfan1989 \n\n\n\n", "szetszwo commented on code in PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#discussion_r2491446323\n\n\n##########\npom.xml:\n##########\n@@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x\n     <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version>\n     <checkstyle.version>8.29</checkstyle.version>\n     <dependency-check-maven.version>7.1.1</dependency-check-maven.version>\n-    <spotbugs.version>4.2.2</spotbugs.version>\n-    <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version>\n+    <spotbugs.version>4.8.3</spotbugs.version>\n\nReview Comment:\n   @slfan1989 , we probably should move back to 4.8.x?\n\n\n\n", "szetszwo commented on code in PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#discussion_r2491446323\n\n\n##########\npom.xml:\n##########\n@@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x\n     <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version>\n     <checkstyle.version>8.29</checkstyle.version>\n     <dependency-check-maven.version>7.1.1</dependency-check-maven.version>\n-    <spotbugs.version>4.2.2</spotbugs.version>\n-    <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version>\n+    <spotbugs.version>4.8.3</spotbugs.version>\n\nReview Comment:\n   ~@slfan1989 , we probably should move back to 4.8.x?~\r\n   \r\n   @pan3793 , Actually, SpotBugs is build tool but not a runtime library.  One option is to run SpotBugs code analysis using jdk11 or above.  The runtime still can be built with and run with JDK8.  Would it work?\n\n\n\n", "slfan1989 commented on code in PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#discussion_r2492132718\n\n\n##########\npom.xml:\n##########\n@@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x\n     <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version>\n     <checkstyle.version>8.29</checkstyle.version>\n     <dependency-check-maven.version>7.1.1</dependency-check-maven.version>\n-    <spotbugs.version>4.2.2</spotbugs.version>\n-    <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version>\n+    <spotbugs.version>4.8.3</spotbugs.version>\n\nReview Comment:\n   Thank you all for the discussion! I believe what @szetszwo mentioned makes sense. \r\n   \r\n   \r\n   Let me explain the situation:\r\n   \r\n   We have upgraded the JDK version in the `trunk` to `JDK 17` and `JDK 21`. Regarding the issue of `mvn site` failing to compile, this is a known issue and is unrelated to the `SpotBugs` plugin.\r\n   \r\n   Here\u2019s a detailed explanation of the two issues:\r\n   \r\n   1. The reason for upgrading is that `SpotBugs 4.2.0`, as indicated by `Yetus`, does not support scanning for JDK 17 and above. Therefore, we upgraded to a higher version, choosing `SpotBugs 4.9.7.` From my perspective, upgrading to `SpotBugs 4.9.7` is reasonable. Although new warning messages have appeared, I do not plan to roll back the version.\r\n   \r\n   Issue after upgrading to `SpotBugs 4.9.7`: Due to the introduction of new rules, new warning messages appeared during compilation. We have formulated a solution:\r\n   \r\n   - Temporary solution: We are filtering out these new warnings globally to mitigate the impact of the new filtering rules. You can refer to the related PR #8053.\r\n   \r\n   2. mvn site compilation failure: This issue is related to our custom annotations, which we can see in the error logs. \r\n   \r\n   We already have a solution for this:\r\n   \r\n   Referring to PR #8038. we have rewritten all the annotations under `hadoop-common-project/hadoop-annotations/src/main/java17/org/apache/hadoop/classification/tools/*` and added support for JDIFF under JDK 17.\r\n   \r\n   Currently, these two PRs are being followed up by HuaLong. In offline communication, HuaLong mentioned that he is currently on leave, so it may take some more time to complete.\r\n   \r\n   I hope this makes the situation clearer!\n\n\n\n", "slfan1989 commented on code in PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#discussion_r2492132718\n\n\n##########\npom.xml:\n##########\n@@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x\n     <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version>\n     <checkstyle.version>8.29</checkstyle.version>\n     <dependency-check-maven.version>7.1.1</dependency-check-maven.version>\n-    <spotbugs.version>4.2.2</spotbugs.version>\n-    <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version>\n+    <spotbugs.version>4.8.3</spotbugs.version>\n\nReview Comment:\n   Thank you all for the discussion! I believe what @szetszwo mentioned makes sense. \r\n   \r\n   \r\n   Let me explain the situation:\r\n   \r\n   We have upgraded the JDK version in the `trunk` to `JDK 17` and `JDK 21`. Regarding the issue of `mvn site` failing to compile, this is a known issue and is unrelated to the `SpotBugs` plugin.\r\n   \r\n   Here\u2019s a detailed explanation of the two issues:\r\n   \r\n   1. The reason for upgrading is that `SpotBugs 4.2.0`, as indicated by `Yetus`, does not support scanning for JDK 17 and above. Therefore, we upgraded to a higher version, choosing `SpotBugs 4.9.7.` From my perspective, upgrading to `SpotBugs 4.9.7` is reasonable. Although new warning messages have appeared, I do not plan to roll back the version.  Issue after upgrading to `SpotBugs 4.9.7`: Due to the introduction of new rules, new warning messages appeared during compilation. We have formulated a solution:\r\n   \r\n   - Temporary solution: We are filtering out these new warnings globally to mitigate the impact of the new filtering rules. You can refer to the related PR #8053.\r\n   \r\n   2. mvn site compilation failure: This issue is related to our custom annotations, which we can see in the error logs. We already have a solution for this:\r\n   \r\n   - Referring to PR #8038. we have rewritten all the annotations under `hadoop-common-project/hadoop-annotations/src/main/java17/org/apache/hadoop/classification/tools/*` and added support for JDIFF under JDK 17.\r\n   \r\n   Currently, these two PRs are being followed up by HuaLong. In offline communication, HuaLong mentioned that he is currently on leave, so it may take some more time to complete.\r\n   \r\n   I hope this makes the situation clearer!\n\n\n\n"], "labels": ["pull-request-available"], "summary": "The current SpotBugs version used in the project is 4", "qna": [{"question": "What is the issue title?", "answer": "Upgrade SpotBugs Version to Support JDK 17 Compilation"}, {"question": "Who reported this issue?", "answer": "Shilun Fan"}]}
{"key": "HADOOP-19724", "project": "HADOOP", "title": "[RISC-V]  Add rv bulk CRC32 (non-CRC32C) optimized path", "status": "Open", "reporter": "Ptroc", "created": "2025-10-12T12:49:32.000+0000", "description": null, "comments": ["PeterPtroc opened a new pull request, #8031:\nURL: https://github.com/apache/hadoop/pull/8031\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   - Introduces a riscv64 native implementation path for CRC32 (CRC32C not optimized).\r\n   - Adds runtime CPU feature detection on linux-riscv64 to enable hardware-accelerated CRC32 when available; falls back to the existing implementation if native is unavailable or disabled.\r\n   \r\n   Below are the performance changes observed using the built-in CRC32 benchmark. Although performance is poor when bpc <= 64, there are substantial improvements when bpc > 64. To keep the codebase simple and maintainable, I did not add bpc-size-specific handling.\r\n   \r\n   | bpc | #T | Native (origin) | Native (new) | \u0394 (MB/s) | \u0394% |\r\n   |---:|---:|---:|---:|---:|---:|\r\n   | 32 | 1 | 661.5 | 463.5 | -198.0 | -29.9% |\r\n   | 32 | 2 | 642.6 | 491.4 | -151.2 | -23.5% |\r\n   | 32 | 4 | 663.7 | 480.5 | -183.2 | -27.6% |\r\n   | 32 | 8 | 653.0 | 472.0 | -181.0 | -27.7% |\r\n   | 32 | 16 | 656.1 | 473.4 | -182.7 | -27.8% |\r\n   | 64 | 1 | 793.9 | 318.0 | -475.9 | -59.9% |\r\n   | 64 | 2 | 771.3 | 322.1 | -449.2 | -58.2% |\r\n   | 64 | 4 | 787.3 | 315.0 | -472.3 | -60.0% |\r\n   | 64 | 8 | 778.0 | 309.3 | -468.7 | -60.2% |\r\n   | 64 | 16 | 773.5 | 308.1 | -465.4 | -60.2% |\r\n   | 128 | 1 | 878.8 | 2398.8 | +1520.0 | +173.0% |\r\n   | 128 | 2 | 846.8 | 1723.9 | +877.1 | +103.6% |\r\n   | 128 | 4 | 861.2 | 1690.0 | +828.8 | +96.2% |\r\n   | 128 | 8 | 857.8 | 1373.3 | +515.5 | +60.1% |\r\n   | 128 | 16 | 853.8 | 1361.3 | +507.5 | +59.4% |\r\n   | 256 | 1 | 783.9 | 2752.5 | +1968.6 | +251.1% |\r\n   | 256 | 2 | 810.0 | 2053.3 | +1243.3 | +153.5% |\r\n   | 256 | 4 | 835.2 | 1966.5 | +1131.3 | +135.5% |\r\n   | 256 | 8 | 812.4 | 1756.3 | +943.9 | +116.2% |\r\n   | 256 | 16 | 811.8 | 1524.7 | +712.9 | +87.8% |\r\n   | 512 | 1 | 923.6 | 3328.9 | +2405.3 | +260.4% |\r\n   | 512 | 2 | 886.5 | 3295.1 | +2408.6 | +271.7% |\r\n   | 512 | 4 | 910.5 | 2359.9 | +1449.4 | +159.2% |\r\n   | 512 | 8 | 888.1 | 1637.4 | +749.3 | +84.4% |\r\n   | 512 | 16 | 897.0 | 1840.1 | +943.1 | +105.1% |\r\n   | 1024 | 1 | 950.4 | 3045.0 | +2094.6 | +220.4% |\r\n   | 1024 | 2 | 918.0 | 2202.9 | +1284.9 | +140.0% |\r\n   | 1024 | 4 | 937.6 | 2040.4 | +1102.8 | +117.6% |\r\n   | 1024 | 8 | 916.5 | 1961.5 | +1045.0 | +114.0% |\r\n   | 1024 | 16 | 927.4 | 2003.9 | +1076.5 | +116.1% |\r\n   | 2048 | 1 | 962.3 | 3189.1 | +2226.8 | +231.4% |\r\n   | 2048 | 2 | 970.1 | 3192.3 | +2222.2 | +229.1% |\r\n   | 2048 | 4 | 943.4 | 2411.2 | +1467.8 | +155.6% |\r\n   | 2048 | 8 | 937.6 | 1837.7 | +900.1 | +96.0% |\r\n   | 2048 | 16 | 933.1 | 1864.0 | +930.9 | +99.8% |\r\n   | 4096 | 1 | 969.9 | 3654.5 | +2684.6 | +276.8% |\r\n   | 4096 | 2 | 972.0 | 2798.0 | +1826.0 | +187.9% |\r\n   | 4096 | 4 | 960.1 | 2307.0 | +1346.9 | +140.3% |\r\n   | 4096 | 8 | 948.2 | 2753.1 | +1804.9 | +190.4% |\r\n   | 4096 | 16 | 938.7 | 2170.5 | +1231.8 | +131.2% |\r\n   | 8192 | 1 | 973.6 | 4008.1 | +3034.5 | +311.7% |\r\n   | 8192 | 2 | 922.5 | 3018.2 | +2095.7 | +227.2% |\r\n   | 8192 | 4 | 955.6 | 2968.7 | +2013.1 | +210.7% |\r\n   | 8192 | 8 | 943.4 | 2077.9 | +1134.5 | +120.3% |\r\n   | 8192 | 16 | 944.9 | 2191.7 | +1246.8 | +132.0% |\r\n   | 16384 | 1 | 974.4 | 4090.3 | +3115.9 | +319.8% |\r\n   | 16384 | 2 | 978.3 | 2999.6 | +2021.3 | +206.6% |\r\n   | 16384 | 4 | 956.6 | 3248.9 | +2292.3 | +239.6% |\r\n   | 16384 | 8 | 950.8 | 3228.0 | +2277.2 | +239.5% |\r\n   | 16384 | 16 | 941.2 | 2832.1 | +1890.9 | +200.9% |\r\n   | 32768 | 1 | 972.2 | 4205.7 | +3233.5 | +332.6% |\r\n   | 32768 | 2 | 938.6 | 4115.2 | +3176.6 | +338.4% |\r\n   | 32768 | 4 | 957.4 | 2508.9 | +1551.5 | +162.1% |\r\n   | 32768 | 8 | 952.8 | 2319.8 | +1367.0 | +143.5% |\r\n   | 32768 | 16 | 944.5 | 1657.7 | +713.2 | +75.5% |\r\n   | 65536 | 1 | 976.3 | 4226.6 | +3250.3 | +332.9% |\r\n   | 65536 | 2 | 940.0 | 3075.8 | +2135.8 | +227.2% |\r\n   | 65536 | 4 | 958.5 | 1345.2 | +386.7 | +40.3% |\r\n   | 65536 | 8 | 950.2 | 1954.7 | +1004.5 | +105.7% |\r\n   | 65536 | 16 | 945.8 | 2414.0 | +1468.2 | +155.2% |\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Built hadoop-common with native profile on riscv64; verified it's function by TestNativeCrc32.\r\n   Ran Hadoop\u2019s CRC32 benchmark on riscv64 (OpenEuler/EulixOS) with JDK 17.\r\n   Here is the commands and results:\r\n   \r\n   Command\uff1a \r\n   \r\n   ```\r\n   mvn -Pnative \\\r\n     -Dtest=org.apache.hadoop.util.TestNativeCrc32 \\\r\n     -Djava.library.path=\"$HADOOP_COMMON_LIB_NATIVE_DIR\" \\\r\n     test\r\n   ```\r\n   \r\n   Results\r\n   \r\n   ```\r\n   [INFO] -------------------------------------------------------\r\n   [INFO]  T E S T S\r\n   [INFO] -------------------------------------------------------\r\n   [INFO] Running org.apache.hadoop.util.TestNativeCrc32\r\n   [INFO] Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.017 s ", "hadoop-yetus commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3398778305\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  14m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  | 101m 41s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 25s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 206m 55s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8031 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux cf2b3cead534 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0e62b049f710f680823489b1a77892ed49252fc4 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_462-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/testReport/ |\r\n   | Max. process+thread count | 1376 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "PeterPtroc commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3401469367\n\n   @steveloughran could you please review this PR if you have time? thanks!\n\n\n", "PeterPtroc commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3478668222\n\n   Hi @pan3793 @slfan1989 , could you please take a look when you have a moment? Happy to address any feedback. Thanks!\n\n\n", "steveloughran commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3493588407\n\n   Is everyone with a risc-v setup able to test this?\n\n\n", "slfan1989 commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3509541896\n\n   > Hi @pan3793 @slfan1989 , could you please take a look when you have a moment? Happy to address any feedback. Thanks!\r\n   \r\n   @PeterPtroc Thank you for your contribution! However, RISC-V is beyond my current knowledge, and I\u2019m sorry I\u2019m unable to assist with reviewing this part of the code. I recommend reaching out to other team members for assistance with the review.\n\n\n", "pan3793 commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3509563574\n\n   > Is everyone with a risc-v setup able to test this?\r\n   \r\n   to reviewers, https://github.com/apache/hadoop/pull/7924 may help you to set up a dev box on x86 or aarch platform by leveraging Docker & QEMU to simulate riscv env, but it's super super slow, either has no means of performance evaluation.\n\n\n", "steveloughran commented on code in PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#discussion_r2511392823\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32_riscv.c:\n##########\n@@ -16,24 +16,200 @@\n  * limitations under the License.\n  */\n \n+#include <assert.h>\n+#include <stddef.h>  // for size_t\n+#include <stdio.h>\n+#include <string.h>\n+\n+#include \"bulk_crc32.h\"\n+#include \"gcc_optimizations.h\"\n+\n+/**\n+ * Hardware-accelerated CRC32 calculation using RISC-V Zbc extension.\n+ * Uses carry-less multiply instructions (clmul/clmulh) for CRC32 (zlib\n+ * polynomial).\n+ */\n+\n+typedef void (*crc_pipelined_func_t)(uint32_t *, uint32_t *, uint32_t *,\n+                                     const uint8_t *, size_t, int);\n+extern crc_pipelined_func_t pipelined_crc32_zlib_func;\n+\n+#if defined(__riscv) && (__riscv_xlen == 64)\n+\n+#define RV_CRC32_CONST_R3 0x01751997d0ULL\n+#define RV_CRC32_CONST_R4 0x00ccaa009eULL\n+#define RV_CRC32_CONST_R5 0x0163cd6124ULL\n+#define RV_CRC32_MASK32 0x00000000FFFFFFFFULL\n+#define RV_CRC32_POLY_TRUE_LE_FULL 0x01DB710641ULL\n+#define RV_CRC32_CONST_RU 0x01F7011641ULL\n+\n+static inline uint64_t rv_clmul(uint64_t a, uint64_t b) {\n\nReview Comment:\n   explain what it does with a comment. same for the methods below\n\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32_riscv.c:\n##########\n@@ -16,24 +16,200 @@\n  * limitations under the License.\n  */\n \n+#include <assert.h>\n+#include <stddef.h>  // for size_t\n+#include <stdio.h>\n+#include <string.h>\n+\n+#include \"bulk_crc32.h\"\n+#include \"gcc_optimizations.h\"\n+\n+/**\n+ * Hardware-accelerated CRC32 calculation using RISC-V Zbc extension.\n+ * Uses carry-less multiply instructions (clmul/clmulh) for CRC32 (zlib\n+ * polynomial).\n+ */\n+\n+typedef void (*crc_pipelined_func_t)(uint32_t *, uint32_t *, uint32_t *,\n+                                     const uint8_t *, size_t, int);\n+extern crc_pipelined_func_t pipelined_crc32_zlib_func;\n+\n+#if defined(__riscv) && (__riscv_xlen == 64)\n+\n+#define RV_CRC32_CONST_R3 0x01751997d0ULL\n\nReview Comment:\n   explain what these are...assumign they're all defined in the crc spec, say so\n\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32_riscv.c:\n##########\n@@ -16,24 +16,200 @@\n  * limitations under the License.\n  */\n \n+#include <assert.h>\n+#include <stddef.h>  // for size_t\n+#include <stdio.h>\n+#include <string.h>\n+\n+#include \"bulk_crc32.h\"\n+#include \"gcc_optimizations.h\"\n+\n+/**\n+ * Hardware-accelerated CRC32 calculation using RISC-V Zbc extension.\n+ * Uses carry-less multiply instructions (clmul/clmulh) for CRC32 (zlib\n+ * polynomial).\n+ */\n+\n+typedef void (*crc_pipelined_func_t)(uint32_t *, uint32_t *, uint32_t *,\n+                                     const uint8_t *, size_t, int);\n+extern crc_pipelined_func_t pipelined_crc32_zlib_func;\n+\n+#if defined(__riscv) && (__riscv_xlen == 64)\n+\n+#define RV_CRC32_CONST_R3 0x01751997d0ULL\n+#define RV_CRC32_CONST_R4 0x00ccaa009eULL\n+#define RV_CRC32_CONST_R5 0x0163cd6124ULL\n+#define RV_CRC32_MASK32 0x00000000FFFFFFFFULL\n+#define RV_CRC32_POLY_TRUE_LE_FULL 0x01DB710641ULL\n+#define RV_CRC32_CONST_RU 0x01F7011641ULL\n+\n+static inline uint64_t rv_clmul(uint64_t a, uint64_t b) {\n+  uint64_t r;\n+  __asm__ volatile(\n+      \".option push\\n\\t\"\n+      \".option arch, +zbc\\n\\t\"\n+      \"clmul %0, %1, %2\\n\\t\"\n+      \".option pop\\n\\t\"\n+      : \"=r\"(r)\n+      : \"r\"(a), \"r\"(b));\n+  return r;\n+}\n+\n+static inline uint64_t rv_clmulh(uint64_t a, uint64_t b) {\n+  uint64_t r;\n+  __asm__ volatile(\n+      \".option push\\n\\t\"\n+      \".option arch, +zbc\\n\\t\"\n+      \"clmulh %0, %1, %2\\n\\t\"\n+      \".option pop\\n\\t\"\n+      : \"=r\"(r)\n+      : \"r\"(a), \"r\"(b));\n+  return r;\n+}\n+\n+static inline uint32_t rv_crc32_zlib_bitwise(uint32_t crc, const uint8_t *buf,\n+                                             size_t len) {\n+  uint32_t c = crc;\n+  for (size_t i = 0; i < len; ++i) {\n+    c ^= buf[i];\n+    for (int k = 0; k < 8; ++k) {\n+      uint32_t mask = -(int32_t)(c & 1);\n+      c = (c >> 1) ^ (0xEDB88320U & mask);  // reflected polynomial\n+    }\n+  }\n+  return c;\n+}\n+\n+static uint32_t rv_crc32_zlib_clmul(uint32_t crc, const uint8_t *buf,\n+                                    size_t len) {\n+  const uint8_t *p = buf;\n+  size_t n = len;\n+\n+  if (n < 32) {\n+    return rv_crc32_zlib_bitwise(crc, p, n);\n+  }\n+\n+  uintptr_t mis = (uintptr_t)p & 0xF;\n+  if (unlikely(mis)) {\n+    size_t pre = 16 - mis;\n+    if (pre > n) pre = n;\n+    crc = rv_crc32_zlib_bitwise(crc, p, pre);\n+    p += pre;\n+    n -= pre;\n+  }\n+\n+  uint64_t x0 = *(const uint64_t *)(const void *)(p + 0);\n+  uint64_t x1 = *(const uint64_t *)(const void *)(p + 8);\n+  x0 ^= (uint64_t)crc;\n+  p += 16;\n+  n -= 16;\n+\n+  const uint64_t C1 = RV_CRC32_CONST_R3;\n+  const uint64_t C2 = RV_CRC32_CONST_R4;\n+\n+  while (likely(n >= 16)) {\n+    uint64_t tL = rv_clmul(C2, x1);\n+    uint64_t tH = rv_clmulh(C2, x1);\n+    uint64_t yL = rv_clmul(C1, x0);\n+    uint64_t yH = rv_clmulh(C1, x0);\n+    x0 = yL ^ tL;\n+    x1 = yH ^ tH;\n+\n+    uint64_t d0 = *(const uint64_t *)(const void *)(p + 0);\n+    uint64_t d1 = *(const uint64_t *)(const void *)(p + 8);\n+    x0 ^= d0;\n+    x1 ^= d1;\n+    p += 16;\n+    n -= 16;\n+  }\n+\n+  {\n+    uint64_t tH = rv_clmulh(x0, C2);\n+    uint64_t tL = rv_clmul(x0, C2);\n+    x0 = x1 ^ tL;\n+    x1 = tH;\n+  }\n+\n+  uint64_t hi = x1;\n+  uint64_t lo = x0;\n+  uint64_t t2 = (lo >> 32) | (hi << 32);\n+  lo &= RV_CRC32_MASK32;\n+\n+  lo = rv_clmul(RV_CRC32_CONST_R5, lo) ^ t2;\n+  uint64_t tmp = lo;\n+  lo &= RV_CRC32_MASK32;\n+  lo = rv_clmul(lo, RV_CRC32_CONST_RU);\n+  lo &= RV_CRC32_MASK32;\n+  lo = rv_clmul(lo, RV_CRC32_POLY_TRUE_LE_FULL) ^ tmp;\n+\n+  uint32_t c = (uint32_t)(lo >> 32);\n+\n+  if (n) {\n+    c = rv_crc32_zlib_bitwise(c, p, n);\n+  }\n+  return c;\n+}\n+\n /**\n- * RISC-V CRC32 hardware acceleration (placeholder)\n+ * Pipelined version of hardware-accelerated CRC32 calculation using\n+ * RISC-V Zbc carry-less multiply instructions.\n  *\n- * Phase 1: provide a RISC-V-specific compilation unit that currently makes\n- * no runtime changes and falls back to the generic software path in\n- * bulk_crc32.c. Future work will add Zbc-based acceleration and runtime\n- * dispatch.\n+ *   crc1, crc2, crc3 : Store initial checksum for each block before\n+ *           calling. When it returns, updated checksums are stored.\n+ *   p_buf : The base address of the data buffer. The buffer should be\n+ *           at least as big as block_size * num_blocks.\n+ *   block_size : The size of each block in bytes.\n+ *   num_blocks : The number of blocks to work on. Min = 1, Max = 3\n\nReview Comment:\n   0 is taken too, just treated as a no-op. mention and that any other value raises an assertion. which isn't going to be picked up, is it?\n\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32_riscv.c:\n##########\n@@ -16,24 +16,200 @@\n  * limitations under the License.\n  */\n \n+#include <assert.h>\n+#include <stddef.h>  // for size_t\n+#include <stdio.h>\n+#include <string.h>\n+\n+#include \"bulk_crc32.h\"\n+#include \"gcc_optimizations.h\"\n+\n+/**\n+ * Hardware-accelerated CRC32 calculation using RISC-V Zbc extension.\n+ * Uses carry-less multiply instructions (clmul/clmulh) for CRC32 (zlib\n+ * polynomial).\n+ */\n+\n+typedef void (*crc_pipelined_func_t)(uint32_t *, uint32_t *, uint32_t *,\n+                                     const uint8_t *, size_t, int);\n+extern crc_pipelined_func_t pipelined_crc32_zlib_func;\n+\n+#if defined(__riscv) && (__riscv_xlen == 64)\n+\n+#define RV_CRC32_CONST_R3 0x01751997d0ULL\n+#define RV_CRC32_CONST_R4 0x00ccaa009eULL\n+#define RV_CRC32_CONST_R5 0x0163cd6124ULL\n+#define RV_CRC32_MASK32 0x00000000FFFFFFFFULL\n+#define RV_CRC32_POLY_TRUE_LE_FULL 0x01DB710641ULL\n+#define RV_CRC32_CONST_RU 0x01F7011641ULL\n+\n+static inline uint64_t rv_clmul(uint64_t a, uint64_t b) {\n+  uint64_t r;\n+  __asm__ volatile(\n+      \".option push\\n\\t\"\n+      \".option arch, +zbc\\n\\t\"\n+      \"clmul %0, %1, %2\\n\\t\"\n+      \".option pop\\n\\t\"\n+      : \"=r\"(r)\n+      : \"r\"(a), \"r\"(b));\n+  return r;\n+}\n+\n+static inline uint64_t rv_clmulh(uint64_t a, uint64_t b) {\n+  uint64_t r;\n+  __asm__ volatile(\n+      \".option push\\n\\t\"\n+      \".option arch, +zbc\\n\\t\"\n+      \"clmulh %0, %1, %2\\n\\t\"\n+      \".option pop\\n\\t\"\n+      : \"=r\"(r)\n+      : \"r\"(a), \"r\"(b));\n+  return r;\n+}\n+\n+static inline uint32_t rv_crc32_zlib_bitwise(uint32_t crc, const uint8_t *buf,\n+                                             size_t len) {\n+  uint32_t c = crc;\n+  for (size_t i = 0; i < len; ++i) {\n+    c ^= buf[i];\n+    for (int k = 0; k < 8; ++k) {\n+      uint32_t mask = -(int32_t)(c & 1);\n+      c = (c >> 1) ^ (0xEDB88320U & mask);  // reflected polynomial\n+    }\n+  }\n+  return c;\n+}\n+\n+static uint32_t rv_crc32_zlib_clmul(uint32_t crc, const uint8_t *buf,\n+                                    size_t len) {\n+  const uint8_t *p = buf;\n+  size_t n = len;\n+\n+  if (n < 32) {\n+    return rv_crc32_zlib_bitwise(crc, p, n);\n+  }\n+\n+  uintptr_t mis = (uintptr_t)p & 0xF;\n\nReview Comment:\n   comment that this is to handle misaligned data and that this is considered unlikely\n\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32_riscv.c:\n##########\n@@ -16,24 +16,200 @@\n  * limitations under the License.\n  */\n \n+#include <assert.h>\n+#include <stddef.h>  // for size_t\n+#include <stdio.h>\n+#include <string.h>\n+\n+#include \"bulk_crc32.h\"\n+#include \"gcc_optimizations.h\"\n+\n+/**\n+ * Hardware-accelerated CRC32 calculation using RISC-V Zbc extension.\n+ * Uses carry-less multiply instructions (clmul/clmulh) for CRC32 (zlib\n+ * polynomial).\n+ */\n+\n+typedef void (*crc_pipelined_func_t)(uint32_t *, uint32_t *, uint32_t *,\n+                                     const uint8_t *, size_t, int);\n+extern crc_pipelined_func_t pipelined_crc32_zlib_func;\n+\n+#if defined(__riscv) && (__riscv_xlen == 64)\n+\n+#define RV_CRC32_CONST_R3 0x01751997d0ULL\n+#define RV_CRC32_CONST_R4 0x00ccaa009eULL\n+#define RV_CRC32_CONST_R5 0x0163cd6124ULL\n+#define RV_CRC32_MASK32 0x00000000FFFFFFFFULL\n+#define RV_CRC32_POLY_TRUE_LE_FULL 0x01DB710641ULL\n+#define RV_CRC32_CONST_RU 0x01F7011641ULL\n+\n+static inline uint64_t rv_clmul(uint64_t a, uint64_t b) {\n+  uint64_t r;\n+  __asm__ volatile(\n+      \".option push\\n\\t\"\n+      \".option arch, +zbc\\n\\t\"\n+      \"clmul %0, %1, %2\\n\\t\"\n+      \".option pop\\n\\t\"\n+      : \"=r\"(r)\n+      : \"r\"(a), \"r\"(b));\n+  return r;\n+}\n+\n+static inline uint64_t rv_clmulh(uint64_t a, uint64_t b) {\n+  uint64_t r;\n+  __asm__ volatile(\n+      \".option push\\n\\t\"\n+      \".option arch, +zbc\\n\\t\"\n+      \"clmulh %0, %1, %2\\n\\t\"\n+      \".option pop\\n\\t\"\n+      : \"=r\"(r)\n+      : \"r\"(a), \"r\"(b));\n+  return r;\n+}\n+\n+static inline uint32_t rv_crc32_zlib_bitwise(uint32_t crc, const uint8_t *buf,\n+                                             size_t len) {\n+  uint32_t c = crc;\n+  for (size_t i = 0; i < len; ++i) {\n+    c ^= buf[i];\n+    for (int k = 0; k < 8; ++k) {\n+      uint32_t mask = -(int32_t)(c & 1);\n+      c = (c >> 1) ^ (0xEDB88320U & mask);  // reflected polynomial\n+    }\n+  }\n+  return c;\n+}\n+\n+static uint32_t rv_crc32_zlib_clmul(uint32_t crc, const uint8_t *buf,\n+                                    size_t len) {\n+  const uint8_t *p = buf;\n+  size_t n = len;\n+\n+  if (n < 32) {\n+    return rv_crc32_zlib_bitwise(crc, p, n);\n+  }\n+\n+  uintptr_t mis = (uintptr_t)p & 0xF;\n+  if (unlikely(mis)) {\n+    size_t pre = 16 - mis;\n+    if (pre > n) pre = n;\n+    crc = rv_crc32_zlib_bitwise(crc, p, pre);\n+    p += pre;\n+    n -= pre;\n+  }\n+\n+  uint64_t x0 = *(const uint64_t *)(const void *)(p + 0);\n+  uint64_t x1 = *(const uint64_t *)(const void *)(p + 8);\n+  x0 ^= (uint64_t)crc;\n+  p += 16;\n+  n -= 16;\n+\n+  const uint64_t C1 = RV_CRC32_CONST_R3;\n+  const uint64_t C2 = RV_CRC32_CONST_R4;\n+\n+  while (likely(n >= 16)) {\n\nReview Comment:\n   and explain this is going through 16 bytes of aligned data\n\n\n\n", "leiwen2025 commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3550285408\n\n   Hi @PeterPtroc ,\r\n   I received a suggestion on my PR to collaborate with you on optimizing the CRC32-related design for better performance.\r\n   I\u2019m happy to work together \u2014 maybe we can review each other\u2019s approaches and align on a combined solution that provides the best CRC32 performance.\n\n\n", "steveloughran commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3563411435\n\n   @PeterPtroc as noted, @leiwen2025 can help here.\r\n   \r\n   @leiwen2025 -can you look at this PR as is and review it. Ideally: check it out and do a -Pnative build running the native tests. \r\n   \r\n   If you two are using different instructions, how do they differ.\r\n   \r\n   Having just looked at what clmul/clmulh does, I can see why it offers benefits\r\n   * how common is the instruction?\r\n   * @leiwen2025 how does your vectorized compare? This opcode is intended to be pipelined and the opcode is designed for these kind of encryption/checksum algorithms.\r\n   \r\n   Looking at #7912 it's calling vclmul.vv -this is generally going to be faster, isn't it?\r\n   \r\n   Which means that while the code is more complex, ultimately it's going be the best option on cores with the right feature flaggs.\r\n   \r\n   This makes me think that this one can go in but the vector one goes in as the followup, with the choice of operation dependent on feature, with priority of: vclmul, cmul, classic. \r\n   \r\n   \r\n   \n\n\n", "leiwen2025 commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3570455914\n\n   > @PeterPtroc as noted, @leiwen2025 can help here.\r\n   > \r\n   > @leiwen2025 -can you look at this PR as is and review it. Ideally: check it out and do a -Pnative build running the native tests.\r\n   > \r\n   > If you two are using different instructions, how do they differ.\r\n   > \r\n   > Having just looked at what clmul/clmulh does, I can see why it offers benefits\r\n   > \r\n   > * how common is the instruction?\r\n   > * @leiwen2025 how does your vectorized compare? This opcode is intended to be pipelined and the opcode is designed for these kind of encryption/checksum algorithms.\r\n   > \r\n   > Looking at #7912 it's calling vclmul.vv -this is generally going to be faster, isn't it?\r\n   > \r\n   > Which means that while the code is more complex, ultimately it's going be the best option on cores with the right feature flaggs.\r\n   > \r\n   > This makes me think that this one can go in but the vector one goes in as the followup, with the choice of operation dependent on feature, with priority of: vclmul, cmul, classic.\r\n   \r\n   @steveloughran Thanks! I\u2019m happy to help. I\u2019ll check out the PR as-is and run a `-Pnative` build with the native tests to verify. Will report back once I have the results.\n\n\n"], "labels": ["native", "pull-request-available", "risc-v"], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "[RISC-V]  Add rv bulk CRC32 (non-CRC32C) optimized path"}, {"question": "Who reported this issue?", "answer": "Ptroc"}]}
{"key": "HADOOP-19723", "project": "HADOOP", "title": "Build multi-arch hadoop image", "status": "Resolved", "reporter": "Attila Doroszlai", "created": "2025-10-09T09:22:57.000+0000", "description": "Build {{apache/hadoop}} Docker image for both amd64 and arm64.", "comments": ["adoroszlai opened a new pull request, #8023:\nURL: https://github.com/apache/hadoop/pull/8023\n\n   ## What changes were proposed in this pull request?\r\n   \r\n   - Update `Dockerfile` (on branch `docker-hadoop-3.4.2-lean`) to support building for `arm64`, too.\r\n       - Use `ghcr.io/apache/hadoop-runner:jdk11-u2204` as base, because `apache/hadoop-runner:latest` only has `amd64` image available.\r\n       - Use `TARGETPLATFORM` to decide which tarball to use.\r\n       - Create args for version and flavor, replacing URL.\r\n   - Update the `build-hadoop-image` workflow to create multi-arch images.\r\n   - Add build-arg `BASE_URL` to allow using mirrors (for faster local build).\r\n   - Replace deprecated `ENV HADOOP_CONF_DIR ` syntax.\r\n   \r\n   https://issues.apache.org/jira/browse/HADOOP-19723\r\n   \r\n   ## How was this patch tested?\r\n   \r\n   Workflow [run](https://github.com/adoroszlai/hadoop/actions/runs/18377713437) in my fork created multi-arch [image](https://github.com/adoroszlai/hadoop/pkgs/container/hadoop/539671710?tag=HADOOP-19723).\r\n   \r\n   ```\r\n   #8 0.060 Building for linux/amd64\r\n   ...\r\n   #8 0.060 + export HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2-lean.tar.gz\r\n   ...\r\n   \r\n   #10 0.076 Building for linux/arm64\r\n   ...\r\n   #10 0.077 + export HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2-aarch64-lean.tar.gz\r\n   ```\r\n   \r\n   Tested on both amd64 and arm64 platforms.\r\n   \r\n   ```\r\n   $ docker run -it --rm ghcr.io/adoroszlai/hadoop:HADOOP-19723 bash -c \"uname -a; hadoop version\"\r\n   Linux cdb5cdd5ace9 6.8.0-65-generic #68~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 15 18:06:34 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n   Hadoop 3.4.2\r\n   Source code repository https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c\r\n   Compiled by ahmarsu on 2025-08-20T10:30Z\r\n   Compiled on platform linux-x86_64\r\n   Compiled with protoc 3.23.4\r\n   From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6\r\n   This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.4.2.jar\r\n   ```\r\n   \r\n   ```\r\n   $ docker run -it --rm ghcr.io/adoroszlai/hadoop:HADOOP-19723 bash -c \"uname -a; hadoop version\"\r\n   Linux 9a1237ba8fbc 6.10.14-linuxkit #1 SMP Thu Oct 24 19:28:55 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux\r\n   Hadoop 3.4.2\r\n   Source code repository https://github.com/apache/hadoop.git -r e1c0dee881820a4d834ec4a4d2c70d0d953bb933\r\n   Compiled by ahmar on 2025-08-07T15:32Z\r\n   Compiled on platform linux-aarch_64\r\n   Compiled with protoc 3.23.4\r\n   From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6\r\n   This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.4.2.jar\r\n   ```\n\n\n", "slfan1989 commented on PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023#issuecomment-3385915162\n\n   LGTM.\n\n\n", "slfan1989 commented on PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023#issuecomment-3385930578\n\n   @adoroszlai Thank you for the contribution. If there are no additional comments, I\u2019ll proceed to merge this PR shortly.\n\n\n", "adoroszlai commented on PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023#issuecomment-3386892439\n\n   @smengcl would you like to take a look?\n\n\n", "slfan1989 commented on PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023#issuecomment-3388049001\n\n   > @smengcl would you like to take a look?\r\n   \r\n   @smengcl I believe this PR is fine, and since @adoroszlai  has extensive experience with this, any issues that may arise in the future can be quickly addressed. I will go ahead and merge this PR.\n\n\n", "slfan1989 merged PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023\n\n\n", "adoroszlai commented on PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023#issuecomment-3388373138\n\n   Thanks @slfan1989 for reviewing and merging this.\n\n\n", "smengcl commented on PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023#issuecomment-3388411324\n\n   Thanks @adoroszlai . The patch looks good.\r\n   \r\n   Thanks @slfan1989 for reviewing this as well.\n\n\n"], "labels": ["pull-request-available"], "summary": "Build {{apache/hadoop}} Docker image for both amd64 and arm64", "qna": [{"question": "What is the issue title?", "answer": "Build multi-arch hadoop image"}, {"question": "Who reported this issue?", "answer": "Attila Doroszlai"}]}
{"key": "HADOOP-19722", "project": "HADOOP", "title": "Pin robotframework version", "status": "Resolved", "reporter": "Attila Doroszlai", "created": "2025-10-09T07:00:41.000+0000", "description": "{{hadoop-runner}} installs {{robotframework}} without version definition.  Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too.", "comments": ["adoroszlai opened a new pull request, #8025:\nURL: https://github.com/apache/hadoop/pull/8025\n\n   ## What changes were proposed in this pull request?\r\n   \r\n   `hadoop-runner` installs `robotframework` without version definition.  Re-building the image for unrelated changes may unexpectedly upgrade `robotframework`, too.\r\n   \r\n   This PR proposes to pin `robotframework` to version 6.1.1.\r\n   \r\n   https://issues.apache.org/jira/browse/HADOOP-19722\r\n   \r\n   ## How was this patch tested?\r\n   \r\n   ```\r\n   $ docker build -t hadoop-runner:dev .\r\n   ...\r\n   \r\n   $ docker run -it --rm hadoop-runner:dev robot --version\r\n   Robot Framework 6.1.1 (Python 3.10.12 on linux)\r\n   ```\r\n   \n\n\n", "smengcl merged PR #8025:\nURL: https://github.com/apache/hadoop/pull/8025\n\n\n", "adoroszlai commented on PR #8025:\nURL: https://github.com/apache/hadoop/pull/8025#issuecomment-3395090403\n\n   Thanks @slfan1989, @smengcl for the review.\n\n\n"], "labels": ["pull-request-available"], "summary": "{{hadoop-runner}} installs {{robotframework}} without version definition", "qna": [{"question": "What is the issue title?", "answer": "Pin robotframework version"}, {"question": "Who reported this issue?", "answer": "Attila Doroszlai"}]}
{"key": "HADOOP-19721", "project": "HADOOP", "title": "Upgrade hadoop-runner to Ubuntu 24.04", "status": "Open", "reporter": "Attila Doroszlai", "created": "2025-10-09T06:23:41.000+0000", "description": "Latest {{hadoop-runner}} images are based on Ubuntu 22.04.  Upgrade to 24.04.", "comments": [], "labels": [], "summary": "Latest {{hadoop-runner}} images are based on Ubuntu 22", "qna": [{"question": "What is the issue title?", "answer": "Upgrade hadoop-runner to Ubuntu 24.04"}, {"question": "Who reported this issue?", "answer": "Attila Doroszlai"}]}
{"key": "HADOOP-19720", "project": "HADOOP", "title": "Publish multi-arch hadoop-runner image to GitHub", "status": "Resolved", "reporter": "Attila Doroszlai", "created": "2025-10-08T17:21:10.000+0000", "description": "Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry.  Build for both amd64 and arm64.", "comments": ["adoroszlai opened a new pull request, #8021:\nURL: https://github.com/apache/hadoop/pull/8021\n\n   ## What changes were proposed in this pull request?\r\n   \r\n   Add workflow to publish `apache/hadoop-runner` (`jdk11-u2204` in this case) to GitHub Container Registry.\r\n   \r\n   https://issues.apache.org/jira/browse/HADOOP-19720\r\n   \r\n   ## How was this patch tested?\r\n   \r\n   [Workflow run](https://github.com/adoroszlai/hadoop/actions/runs/18353000644) in my fork for push to branch `docker-hadoop-runner-HADOOP-19720-jdk11-u2204` built [image](https://github.com/adoroszlai/hadoop/pkgs/container/hadoop-runner/538747134?tag=HADOOP-19720-jdk11-u2204) `ghcr.io/adoroszlai/hadoop-runner:HADOOP-19720-jdk11-u2204`.  It has both amd64 and arm64 arch.\r\n   \r\n   ```bash\r\n   $ docker run -it --rm ghcr.io/adoroszlai/hadoop-runner:HADOOP-19720-jdk11-u2204 bash -c 'uname -a; cat /etc/lsb-release; java -version'\r\n   Linux 8099f50d7322 6.8.0-65-generic #68~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 15 18:06:34 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n   DISTRIB_ID=Ubuntu\r\n   DISTRIB_RELEASE=22.04\r\n   DISTRIB_CODENAME=jammy\r\n   DISTRIB_DESCRIPTION=\"Ubuntu 22.04.5 LTS\"\r\n   openjdk version \"11.0.28\" 2025-07-15\r\n   OpenJDK Runtime Environment Temurin-11.0.28+6 (build 11.0.28+6)\r\n   OpenJDK 64-Bit Server VM Temurin-11.0.28+6 (build 11.0.28+6, mixed mode, sharing)\r\n   ```\n\n\n", "slfan1989 commented on PR #8021:\nURL: https://github.com/apache/hadoop/pull/8021#issuecomment-3383664657\n\n   @adoroszlai Many thanks for your contribution. Could we consider upgrading the image to ubuntu 24.04?\n\n\n", "smengcl merged PR #8021:\nURL: https://github.com/apache/hadoop/pull/8021\n\n\n", "smengcl commented on PR #8021:\nURL: https://github.com/apache/hadoop/pull/8021#issuecomment-3383797788\n\n   > @adoroszlai Many thanks for your contribution. Could we consider upgrading the image to ubuntu 24.04?\r\n   \r\n   We could file a new jira for that task.\n\n\n", "slfan1989 commented on PR #8021:\nURL: https://github.com/apache/hadoop/pull/8021#issuecomment-3383809271\n\n   > > @adoroszlai Many thanks for your contribution. Could we consider upgrading the image to ubuntu 24.04?\r\n   > \r\n   > We could file a new jira for that task.\r\n   \r\n   You\u2019ve got a valid point. The branch name is indeed for 22.04, so we can address it in the next JIRA task.\n\n\n", "adoroszlai commented on PR #8021:\nURL: https://github.com/apache/hadoop/pull/8021#issuecomment-3384254098\n\n   Thanks @slfan1989, @smengcl for the review.\n\n\n"], "labels": ["pull-request-available"], "summary": "Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image ", "qna": [{"question": "What is the issue title?", "answer": "Publish multi-arch hadoop-runner image to GitHub"}, {"question": "Who reported this issue?", "answer": "Attila Doroszlai"}]}
{"key": "HADOOP-19719", "project": "HADOOP", "title": "Upgrade to wildfly version with support for openssl 3", "status": "Resolved", "reporter": "Steve Loughran", "created": "2025-10-08T10:52:33.000+0000", "description": "Wildfly 2.1.4\r\n* doesn't work with openssl 3 (that symbol change...why did they do that?)\r\n\r\n\r\nwe need a version with \r\nhttps://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d\r\n\r\n2.2.5.Final does the openssl 3 support. \r\n\r\n*important* there is a followup patch with the same JIRA ID to ensure that {{TestDelegatingSSLSocketFactory}} is skipped on RHEL8 systems. You MUST include this when backporting", "comments": ["steveloughran opened a new pull request, #8019:\nURL: https://github.com/apache/hadoop/pull/8019\n\n   \r\n   ### How was this patch tested?\r\n   \r\n   Going to see if it works on a mac...\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8019:\nURL: https://github.com/apache/hadoop/pull/8019#issuecomment-3381330281\n\n   the test which is parameterized on ssl (and storediag when a store is forced to OpenSSL)\r\n   ```\r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractSeek.testReadFullyZeroBytebufferPastEOF ", "steveloughran commented on PR #8019:\nURL: https://github.com/apache/hadoop/pull/8019#issuecomment-3381381104\n\n   s3a tests all good, s3 london `-Dparallel-tests -DtestsThreadCount=8`\n\n\n", "steveloughran commented on PR #8019:\nURL: https://github.com/apache/hadoop/pull/8019#issuecomment-3405882774\n\n   OK, 2.2.5 doesn't include the arm linux binaries. It does in our private builds, which is why I was confused.\n\n\n", "steveloughran merged PR #8019:\nURL: https://github.com/apache/hadoop/pull/8019\n\n\n", "This is probably the cause of yetus test failures\r\n{code}\r\njava.io.IOException: java.security.NoSuchAlgorithmException: Error constructing implementation (algorithm: openssl.TLS, provider: openssl, class: org.wildfly.openssl.OpenSSLContextSPI$OpenSSLTLSContextSpi)\r\n\tat org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory.<init>(DelegatingSSLSocketFactory.java:137)\r\n\tat org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory.initializeDefaultFactory(DelegatingSSLSocketFactory.java:106)\r\n\tat org.apache.hadoop.security.ssl.TestDelegatingSSLSocketFactory.testOpenSSL(TestDelegatingSSLSocketFactory.java:42)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\nCaused by: java.security.NoSuchAlgorithmException: Error constructing implementation (algorithm: openssl.TLS, provider: openssl, class: org.wildfly.openssl.OpenSSLContextSPI$OpenSSLTLSContextSpi)\r\n\tat java.base/java.security.Provider$Service.newInstance(Provider.java:1924)\r\n\tat java.base/sun.security.jca.GetInstance.getInstance(GetInstance.java:236)\r\n\tat java.base/sun.security.jca.GetInstance.getInstance(GetInstance.java:164)\r\n\tat java.base/javax.net.ssl.SSLContext.getInstance(SSLContext.java:185)\r\n\tat org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory.bindToOpenSSLProvider(DelegatingSSLSocketFactory.java:207)\r\n\tat org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory.initializeSSLContext(DelegatingSSLSocketFactory.java:170)\r\n\tat org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory.<init>(DelegatingSSLSocketFactory.java:135)\r\n\t... 5 more\r\nCaused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException\r\n\tat org.wildfly.openssl.SSL.init(SSL.java:87)\r\n\tat org.wildfly.openssl.OpenSSLContextSPI.<init>(OpenSSLContextSPI.java:137)\r\n\tat org.wildfly.openssl.OpenSSLContextSPI$OpenSSLTLSContextSpi.<init>(OpenSSLContextSPI.java:453)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat java.base/java.security.Provider$Service.newInstanceOf(Provider.java:1935)\r\n\tat java.base/java.security.Provider$Service.newInstanceUtil(Provider.java:1942)\r\n\tat java.base/java.security.Provider$Service.newInstance(Provider.java:1917)\r\n\t... 11 more\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.wildfly.openssl.SSL.init(SSL.java:82)\r\n\t... 21 more\r\nCaused by: java.lang.UnsatisfiedLinkError: /tmp/tmp-9253732007141816518openssl/libwfssl.so: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /tmp/tmp-9253732007141816518openssl/libwfssl.so)\r\n\tat java.base/jdk.internal.loader.NativeLibraries.load(Native Method)\r\n\tat java.base/jdk.internal.loader.NativeLibraries$NativeLibraryImpl.open(NativeLibraries.java:388)\r\n\tat java.base/jdk.internal.loader.NativeLibraries.loadLibrary(NativeLibraries.java:232)\r\n\tat java.base/jdk.internal.loader.NativeLibraries.loadLibrary(NativeLibraries.java:174)\r\n\tat java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2420)\r\n\tat java.base/java.lang.Runtime.loadLibrary0(Runtime.java:818)\r\n\tat java.base/java.lang.System.loadLibrary(System.java:2006)\r\n\tat org.wildfly.openssl.SSL$LibraryLoader.load(SSL.java:288)\r\n\t... 26 more\r\n{code}\r\n\r\nwildfly native libs need to be built against an older version of glibc.\r\n\r\n* I don't want to roll back to the older version as removes the openssl 3.0 option\r\n* we can't do our own private build/release (we,, we could, but I don't want to own anything ssl related)\r\n\r\nProposed we catch and downgrade this test failure", "steveloughran opened a new pull request, #8071:\nURL: https://github.com/apache/hadoop/pull/8071\n\n   \r\n   Downgrade to a skipped tests if the OS doesn't have GLIBC_2.34.\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran opened a new pull request, #8075:\nURL: https://github.com/apache/hadoop/pull/8075\n\n   \r\n   Downgrade to a skipped tests if the OS doesn't have GLIBC_2.34.\r\n   \r\n   The branch-3.4 version of the patch doesn't delete testJSEENoGCMJava8() and uses classic junit assumeTrue() calls.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Yetus needs to do this\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8071:\nURL: https://github.com/apache/hadoop/pull/8071#issuecomment-3516554865\n\n   the build seemed to fail in publishing, but the modified test wasn't executed, so we can't be sure it works again. I will have to rebase this after another change goes into trunk\n\n\n", "hadoop-yetus commented on PR #8075:\nURL: https://github.com/apache/hadoop/pull/8075#issuecomment-3517080429\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  2s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 51s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |  16m 32s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 18s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 15s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 30s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 28s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 49s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  compile  |   1m 12s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   1m 12s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   1m  6s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   1m  6s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 50s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 58s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 48s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  shadedclient  |   8m 55s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 58s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 30s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8075 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ea0517b729be 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / 810015b8c9fe620bf23e174a7159b1f922204088 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/testReport/ |\r\n   | Max. process+thread count | 552 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8075:\nURL: https://github.com/apache/hadoop/pull/8075#issuecomment-3517628608\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  6s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 19s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |  16m 45s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 21s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 33s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 49s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  compile  |   1m 11s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   1m 11s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   1m  7s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   1m  7s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 48s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 54s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  shadedclient  |   8m 12s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 55s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m 53s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8075 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1f4776ca6797 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / c443d1df82b1be1ef3f3c3a08b6768cbd9903345 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8071:\nURL: https://github.com/apache/hadoop/pull/8071#issuecomment-3522277088\n\n   force pushed to trigger a rebuild; just changed the commit text\n\n\n", "hadoop-yetus commented on PR #8071:\nURL: https://github.com/apache/hadoop/pull/8071#issuecomment-3523174131\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 39s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 44s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   3m  2s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/2/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) |  hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 55s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 56s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 53s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  21m 59s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/2/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  3s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 204m 10s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8071 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 48a8f70747ac 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 69931447b23123427249eb1496582b93d7dc6e22 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/2/testReport/ |\r\n   | Max. process+thread count | 1810 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8075:\nURL: https://github.com/apache/hadoop/pull/8075#issuecomment-3523248122\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   3m  1s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  38m  4s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/4/artifact/out/branch-mvninstall-root.txt) |  root in branch-3.4 failed.  |\r\n   | +1 :green_heart: |  compile  |  17m  8s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m  9s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 17s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 55s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 34s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 10s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 54s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m  5s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 10s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 11s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 55s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 30s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  18m 58s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/4/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 211m 22s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8075 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux eed472423547 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / 7eeec006b38e8783b82b5b49d857ec7f91092275 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/4/testReport/ |\r\n   | Max. process+thread count | 1263 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8071:\nURL: https://github.com/apache/hadoop/pull/8071#issuecomment-3529114129\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 49s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 58s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 22s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   2m 59s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/3/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) |  hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 36s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 29s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 21s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 46s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  21m 56s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/3/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  2s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 197m 33s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8071 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6366df5a7c9f 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d1b7076e7a9e241a2a4aeb3e322cb1ddd80bb006 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/3/testReport/ |\r\n   | Max. process+thread count | 3153 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8071:\nURL: https://github.com/apache/hadoop/pull/8071#issuecomment-3533108818\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  32m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 34s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 57s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 17s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   3m  3s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/4/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) |  hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  29m 18s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 59s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m  8s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 21s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 46s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 27s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  1s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 196m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8071 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a14da3b36332 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b72c0fb68b9910877bc01b422a356ea96599fc75 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/4/testReport/ |\r\n   | Max. process+thread count | 1301 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8071/4/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran merged PR #8071:\nURL: https://github.com/apache/hadoop/pull/8071\n\n\n", "steveloughran commented on PR #8075:\nURL: https://github.com/apache/hadoop/pull/8075#issuecomment-3547009009\n\n   build failure rather than test failure; will rebase and repeat. As this is a problem affecting yetus vms, I have to see it works here before merging\n\n\n", "hadoop-yetus commented on PR #8075:\nURL: https://github.com/apache/hadoop/pull/8075#issuecomment-3547914345\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 25s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 25s |  |  branch-3.4 passed  |\r\n   | -1 :x: |  compile  |  11m 39s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/6/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in branch-3.4 failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  compile  |  15m 40s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 20s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 14s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 33s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 10s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 34s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 12s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  6s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  18m 58s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 204m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8075 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d88f58635e50 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / 8f09ead1cd53f058af2200e8ca33efb9d9fa4d3e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/6/testReport/ |\r\n   | Max. process+thread count | 1263 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8075/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran merged PR #8075:\nURL: https://github.com/apache/hadoop/pull/8075\n\n\n"], "labels": ["pull-request-available"], "summary": "Wildfly 2", "qna": [{"question": "What is the issue title?", "answer": "Upgrade to wildfly version with support for openssl 3"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19718", "project": "HADOOP", "title": "[ABFS]: Throw HTTPException when AAD token fetch fails ", "status": "Open", "reporter": "Sneha Vijayarajan", "created": "2025-10-07T12:56:26.000+0000", "description": "Reported by [~enigma25] :\r\nIn [this code snippet](https://github.com/Azure/azure-data-lake-store-java/blob/26eca936da60286aa70b0dd7823ff5e627987bc4/src/main/java/com/microsoft/azure/datalake/store/oauth2/AzureADAuthenticator.java#L252-L293) from AzureADAuthenticator, the `conn` is being returned as null due to a (possibly) corrupted connection between Azure Data Lake Store Client and AAD while fetching AAD token. The code snippet linked, runs into an NPE. I wanted to know from the maintainers and community if this bug/symptom has been seen by them earlier and what might be the best way to handle this? Secondly, I wanted to know the right place for the fix too - whether it should be the application code or the SDK code itself should handle such NPEs and fail more gracefully?\r\nOpen to thoughts and comments.\r\nCheers,\r\nNikhil\r\n\u00a0\r\n\u00a0\r\n```\r\njava.util.concurrent.ExecutionException: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.result(ConvertingFutureCallback.java:135)\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.get(ConvertingFutureCallback.java:122)\r\nat org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource.validateConfigs(ConnectorPluginsResource.java:129)\r\nat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\nat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\r\nat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:219)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\r\nat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:478)\r\nat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:400)\r\nat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\r\nat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:256)\r\nat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\r\nat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\r\nat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\r\nat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\r\nat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\r\nat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\r\nat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)\r\nat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\r\nat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\r\nat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)\r\nat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:358)\r\nat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:311)\r\nat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)\r\nat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\r\nat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\nat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\nat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\r\nat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\r\nat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\r\nat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\nat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\r\nat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\r\nat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\r\nat org.eclipse.jetty.server.Server.handle(Server.java:516)\r\nat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\r\nat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\r\nat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\r\nat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\r\nat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\r\nat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\r\nat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\r\nat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\r\nat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\r\nat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\r\nat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.consumeInputStream(AzureADAuthenticator.java:345)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:275)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:216)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenUsingClientCreds(AzureADAuthenticator.java:95)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider.refreshToken(ClientCredsTokenProvider.java:58)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider.getToken(AccessTokenProvider.java:50)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAccessToken(AbfsClient.java:583)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:162)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getFilesystemProperties(AbfsClient.java:205)\r\nat io.confluent.connect.azure.datalake.gen2.validation.Validations.verifyClient(Validations.java:175)\r\nat io.confluent.connect.azure.datalake.gen2.validation.Validations.createAndValidateClient(Validations.java:395)\r\nat io.confluent.connect.azure.datalake.gen2.validation.Validations.validateAll(Validations.java:131)\r\nat io.confluent.connect.utils.validators.all.ConfigValidation.lambda$callValidators$0(ConfigValidation.java:222)\r\nat java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024)\r\nat java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)\r\nat io.confluent.connect.utils.validators.all.ConfigValidation.callValidators(ConfigValidation.java:222)\r\nat io.confluent.connect.utils.validators.all.ConfigValidation.validate(ConfigValidation.java:182)\r\nat io.confluent.connect.azure.datalake.gen2.AzureDataLakeGen2SinkConnector.validate(AzureDataLakeGen2SinkConnector.java:97)\r\nat org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:641)\r\nat org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$7(AbstractHerder.java:493)\r\nat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\r\nat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n... 1 more\r\n```", "comments": [], "labels": [], "summary": "Reported by [~enigma25] :\r\nIn [this code snippet](https://github", "qna": [{"question": "What is the issue title?", "answer": "[ABFS]: Throw HTTPException when AAD token fetch fails "}, {"question": "Who reported this issue?", "answer": "Sneha Vijayarajan"}]}
{"key": "HADOOP-19717", "project": "HADOOP", "title": "Resolve build error caused by missing Checker Framework (NonNull not recognized)", "status": "Resolved", "reporter": "Shilun Fan", "created": "2025-10-07T03:49:55.000+0000", "description": "In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed.\r\n{code:java}\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,50] package org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol\r\n[ERROR]   symbol:   class NonNull\r\n[ERROR]   location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory\r\n {code}\r\nI checked the usage in the related modules, and we should use *org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.NonNull* instead of directly using {*}org.checkerframework.checker.nullness.qual.NonNull{*}.\r\n\r\n\u00a0", "comments": ["slfan1989 opened a new pull request, #8015:\nURL: https://github.com/apache/hadoop/pull/8015\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19717. Resolve build error caused by missing Checker Framework  (NonNull not recognized).\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "pan3793 commented on code in PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#discussion_r2409341008\n\n\n##########\nhadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:\n##########\n@@ -19,7 +19,7 @@\n package org.apache.hadoop.fs.tosfs.util;\n \n import org.apache.hadoop.util.Preconditions;\n-import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n\nReview Comment:\n   I suspect this makes the `Nullable` useless, I don't think the static analyzer tools can recognize such a relocated annotation.\n\n\n\n", "hadoop-yetus commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3375421893\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 35s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  22m 41s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   5m 25s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   4m 39s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   0m 24s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 25s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in trunk failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 16s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  24m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 23s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 37s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |   5m 41s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   5m 41s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   5m 11s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   5m 10s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  38m 54s |  |  hadoop-hdfs-rbf in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 54s |  |  hadoop-tos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 154m  4s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8015 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1ad3180c6b2e 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f0c771fd1f48e1cc45617d4e2eb0afb552e5ba1f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/testReport/ |\r\n   | Max. process+thread count | 4610 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on code in PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#discussion_r2410285097\n\n\n##########\nhadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:\n##########\n@@ -19,7 +19,7 @@\n package org.apache.hadoop.fs.tosfs.util;\n \n import org.apache.hadoop.util.Preconditions;\n-import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n\nReview Comment:\n   I think what you said makes some sense, but there are similar users in AzureBFS as well.\r\n   \r\n   https://github.com/apache/hadoop/blob/1566613c725979d0ccda45822dfa275cbd97467a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java#L38\n\n\n\n", "slfan1989 commented on code in PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#discussion_r2410285097\n\n\n##########\nhadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:\n##########\n@@ -19,7 +19,7 @@\n package org.apache.hadoop.fs.tosfs.util;\n \n import org.apache.hadoop.util.Preconditions;\n-import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n\nReview Comment:\n   I think what you said makes some sense, but there are similar users in AzureBFS as well.\r\n   \r\n   https://github.com/apache/hadoop/blob/1566613c725979d0ccda45822dfa275cbd97467a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java#L38\r\n   \r\n   @steveloughran I\u2019d like to hear your thoughts \u2014 do you think we should reintroduce a new dependency to resolve the issue where org.checkerframework.checker.nullness.qual.Nullable cannot be found?\r\n   \r\n   cc: @szetszwo \n\n\n\n", "hadoop-yetus commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3376863181\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 12s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  22m 36s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   5m 21s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   4m 43s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 24s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   0m 24s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   1m  4s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 22s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in trunk failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 17s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 22s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m  4s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |   5m 49s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   5m 49s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   5m  5s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   5m  5s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  hadoop-yarn-server-resourcemanager in the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  hadoop-yarn-server-resourcemanager in the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2)  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  90m 40s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  38m  2s |  |  hadoop-hdfs-rbf in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 53s |  |  hadoop-tos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 241m 25s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8015 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux de756f6ac704 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / aca0e73a716b49f41b6eb3e0a57c876842a258a8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/testReport/ |\r\n   | Max. process+thread count | 4761 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377007084\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 46s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  26m 13s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   6m 12s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   5m 18s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 19s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   0m 25s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 25s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in trunk failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 16s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 25s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 41s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |   6m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   6m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   5m 50s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   5m 50s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  39m  4s |  |  hadoop-hdfs-rbf in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 54s |  |  hadoop-tos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 158m 17s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8015 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 30e4a84a468c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f0c771fd1f48e1cc45617d4e2eb0afb552e5ba1f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/testReport/ |\r\n   | Max. process+thread count | 4202 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377166843\n\n   @szetszwo @pan3793 My thought is that since `AzureBFS` already uses this approach, we should be able to apply the same solution in other places as well. For now, we can use`org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable` instead of `org.checkerframework.checker.nullness.qual.Nullable` to unblock the trunk build issue first. \r\n   \r\n   A follow-up PR can be submitted later to fully resolve this dependency problem in a cleaner way.\r\n   \r\n   Currently, the build result is as expected \u2014 before applying this patch, the trunk could not compile successfully, but after merging it, the build now passes under both JDK 8 and JDK 11.\n\n\n", "szetszwo commented on code in PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#discussion_r2411050812\n\n\n##########\nhadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:\n##########\n@@ -19,7 +19,7 @@\n package org.apache.hadoop.fs.tosfs.util;\n \n import org.apache.hadoop.util.Preconditions;\n-import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n\nReview Comment:\n   > ...  this makes the Nullable useless, ...\r\n   \r\n   Making it useless seems better than breaking the build.\r\n   \r\n   Unforturately, the the builds after this remain failing.\n\n\n\n", "szetszwo commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377457340\n\n   @slfan1989 , if it can fix the build, then it is fine.  But the builds after this remain failing.\n\n\n", "slfan1989 commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377471954\n\n   > @slfan1989 , if it can fix the build, then it is fine. But the builds after this remain failing.\r\n   \r\n   @szetszwo A new issue occurred during the compilation of yarn-ui. The log output is as follows:\r\n   \r\n   ```\r\n   [INFO] [2/4] Fetching packages...\r\n   [INFO] error color@5.0.2: The engine \"node\" is incompatible with this module. Expected version \">=18\". Got \"12.22.1\"\r\n   [INFO] error Found incompatible module.\r\n   ```\n\n\n", "slfan1989 commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377538969\n\n   > > @slfan1989 , if it can fix the build, then it is fine. But the builds after this remain failing.\r\n   > \r\n   > @szetszwo A new issue occurred during the compilation of yarn-ui. The log output is as follows:\r\n   > \r\n   > ```\r\n   > [INFO] [2/4] Fetching packages...\r\n   > [INFO] error color@5.0.2: The engine \"node\" is incompatible with this module. Expected version \">=18\". Got \"12.22.1\"\r\n   > [INFO] error Found incompatible module.\r\n   > ```\r\n   > \r\n   > I tried to apply a local fix for this issue.\r\n   \r\n   I manually specified `color@^3.1.3` in the package.json, and it took effect successfully. I will submit a PR to fix this issue.\r\n   \r\n   ```\r\n   [INFO] -", "slfan1989 merged PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015\n\n\n", "slfan1989 commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377550294\n\n   @szetszwo Thank you very much for the review! \n\n\n", "szetszwo commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377632827\n\n   @slfan1989 , thanks for fixing it!\n\n\n", "this is complicating the new thirdparty release FWIW. this should all be using the unshaded javax. Nullable/nonnull. And the hadoop-thirdparty release needs to address this stuff getting left out so 1.5.0 can be a drop-in replacement for 1.4.0\r\n{code}\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project hadoop-tos: Compilation failure: Compilation failure: \r\n[ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:[22,79] package org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:[89,29] cannot find symbol\r\n[ERROR]   symbol:   class Nullable\r\n[ERROR]   location: class org.apache.hadoop.fs.tosfs.util.Iterables\r\n[ERROR] -> [Help 1]\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project hadoop-azure: Compilation failure: Compilation failure: \r\n[ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java:[38,79] package org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java:[183,30] cannot find symbol\r\n[ERROR]   symbol: class Nullable\r\n[ERROR] -> [Help 1]\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project hadoop-hdfs-rbf: Compilation failure: Compilation failure: \r\n[ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[102,79] package org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol\r\n[ERROR]   symbol:   class NonNull\r\n[ERROR]   location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory\r\n[ERROR] -> [Help 1]\r\n[ERROR] \r\n{code}\r\n"], "labels": ["pull-request-available"], "summary": "In the recent build, we encountered the following issue: *org", "qna": [{"question": "What is the issue title?", "answer": "Resolve build error caused by missing Checker Framework (NonNull not recognized)"}, {"question": "Who reported this issue?", "answer": "Shilun Fan"}]}
{"key": "HADOOP-19716", "project": "HADOOP", "title": "Create lean docker image", "status": "Resolved", "reporter": "Attila Doroszlai", "created": "2025-10-06T09:52:36.000+0000", "description": "Create a new docker image based on the lean tarball.\r\n\r\nhadoop-3.4.2-lean.tar.gz", "comments": ["adoroszlai opened a new pull request, #8013:\nURL: https://github.com/apache/hadoop/pull/8013\n\n   ### Description of PR\r\n   \r\n   Create a new docker image based on `hadoop-3.4.2-lean.tar.gz`, which omits AWS `bundle-2.29.52.jar`.\r\n   \r\n   This PR should not be merged.  I will push it from CLI as a new branch to publish the image with Docker tag `3.4.2-lean`, rather than overwrite the existing image `3.4.2`.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   [Workflow run](https://github.com/adoroszlai/hadoop/actions/runs/18277349554/job/52032416240) in my fork created the [image](https://github.com/adoroszlai/hadoop/pkgs/container/hadoop/535792302?tag=3.4.2-lean).\n\n\n", "steveloughran commented on PR #8013:\nURL: https://github.com/apache/hadoop/pull/8013#issuecomment-3371636931\n\n   I did check the url resolved, BTW.\r\n   \r\n   Note that in #7980 packaging will change where we move hadoop-aws and hadoop azure to common/lib, with all dependencies except bundle.jar; that'll come iff you do a \"-Paws-sdk\" build. And the other cloud modules will come in if you explicitly ask for them.\r\n   \r\n   Still a WiP; hope to be done ASAP with hadoop 3.4.3 like this. No more \"let's strip the build\" work, instead just choose the build options for a release. \n\n\n", "adoroszlai commented on PR #8013:\nURL: https://github.com/apache/hadoop/pull/8013#issuecomment-3371660323\n\n   Thanks @steveloughran for the review.  Pushed 4cb319a9a98350bb0711029cf13c170f3c9ce043 to `docker-hadoop-3.4.2-lean`.\n\n\n", "adoroszlai closed pull request #8013: HADOOP-19716. Create lean docker image\nURL: https://github.com/apache/hadoop/pull/8013\n\n\n", "slfan1989 commented on PR #8013:\nURL: https://github.com/apache/hadoop/pull/8013#issuecomment-3375033744\n\n   > Thanks @steveloughran for the review. Pushed [4cb319a](https://github.com/apache/hadoop/commit/4cb319a9a98350bb0711029cf13c170f3c9ce043) to `docker-hadoop-3.4.2-lean`.\r\n   \r\n   @adoroszlai Thanks for the contribution! LGTM. \n\n\n"], "labels": ["pull-request-available"], "summary": "Create a new docker image based on the lean tarball", "qna": [{"question": "What is the issue title?", "answer": "Create lean docker image"}, {"question": "Who reported this issue?", "answer": "Attila Doroszlai"}]}
{"key": "HADOOP-19715", "project": "HADOOP", "title": "Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1", "status": "In Progress", "reporter": "Shilun Fan", "created": "2025-10-06T01:54:13.000+0000", "description": "The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}.", "comments": ["slfan1989 opened a new pull request, #8012:\nURL: https://github.com/apache/hadoop/pull/8012\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19715. Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   CI.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8012:\nURL: https://github.com/apache/hadoop/pull/8012#issuecomment-3379348657\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 43s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 25s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 25s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  mvnsite  |   0m 25s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  shadedclient  |   3m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 24s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   0m 25s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |   4m 41s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 28s |  |  ASF License check generated no output?  |\r\n   |  |   |  13m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8012 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux bcae851086b8 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f59839a3b410ba91d777948cc1b8683d10006e31 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/testReport/ |\r\n   | Max. process+thread count | 55 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8012:\nURL: https://github.com/apache/hadoop/pull/8012#issuecomment-3379885563\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   3m 52s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   1m 29s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   3m 18s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  mvnsite  |   1m 41s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |  13m 13s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   1m 44s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 39s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 39s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 24s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   0m 24s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |   4m  6s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 24s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  23m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8012 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 679e127e6ac4 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 67cae1f9eee8d0b5bb049e7b261a4e70c00d46a3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/testReport/ |\r\n   | Max. process+thread count | 106 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8012:\nURL: https://github.com/apache/hadoop/pull/8012#issuecomment-3383838383\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 42s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  59m 35s |  |  trunk passed  |\r\n   | -1 :x: |  compile  |  19m 32s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   9m 12s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  mvnsite  |   0m 39s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 43s | [/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 45s | [/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |  93m 50s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 21s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 21s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   0m  9s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  shadedclient  |   3m  9s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 13s |  |  ASF License check generated no output?  |\r\n   |  |   | 101m 41s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8012 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux cfb99cb39a9f 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 96e6841274b4e99041a4d0bc12af671c07a5d62f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/testReport/ |\r\n   | Max. process+thread count | 258 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "The project is currently using {{restrict-imports-enforcer-rule}} version {*}2", "qna": [{"question": "What is the issue title?", "answer": "Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1"}, {"question": "Who reported this issue?", "answer": "Shilun Fan"}]}
{"key": "HADOOP-19713", "project": "HADOOP", "title": "make container build work on macOS Tahoe", "status": "Open", "reporter": "Sangjin Lee", "created": "2025-10-02T23:15:57.000+0000", "description": "macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe.\r\n\r\nIt would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM.", "comments": ["This could be someone's good hack project.", "this means you can run linux in it? cool. puts it on a par with windows -though that has the advantage you can just dual boot the machine to linux or just replace windows entirely", "Correct. This is a container daemon/runtime that runs natively on Apple (Silicon), which does pretty much all the things that a Docker runtime would do without involving VMs. Also, I understand you can install this on macOS before Tahoe. Here's one article (among many out there): [https://www.infoq.com/news/2025/06/apple-container-linux/]", "after the NPM attack last month, I'm thinking I should do all builds which pull in remote artifacts in its own container, one with restricted access to the rest of the system lyes, complicates getting aws credentials, but that's part of what I want to lock down). ..."], "labels": [], "summary": "macOS Tahoe includes a native container daemon and runtime and it is supposed to", "qna": [{"question": "What is the issue title?", "answer": "make container build work on macOS Tahoe"}, {"question": "Who reported this issue?", "answer": "Sangjin Lee"}]}
{"key": "HADOOP-19712", "project": "HADOOP", "title": "S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()", "status": "Resolved", "reporter": "Steve Loughran", "created": "2025-10-01T15:29:06.000+0000", "description": "\r\nWe have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close();\r\n\r\n{code}\r\njdk.internal.misc.Unsafe.park(Native Method)\r\njava.util.concurrent.locks.LockSupport.park(LockSupport.java:341)\r\njava.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468)\r\njava.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:687)\r\njava.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:927)\r\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\njava.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)\r\norg.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap.entrySet(EvaluatingStatisticsMap.java:166)\r\njava.util.Collections$UnmodifiableMap.entrySet(Collections.java:1529)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.copyMap(IOStatisticsBinding.java:172)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:216)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:199)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.snapshot(IOStatisticsSnapshot.java:165)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.<init>(IOStatisticsSnapshot.java:125)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSupport.snapshotIOStatistics(IOStatisticsSupport.java:49)\r\n{code}\r\n\r\nthe code in question is calling `parallelStream()`, which uses a fixed pool of threads shared by all uses of the API\r\n{code}\r\n    Set<Entry<String, E>> r = evalEntries.parallelStream().map((e) ->\r\n        new EntryImpl<>(e.getKey(), e.getValue().apply(e.getKey())))\r\n        .collect(Collectors.toSet());\r\n{code}\r\n\r\nProposed: \r\n* move off parallelStream() to stream()\r\n* review code to if there is any other way this iteration can lead to a deadlock, e.g. the apply() calls.\r\n* could we do the merge more efficiently?\r\n\r\n", "comments": ["steveloughran opened a new pull request, #8006:\nURL: https://github.com/apache/hadoop/pull/8006\n\n   \r\n   Reworked how entrySet() and values() work, using .forEach() iterators after reviewing what ConcurrentHashMap does internally; it does a (safe) traverse.\r\n   \r\n   Add EvaluatingStatisticsMap.forEach() implementation which maps the passed in BiConsumer down to the evaluators.forEach, evaluating each value as it goes.\r\n   \r\n   Use that in IOStatisticsBinding.snapshot() code.\r\n   \r\n   Tests for all this.\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3357467095\n\n   tested s3 london args ` -Dparallel-tests -DtestsThreadCount=8 -Dscale`\r\n   ```\r\n   [ERROR] Failures: \r\n   [ERROR]   ITestS3APrefetchingInputStream.testReadLargeFileFully:130 [Maxiumum named action_executor_acquired.max] \r\n   Expecting:                                                                                                                                                                                                                                             \r\n    <0L>                                                                                                                                                                                                                                                  \r\n   to be greater than:                                                                                                                                                                                                                                    \r\n    <0L>           \r\n   ```\r\n   \n\n\n", "hadoop-yetus commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3358241968\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  55m 44s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  12m 48s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |  10m 53s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  7s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 58s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |  12m 27s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |  12m 27s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |  10m 39s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |  10m 39s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 42s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 57s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 15s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 223m 40s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8006 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 48973a6e0048 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 40e7c252a5e39fb8e483afe5f900412bf17cd4a3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/testReport/ |\r\n   | Max. process+thread count | 3134 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3365176292\n\n   build failures are in the yarn-ui; it complains that node is too old\r\n   ```\r\n   [INFO] -", "steveloughran commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3365209898\n\n   I see yarn-ui failure is already covered in a yarn jira.\n\n\n", "hadoop-yetus commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3365792744\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  52m 57s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  11m 35s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   9m 54s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  4s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m  0s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |  11m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |  11m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   9m 47s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   9m 47s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 42s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 5 new + 0 unchanged - 0 fixed = 5 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 58s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 38s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 56s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 17s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 214m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8006 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux bac1487e44d1 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 78880e82a6bb33e328b07c3273a73289ba5e717d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/testReport/ |\r\n   | Max. process+thread count | 1438 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3373592220\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m  3s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  54m 40s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  11m 34s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   9m 42s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  5s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 18s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 58s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |  11m 25s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |  11m 25s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   9m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   9m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 57s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 32s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 237m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8006 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 92090e4cf01b 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c3c15b84a4086d834b4fd9aa71b8078a2cb57b65 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/testReport/ |\r\n   | Max. process+thread count | 1449 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3403808610\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 13s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 19s |  |  trunk passed  |\r\n   | -1 :x: |  spotbugs  |   1m 36s | [/branch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  37m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  16m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 17s |  |  the patch passed  |\r\n   | -1 :x: |  spotbugs  |   1m 37s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 31s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 47s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  3s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 204m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8006 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d65a60138612 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 93fbc42bff57ec8dc97f2486fa6ba5ba82e31bdf |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/testReport/ |\r\n   | Max. process+thread count | 1474 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran merged PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006\n\n\n"], "labels": ["pull-request-available"], "summary": "\r\nWe have evidence that `IOStatisticsSupport", "qna": [{"question": "What is the issue title?", "answer": "S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19711", "project": "HADOOP", "title": "Upgrade hadoop3 docker scripts to 3.4.2", "status": "Resolved", "reporter": "Shilun Fan", "created": "2025-10-01T08:56:49.000+0000", "description": "The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2.", "comments": ["slfan1989 opened a new pull request, #8005:\nURL: https://github.com/apache/hadoop/pull/8005\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#issuecomment-3355515590\n\n   @jojochuang @adoroszlai @ayushtkn Hadoop 3.4.2 has been released, and we are preparing a corresponding Docker image for Hadoop 3.4.2. I have created this PR to complete the Docker image release. Could you please review this PR? Thank you very much!\n\n\n", "adoroszlai commented on code in PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#discussion_r2394132623\n\n\n##########\nDockerfile:\n##########\n@@ -14,7 +14,7 @@\n # limitations under the License.\n \n FROM apache/hadoop-runner\n-ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n+ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar\n\nReview Comment:\n   BTW do you know why the tarball is published without `.gz`?  It still seems to be gzipped:\r\n   \r\n   ```\r\n   $ file hadoop-3.4.2.tar\r\n   hadoop-3.4.2.tar: gzip compressed data, ...\r\n   ```\n\n\n\n", "slfan1989 commented on code in PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#discussion_r2396443322\n\n\n##########\nDockerfile:\n##########\n@@ -14,7 +14,7 @@\n # limitations under the License.\n \n FROM apache/hadoop-runner\n-ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n+ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar\n\nReview Comment:\n   Thank you very much for helping to review the code! I'm not sure why this package doesn't have the `.gz`\r\n   \r\n   @ahmarsuhail Could you please help take a look at this question? Thank you very much!\n\n\n\n", "slfan1989 commented on PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#issuecomment-3358754054\n\n   > Thanks @slfan1989 for the patch.\r\n   > \r\n   > ```\r\n   > $ docker run -it --rm ghcr.io/slfan1989/hadoop:3.4.2 hadoop version\r\n   > ...\r\n   > Hadoop 3.4.2\r\n   > Source code repository https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c\r\n   > Compiled by ahmarsu on 2025-08-20T10:30Z\r\n   > Compiled on platform linux-x86_64\r\n   > Compiled with protoc 3.23.4\r\n   > From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6\r\n   > This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.4.2.jar\r\n   > ```\r\n   > \r\n   > After this is merged, I suggest someone from Hadoop PMC upload the same image to Docker Hub, something like:\r\n   > \r\n   > ```\r\n   > docker pull ghcr.io/apache/hadoop:3.4.2\r\n   > docker tag ghcr.io/apache/hadoop:3.4.2 apache/hadoop:3.4.2\r\n   > docker push apache/hadoop:3.4.2\r\n   > ```\r\n   \r\n   @adoroszlai Thank you very much for the detailed explanation. However, I have never published a Docker image before, and pushing to Docker Hub should require some additional authentication information. @jojochuang  @ayushtkn , could you please take a look? Thank you very much!\r\n   \n\n\n", "ahmarsuhail commented on code in PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#discussion_r2398213215\n\n\n##########\nDockerfile:\n##########\n@@ -14,7 +14,7 @@\n # limitations under the License.\n \n FROM apache/hadoop-runner\n-ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n+ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar\n\nReview Comment:\n   Hey, sorry I think I made a mistake while uploading the tar to [the staging repo](https://dist.apache.org/repos/dist/dev/hadoop/3.4.2-RC3/), and the it got copied incorrectly to the release directory. can someone from the PMC please update the file name in the release directory? \r\n   \r\n   it is gzipped, just missing the `.gz` . My apologies for the miss. \n\n\n\n", "slfan1989 commented on code in PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#discussion_r2400690548\n\n\n##########\nDockerfile:\n##########\n@@ -14,7 +14,7 @@\n # limitations under the License.\n \n FROM apache/hadoop-runner\n-ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n+ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar\n\nReview Comment:\n   @ahmarsuhail Thank you for the information\u2014no need to apologize. I\u2019ll try adding the `.gz` extension. Thanks again for your contribution to the hadoop-3.4.2 release.\r\n   \n\n\n\n", "slfan1989 commented on code in PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#discussion_r2403881944\n\n\n##########\nDockerfile:\n##########\n@@ -14,7 +14,7 @@\n # limitations under the License.\n \n FROM apache/hadoop-runner\n-ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n+ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar\n\nReview Comment:\n   I\u2019ve already updated the [dist repo](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/ ), but the dlcdn hasn\u2019t synchronized yet. It may take a few more hours.\n\n\n\n", "slfan1989 commented on code in PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#discussion_r2403881944\n\n\n##########\nDockerfile:\n##########\n@@ -14,7 +14,7 @@\n # limitations under the License.\n \n FROM apache/hadoop-runner\n-ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n+ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar\n\nReview Comment:\n   I\u2019ve already updated the [dist repo](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/ ), but the dlcdn hasn\u2019t synchronized yet. It may take a few more hours.\r\n   \r\n   ```\r\n   ....\r\n   [hadoop-3.4.2.tar.gz](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz)\r\n   [hadoop-3.4.2.tar.gz.asc](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz.asc)\r\n   [hadoop-3.4.2.tar.gz.sha512](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz.sha512)\r\n   ....\r\n   ```\n\n\n\n", "slfan1989 merged PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005\n\n\n", "slfan1989 commented on PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#issuecomment-3369556514\n\n   @adoroszlai @ahmarsuhail Thank you very much for helping review the code!\n\n\n"], "labels": ["pull-request-available"], "summary": "The Hadoop 3", "qna": [{"question": "What is the issue title?", "answer": "Upgrade hadoop3 docker scripts to 3.4.2"}, {"question": "Who reported this issue?", "answer": "Shilun Fan"}]}
{"key": "HADOOP-19710", "project": "HADOOP", "title": "ABFS: Read Buffer Manager V2 should not be allowed untill implemented", "status": "Open", "reporter": "Anuj Modi", "created": "2025-09-29T08:59:21.000+0000", "description": "Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used.\r\nThis is to stop any user explicitly enabling the config to enable RBMV2.", "comments": ["anujmodi2021 opened a new pull request, #8002:\nURL: https://github.com/apache/hadoop/pull/8002\n\n   Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used.\r\n   This is to stop any user explicitly enabling the config to enable RBMV2.\r\n   \r\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19710\r\n   \n\n\n", "hadoop-yetus commented on PR #8002:\nURL: https://github.com/apache/hadoop/pull/8002#issuecomment-3346210665\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   9m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  30m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 20s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  90m  2s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8002 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7ed8c85f9284 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bc356262478fb82c786dc3bee7c312b2b1a29634 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/testReport/ |\r\n   | Max. process+thread count | 566 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 merged PR #8002:\nURL: https://github.com/apache/hadoop/pull/8002\n\n\n"], "labels": ["pull-request-available"], "summary": "Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used", "qna": [{"question": "What is the issue title?", "answer": "ABFS: Read Buffer Manager V2 should not be allowed untill implemented"}, {"question": "Who reported this issue?", "answer": "Anuj Modi"}]}
{"key": "HADOOP-19709", "project": "HADOOP", "title": "[JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default", "status": "Open", "reporter": "Vinayakumar B", "created": "2025-09-27T04:38:55.000+0000", "description": "Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.", "comments": ["vinayakumarb opened a new pull request, #8001:\nURL: https://github.com/apache/hadoop/pull/8001\n\n   This commit introduces support for Debian 12 (Bookworm) and Debian 13 (Trixie) as build platforms, following the approach established for Ubuntu 24.\r\n   \r\n   Key changes include:\r\n   - Creation of `Dockerfile_debian_12` and `Dockerfile_debian_13` based on `Dockerfile_ubuntu_24`, with appropriate base images and package resolver arguments.\r\n   - Updates to `dev-support/docker/pkg-resolver/packages.json` to include package definitions for `debian:12` and `debian:13`.\r\n   - Addition of `debian:12` and `debian:13` to `dev-support/docker/pkg-resolver/platforms.json`.\r\n   - Modification of `BUILDING.txt` to list `debian_12` and `debian_13` as supported OS platforms.\r\n   \n\n\n", "hadoop-yetus commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3341301544\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 38s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  37m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 53s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  98m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8001 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 01b5b1df2935 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7ccea846897ea6a8209a2238c06933afb4c489bc |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3342170970\n\n   Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly?\n\n\n", "slfan1989 commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3349642263\n\n   > Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly?\r\n   \r\n   @vinayakumarb Thank you very much for your contribution. However, I still have some concerns. Expanding support to many operating systems could be a rather heavy undertaking, since it requires us to pay closer attention to their EOL and version lifecycles. I'm wondering if it might be more sustainable to maintain a smaller subset of supported systems instead. If users have other requirements, they could always try customizing the build themselves.\r\n   \r\n   cc: @pan3793 @ayushtkn @cnauroth \n\n\n", "vinayakumarb commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3376374939\n\n   > > Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly?\r\n   > \r\n   > @vinayakumarb Thank you very much for your contribution. However, I still have some concerns. Expanding support to many operating systems could be a rather heavy undertaking, since it requires us to pay closer attention to their EOL and version lifecycles. I'm wondering if it might be more sustainable to maintain a smaller subset of supported systems instead. If users have other requirements, they could always try customizing the build themselves.\r\n   > \r\n   > cc: @pan3793 @ayushtkn @cnauroth\r\n   \r\n   I understand the concern.\r\n   Directly upgrading the debian:10 to debian:13 may break existing pipelines.\r\n   \r\n   However, having a Dockerfiles for various platforms provides the developers to build an environment as per their choice. It not necessarily means Hadoop binaries (jars and tar) are compiled in these.\r\n   \r\n   if users are interested in building Hadoop in their own choice of environment, these Dockerfiles will be a good starting point.\n\n\n", "slfan1989 commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3376469046\n\n   @vinayakumarb Thank you for the clarification \u2014 I agree (+1). However, given the complexity of operating system EOL management, I would carefully evaluate the introduction of Docker support for new systems in the future, considering both maintenance costs and long-term sustainability.\n\n\n", "hadoop-yetus commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3376715593\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  31m 43s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  1s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  40m 53s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/2/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 59s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 113m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8001 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 9e7987a7030b 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 92bd7478449829a0e7b987157945cfd72199e4ac |\r\n   | Max. process+thread count | 647 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/2/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on code in PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#discussion_r2412409276\n\n\n##########\ndev-support/docker/pkg-resolver/packages.json:\n##########\n@@ -263,6 +352,14 @@\n       \"openjdk-11-jdk\",\n       \"openjdk-17-jdk\"\n     ],\n+    \"debian:12\": [\n+      \"temurin-17-jdk\",\n+      \"temurin-24-jdk\"\n+    ],\n+    \"debian:13\": [\n+      \"temurin-17-jdk\",\n+      \"temurin-24-jdk\"\n\nReview Comment:\n   temurin-25 is out\n   \n   BTW, I think we should prefer to use the JDK provided by official APT repo if possible, Debian 13 already has `openjdk-25-jdk`\n\n\n\n", "pan3793 commented on code in PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#discussion_r2412409614\n\n\n##########\ndev-support/docker/pkg-resolver/packages.json:\n##########\n@@ -353,26 +472,34 @@\n   },\n   \"software-properties-common\": {\n     \"debian:11\": \"software-properties-common\",\n+    \n+\n\nReview Comment:\n   ?\n\n\n\n", "pan3793 commented on code in PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#discussion_r2412425091\n\n\n##########\ndev-support/docker/Dockerfile_debian_13:\n##########\n@@ -0,0 +1,110 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Dockerfile for installing the necessary dependencies for building Hadoop.\n+# See BUILDING.txt.\n+\n+FROM debian:13\n+\n+WORKDIR /root\n+\n+SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\n+\n+#####\n+# Disable suggests/recommends\n+#####\n+RUN echo 'APT::Install-Recommends \"0\";' > /etc/apt/apt.conf.d/10disableextras\n+RUN echo 'APT::Install-Suggests \"0\";' >>  /etc/apt/apt.conf.d/10disableextras\n+\n+ENV DEBIAN_FRONTEND=noninteractive\n+ENV DEBCONF_TERSE=true\n+\n+######\n+# Platform package dependency resolver\n+######\n+COPY pkg-resolver pkg-resolver\n+RUN chmod a+x pkg-resolver/*.sh pkg-resolver/*.py \\\n+    && chmod a+r pkg-resolver/*.json\n+\n+######\n+# Install packages from apt\n+######\n+# hadolint ignore=DL3008,SC2046\n+RUN apt-get -q update\n+RUN apt-get -q install -y --no-install-recommends wget apt-transport-https gpg gpg-agent gawk ca-certificates\n+RUN apt-get -q install -y --no-install-recommends python3\n+RUN echo \"deb https://packages.adoptium.net/artifactory/deb $(awk -F= '/^VERSION_CODENAME/{print$2}' /etc/os-release) main\" > /etc/apt/sources.list.d/adoptium.list\n+RUN wget -q -O - https://packages.adoptium.net/artifactory/api/gpg/key/public > /etc/apt/trusted.gpg.d/adoptium.asc\n+RUN apt-get -q update\n+RUN apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:13)\n+RUN apt-get clean\n+RUN update-java-alternatives -s temurin-17-jdk-amd64\n+RUN rm -rf /var/lib/apt/lists/*\n\nReview Comment:\n   each RUN produces one image layer, you should concat those shell commands by && instead\n\n\n\n", "pan3793 commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3379465889\n\n   @vinayakumarb, in addition to creating a dev container from the Dockerfile, have you verified that Hadoop can build successfully with native and frontend components in the created dev container?\n\n\n", "vinayakumarb commented on code in PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#discussion_r2422824903\n\n\n##########\ndev-support/docker/pkg-resolver/packages.json:\n##########\n@@ -263,6 +352,14 @@\n       \"openjdk-11-jdk\",\n       \"openjdk-17-jdk\"\n     ],\n+    \"debian:12\": [\n+      \"temurin-17-jdk\",\n+      \"temurin-24-jdk\"\n+    ],\n+    \"debian:13\": [\n+      \"temurin-17-jdk\",\n+      \"temurin-24-jdk\"\n\nReview Comment:\n   Done. Using openjdk-25-jdk in debian-13 and temurin-25-jdk in debian 12 as openjdk is not available in debian repository for bookworm.\n\n\n\n", "vinayakumarb commented on code in PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#discussion_r2422825814\n\n\n##########\ndev-support/docker/Dockerfile_debian_13:\n##########\n@@ -0,0 +1,110 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Dockerfile for installing the necessary dependencies for building Hadoop.\n+# See BUILDING.txt.\n+\n+FROM debian:13\n+\n+WORKDIR /root\n+\n+SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\n+\n+#####\n+# Disable suggests/recommends\n+#####\n+RUN echo 'APT::Install-Recommends \"0\";' > /etc/apt/apt.conf.d/10disableextras\n+RUN echo 'APT::Install-Suggests \"0\";' >>  /etc/apt/apt.conf.d/10disableextras\n+\n+ENV DEBIAN_FRONTEND=noninteractive\n+ENV DEBCONF_TERSE=true\n+\n+######\n+# Platform package dependency resolver\n+######\n+COPY pkg-resolver pkg-resolver\n+RUN chmod a+x pkg-resolver/*.sh pkg-resolver/*.py \\\n+    && chmod a+r pkg-resolver/*.json\n+\n+######\n+# Install packages from apt\n+######\n+# hadolint ignore=DL3008,SC2046\n+RUN apt-get -q update\n+RUN apt-get -q install -y --no-install-recommends wget apt-transport-https gpg gpg-agent gawk ca-certificates\n+RUN apt-get -q install -y --no-install-recommends python3\n+RUN echo \"deb https://packages.adoptium.net/artifactory/deb $(awk -F= '/^VERSION_CODENAME/{print$2}' /etc/os-release) main\" > /etc/apt/sources.list.d/adoptium.list\n+RUN wget -q -O - https://packages.adoptium.net/artifactory/api/gpg/key/public > /etc/apt/trusted.gpg.d/adoptium.asc\n+RUN apt-get -q update\n+RUN apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:13)\n+RUN apt-get clean\n+RUN update-java-alternatives -s temurin-17-jdk-amd64\n+RUN rm -rf /var/lib/apt/lists/*\n\nReview Comment:\n   Done.\n\n\n\n", "vinayakumarb commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393325468\n\n   > @vinayakumarb, in addition to creating a dev container from the Dockerfile, have you verified that Hadoop can build successfully with native and frontend components in the created dev container?\r\n   \r\n   Yes. I have verified building both native and frontend.\n\n\n", "vinayakumarb commented on code in PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#discussion_r2422836979\n\n\n##########\ndev-support/docker/pkg-resolver/packages.json:\n##########\n@@ -353,26 +472,34 @@\n   },\n   \"software-properties-common\": {\n     \"debian:11\": \"software-properties-common\",\n+    \n+\n\nReview Comment:\n   Forgot to remove empty lines. `software-properties-common` not available for debian 12 and 13. Also does not look like it was needed. \n\n\n\n", "hadoop-yetus commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393364407\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  1s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  24m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 24s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  61m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8001 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 5a71556ea849 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e371f08075c145585c0d620978733fceb30c93b0 |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393395506\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  11m 43s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  1s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  14m  3s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  hadolint  |   0m  1s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  40m 45s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8001 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 4e9b089f3372 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e371f08075c145585c0d620978733fceb30c93b0 |\r\n   | Max. process+thread count | 575 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/console |\r\n   | versions | git=2.30.2 maven=3.9.11 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393395990\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/console in case of problems.\r\n   \n\n\n", "hadoop-yetus commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393432943\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  1s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  19m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  hadolint  |   0m  2s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 24s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  49m 15s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8001 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 167189e0eac1 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e371f08075c145585c0d620978733fceb30c93b0 |\r\n   | Max. process+thread count | 568 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "vinayakumarb merged PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001\n\n\n"], "labels": ["pull-request-available"], "summary": "Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with J", "qna": [{"question": "What is the issue title?", "answer": "[JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default"}, {"question": "Who reported this issue?", "answer": "Vinayakumar B"}]}
{"key": "HADOOP-19708", "project": "HADOOP", "title": "volcano tos: disable shading when -DskipShade is set on a build", "status": "In Progress", "reporter": "Steve Loughran", "created": "2025-09-26T09:30:31.000+0000", "description": "hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside\r\n{code}\r\n 92K    share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n912K    share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar\r\n808K    share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar\r\n 36K    share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n 72K    share/hadoop/common/lib/hadoop-cos-3.5.0-20250916.124028-683.jar\r\n136K    share/hadoop/common/lib/hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n140K    share/hadoop/common/lib/hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n3.8M    share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar\r\n{code}\r\n\r\nOne thing it includes is yet-another mozilla/public-suffix-list.txt. These are a recurrent PITA and I don't want\r\nto find them surfacing again.\r\n{code}\r\n\r\n15. Required Resources\r\n======================\r\n\r\nresource: mozilla/public-suffix-list.txt\r\n       jar:file:/Users/stevel/Projects/Releases/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar!/mozilla/public-suffix-list.txt\r\n{code}\r\n\r\nPlan\r\n* Move the shade stage behind a profile; off for ASF releases. Exclude mozilla/public-suffix-list.txt  \r\n* Explicitly declare and manage httpclient5 dependency\r\n* hadoop-cloud-storage pom to include hadoop-tos but not dependencies in build, unless asked.\r\n* LICENSE-binary to declare optional redist of ve-tos-java-sdk-hadoop and its license.\r\n\r\n", "comments": ["doing this inside HADOOP-19696, as that's where I need the leaner artifacts", "(note the shading is troubled anyway\r\n{code}\r\n[INFO] Dependency-reduced POM written at: /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/dependency-reduced-pom.xml\r\n[WARNING] httpclient5-5.3.jar, httpcore5-5.2.4.jar, httpcore5-h2-5.2.4.jar define 3 overlapping resources: \r\n[WARNING]   - META-INF/DEPENDENCIES\r\n[WARNING]   - META-INF/LICENSE\r\n[WARNING]   - META-INF/NOTICE\r\n[WARNING] hadoop-tos-3.5.0-SNAPSHOT.jar, httpclient5-5.3.jar, httpcore5-5.2.4.jar, httpcore5-h2-5.2.4.jar, nimbus-jose-jwt-10.4.jar, ve-tos-java-sdk-hadoop-2.8.9.jar define 1 overlapping resource: \r\n[WARNING]   - META-INF/MANIFEST.MF\r\n[WARNING] maven-shade-plugin has detected that some files are\r\n[WARNING] present in two or more JARs. When this happens, only one\r\n[WARNING] single version of the file is copied to the uber jar.\r\n[WARNING] Usually this is not harmful and you can skip these warnings,\r\n[WARNING] otherwise try to manually exclude artifacts based on\r\n[WARNING] mvn dependency:tree -Ddetail=true and the above output.\r\n[WARNING] See https://maven.apache.org/plugins/maven-shade-plugin/\r\n[INFO] Replacing original artifact with shaded artifact.\r\n[INFO] Replacing /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/target/hadoop-tos-3.5.0-SNAPSHOT.jar with /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/target/hadoop-tos-3.5.0-SNAPSHOT-shaded.jar\r\n[INFO] \r\n[INFO] --- cyclonedx:2.9.1:makeBom (default) @ hadoop-tos ---\r\n{code}\r\n"], "labels": [], "summary": "hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M ", "qna": [{"question": "What is the issue title?", "answer": "volcano tos: disable shading when -DskipShade is set on a build"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19707", "project": "HADOOP", "title": "Surefire upgrade leads to increased report output, can cause Jenkins OOM", "status": "Resolved", "reporter": "Michael Smith", "created": "2025-09-25T23:37:18.000+0000", "description": "The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to\r\n{code}\r\nRecording test results\r\nERROR: Step \u2018Publish JUnit test result report\u2019 aborted due to exception: \r\njava.lang.OutOfMemoryError: Java heap space\r\n{code}\r\n\r\nCapturing stdout seems useful when doing ad-hoc testing or smaller test runs. I'd propose adding a profile to set enableOutErrElements=false for CI environments where that extra output can cause problems.", "comments": ["MikaelSmith opened a new pull request, #7998:\nURL: https://github.com/apache/hadoop/pull/7998\n\n   ### Description of PR\r\n   \r\n   Adds the `quiet-surefire` profile to set enableOutErrElements=false for maven-surefire-plugin. This restores the behavior prior to Surefire 3.3 that stdout/stderr are not included in the TEST-<package>.<class>.xml file for passing tests. The newer default behavior results in much larger TEST-*.xml files that can be a problem for CI tools processing them.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran `mvn clean test -Dtest=TestHttpServer` and `mvn clean test -Dtest=TestHttpServer -Pquiet-surefire` and compared size and contents of hadoop-common-project/hadoop-common/target/surefire-reports/TEST-org.apache.hadoop.http.TestHttpServer.xml.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7998:\nURL: https://github.com/apache/hadoop/pull/7998#issuecomment-3336586704\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  56m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  98m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 17s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 163m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7998 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 6c02dd609cd9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 895391e2c52eadd071997eacaaf7bb2f2af8be30 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran merged PR #7998:\nURL: https://github.com/apache/hadoop/pull/7998\n\n\n"], "labels": ["pull-request-available"], "summary": "The Surefire upgrade to 3", "qna": [{"question": "What is the issue title?", "answer": "Surefire upgrade leads to increased report output, can cause Jenkins OOM"}, {"question": "Who reported this issue?", "answer": "Michael Smith"}]}
{"key": "HADOOP-19706", "project": "HADOOP", "title": "Support Java Modularity", "status": "Open", "reporter": "Tsz-wo Sze", "created": "2025-09-24T20:03:42.000+0000", "description": "This is an umbrella JIRA for supporting Java 9 Modularity.", "comments": [], "labels": [], "summary": "This is an umbrella JIRA for supporting Java 9 Modularity", "qna": [{"question": "What is the issue title?", "answer": "Support Java Modularity"}, {"question": "Who reported this issue?", "answer": "Tsz-wo Sze"}]}
{"key": "HADOOP-19705", "project": "HADOOP", "title": "[JDK17] Do not use Long(long) and similar constructors", "status": "Open", "reporter": "Tsz-wo Sze", "created": "2025-09-24T19:00:19.000+0000", "description": "'Long(long)' is deprecated since version 9 and marked for removal.", "comments": [], "labels": [], "summary": "'Long(long)' is deprecated since version 9 and marked for removal", "qna": [{"question": "What is the issue title?", "answer": "[JDK17] Do not use Long(long) and similar constructors"}, {"question": "Who reported this issue?", "answer": "Tsz-wo Sze"}]}
{"key": "HADOOP-19703", "project": "HADOOP", "title": "UserGroupInformation.java is using a non-support operation in JDK25", "status": "Open", "reporter": "Hugo Costa", "created": "2025-09-24T13:42:31.000+0000", "description": "Hello,\r\n\r\nI'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally\r\n{code:java}\r\n\u00a0\u00a0\u00a0 java.lang.UnsupportedOperationException: getSubject is not supported\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3852)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3842)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3630)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.hadoop.util.HadoopOutputFile.fromPath(HadoopOutputFile.java:58)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriter.write(ParquetWriter.kt:75)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest$test \r\nwriting to parquet$1.invokeSuspend(ParquetWriterTest.kt:88)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt$runTest$2$1$1.invokeSuspend(TestBuilders.kt:318)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:101)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestDispatcher.processEvent$kotlinx_coroutines_test(TestDispatcher.kt:24)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at \r\nkotlinx.coroutines.test.TestCoroutineScheduler.tryRunNextTaskUnless$kotlinx_coroutines_test(TestCoroutineScheduler.kt:99)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt$runTest$2$1$workRunner$1.invokeSuspend(TestBuilders.kt:327)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:101)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:263)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:95)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:69)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:47)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersJvmKt.createTestResult(TestBuildersJvm.kt:10)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0(TestBuilders.kt:310)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0(TestBuilders.kt:168)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0$default(TestBuilders.kt:160)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0$default(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest.test writing to parquet(ParquetWriterTest.kt:76)\r\n {code}\r\nThe class making this unsupported call is UserGroupInformation, which is part of the common hadoop pkg - https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/3.4.2", "comments": ["Hadoop only supports Java 11.\r\nhttps://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\r\n\r\nThere is work in progress to support Java 17. Java 21 and 25 will be worked on later.\r\nHADOOP-19486", "Appreciate the link, thanks PJ, will keep a watch on that :)"], "labels": [], "summary": "Hello,\r\n\r\nI'm trying to upgrade my version of ParquetJava and I'm seeing the fol", "qna": [{"question": "What is the issue title?", "answer": "UserGroupInformation.java is using a non-support operation in JDK25"}, {"question": "Who reported this issue?", "answer": "Hugo Costa"}]}
{"key": "HADOOP-19702", "project": "HADOOP", "title": "Update non-thirdparty Guava version to  33.4.8-jre", "status": "Resolved", "reporter": "Istvan Toth", "created": "2025-09-23T17:28:42.000+0000", "description": "Keep in sync with recently upgraded thirdparty Guava", "comments": ["stoty opened a new pull request, #7994:\nURL: https://github.com/apache/hadoop/pull/7994\n\n   ### Description of PR\r\n   \r\n   Update non-thirdparty Guava version to 33.4.8-jre\r\n   \r\n   The motivation is the same as for updating the thirdparty one.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   CI\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7994:\nURL: https://github.com/apache/hadoop/pull/7994#issuecomment-3326537486\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  31m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  33m 33s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  9s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  9s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m  8s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 10s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |   1m 22s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 10s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  45m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7994 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 9c1032fe6d33 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bd6bdde979214be1291cda6a340e8e629a848d7a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/testReport/ |\r\n   | Max. process+thread count | 98 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7994:\nURL: https://github.com/apache/hadoop/pull/7994#issuecomment-3332389576\n\n   @stoty In my opinion, there is no issue with this PR. I have one question, why not upgrade to 33.5.0-jre?\r\n   \n\n\n", "stoty commented on PR #7994:\nURL: https://github.com/apache/hadoop/pull/7994#issuecomment-3332540273\n\n   > @stoty In my opinion, there is no issue with this PR. I have one question, why not upgrade to 33.5.0-jre?\r\n   \r\n   The hadoop-thirdparty guava is being updated to 33.4.8-jre, and I thought that it's easier to manage if we keep the unshaded version in sync with that, as long as we're able to.\r\n   \r\n   Also, we've already updated to 33.4.8-jre at my day job without issues, but I don't have experience yet with 33.5.0.\n\n\n", "slfan1989 commented on PR #7994:\nURL: https://github.com/apache/hadoop/pull/7994#issuecomment-3349644523\n\n   If there are no further comments, I will merge this PR today.\r\n   \n\n\n", "slfan1989 merged PR #7994:\nURL: https://github.com/apache/hadoop/pull/7994\n\n\n", "slfan1989 commented on PR #7994:\nURL: https://github.com/apache/hadoop/pull/7994#issuecomment-3364494335\n\n   @stoty Thanks for the contribution! Merged into trunk.\n\n\n"], "labels": ["pull-request-available"], "summary": "Keep in sync with recently upgraded thirdparty Guava", "qna": [{"question": "What is the issue title?", "answer": "Update non-thirdparty Guava version to  33.4.8-jre"}, {"question": "Who reported this issue?", "answer": "Istvan Toth"}]}
{"key": "HADOOP-19701", "project": "HADOOP", "title": "Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1", "status": "Resolved", "reporter": "Dongjoon Hyun", "created": "2025-09-23T04:55:31.000+0000", "description": null, "comments": ["dongjoon-hyun opened a new pull request, #7990:\nURL: https://github.com/apache/hadoop/pull/7990\n\n   \u2026\r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "dongjoon-hyun commented on PR #7990:\nURL: https://github.com/apache/hadoop/pull/7990#issuecomment-3322518878\n\n   Thank you, @slfan1989 .\n\n\n", "hadoop-yetus commented on PR #7990:\nURL: https://github.com/apache/hadoop/pull/7990#issuecomment-3325310193\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  60m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  20m 20s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m  2s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  11m 38s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   9m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 185m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  49m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  21m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  21m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m 49s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  18m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  21m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |  11m 53s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   9m  0s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  88m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 511m 54s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 880m 54s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.TestReconstructStripedFileWithRandomECPolicy |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7990 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 3a691ff72698 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f944112d4216160cc394f6ea7bd013f7b92796a9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/testReport/ |\r\n   | Max. process+thread count | 2369 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "dongjoon-hyun commented on PR #7990:\nURL: https://github.com/apache/hadoop/pull/7990#issuecomment-3325322984\n\n   All tests passes except `test4tests` and `unit` tests which checks new or revised test cases. So, I believe this PR is ready.\n\n\n", "slfan1989 merged PR #7990:\nURL: https://github.com/apache/hadoop/pull/7990\n\n\n", "slfan1989 commented on PR #7990:\nURL: https://github.com/apache/hadoop/pull/7990#issuecomment-3326078332\n\n   @dongjoon-hyun Thanks for the contribution! Merged into trunk.\n\n\n", "dongjoon-hyun commented on PR #7990:\nURL: https://github.com/apache/hadoop/pull/7990#issuecomment-3326405610\n\n   Thank you so much, @slfan1989 .\n\n\n"], "labels": ["pull-request-available"], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1"}, {"question": "Who reported this issue?", "answer": "Dongjoon Hyun"}]}
{"key": "HADOOP-19700", "project": "HADOOP", "title": "hadoop-thirdparty build to update maven plugin dependencies", "status": "Resolved", "reporter": "Steve Loughran", "created": "2025-09-22T13:05:13.000+0000", "description": "github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list.\r\n\r\ndependency checker 11+ \r\n\r\n{code}\r\nMandatory Upgrade Notice\r\nUpgrading to 10.0.2 or later is mandatory\r\n\r\nOlder versions of dependency-check are causing numerous, duplicative requests that end in processing failures are causing unnecassary load on the NVD API. Dependency-check 10.0.2 uses an updated User-Agent header that will allow the NVD to block calls from the older client.\r\n{code}\r\n\r\n----\r\n\r\nThe upgraded dependency checker now *requires* java11+, and *prefers* the provision of an API key for the national vulnerabilities database. It also skips the sonatype check as that no longer supports anonymous checks at all\r\n", "comments": ["10.x cannot parse the current DB files, even if it manages to download them, [~stevel@apache.org].", "latest pr does it, just needs java11 for that action. And I've turned off the sonatype checking "], "labels": ["pull-request-available"], "summary": "github action builds of PRs for hadoopHthirdparty fail because of throttling NVE", "qna": [{"question": "What is the issue title?", "answer": "hadoop-thirdparty build to update maven plugin dependencies"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19698", "project": "HADOOP", "title": "S3A Analytics-Accelerator: Update LICENSE-binary", "status": "Resolved", "reporter": "Ahmar Suhail", "created": "2025-09-18T10:46:22.000+0000", "description": "update LICENSE-binary to include AAL dependency\u00a0", "comments": ["ahmarsuhail opened a new pull request, #7982:\nURL: https://github.com/apache/hadoop/pull/7982\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Adds in AAL dependency to License-binary.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   not required.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "ahmarsuhail commented on PR #7982:\nURL: https://github.com/apache/hadoop/pull/7982#issuecomment-3306787342\n\n   @steveloughran PR to add in license binary.\n\n\n", "hadoop-yetus commented on PR #7982:\nURL: https://github.com/apache/hadoop/pull/7982#issuecomment-3307163564\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  46m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  90m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7982/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7982 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs |\r\n   | uname | Linux 8e1ed683c4d3 5.15.0-151-generic #161-Ubuntu SMP Tue Jul 22 14:25:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bf18e340d1dc826e0c031dc1e88e59a58fcb59d7 |\r\n   | Max. process+thread count | 530 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7982/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran merged PR #7982:\nURL: https://github.com/apache/hadoop/pull/7982\n\n\n"], "labels": ["pull-request-available"], "summary": "update LICENSE-binary to include AAL dependency\u00a0", "qna": [{"question": "What is the issue title?", "answer": "S3A Analytics-Accelerator: Update LICENSE-binary"}, {"question": "Who reported this issue?", "answer": "Ahmar Suhail"}]}
{"key": "HADOOP-19697", "project": "HADOOP", "title": "google gs connector registration failing", "status": "Open", "reporter": "Steve Loughran", "created": "2025-09-17T14:04:27.000+0000", "description": "Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath.\r\n\r\nThere's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema.\r\n\r\nAs well as a sign of a problem, it's better to just add an entry in core-default.xml as this saves all classloading overhead. It's what the other asf bundled ones do.", "comments": ["Trying to list local root  \r\n\r\nIf there are dependencies needed in the HADOOP-19696 let's make sure they get into common/lib, but this registration process mustn't fail this way, so let's just have a fs.gs.impl declararation in core-default.xml\r\n\r\n{code}\r\n bin/hadoop fs -ls file:///\r\n2025-09-17 15:03:47,688 [main] WARN  fs.FileSystem (FileSystem.java:loadFileSystems(3539)) - Cannot load filesystem\r\njava.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.gs.GoogleHadoopFileSystem Unable to get public no-arg constructor\r\n        at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\r\n        at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)\r\n        at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)\r\n        at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\r\n        at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\r\n        at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\r\n        at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3522)\r\n        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3562)\r\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\r\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\r\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:373)\r\n        at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:347)\r\n        at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:265)\r\n        at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:248)\r\n        at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:105)\r\n        at org.apache.hadoop.fs.shell.Command.run(Command.java:192)\r\n        at org.apache.hadoop.fs.FsShell.run(FsShell.java:327)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)\r\n        at org.apache.hadoop.fs.FsShell.main(FsShell.java:390)\r\nCaused by: java.lang.NoClassDefFoundError: com/google/auth/Credentials\r\n        at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)\r\n        at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373)\r\n        at java.base/java.lang.Class.getConstructor0(Class.java:3578)\r\n        at java.base/java.lang.Class.getConstructor(Class.java:2271)\r\n        at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:666)\r\n        at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:663)\r\n        at java.base/java.security.AccessController.doPrivileged(AccessController.java:569)\r\n        at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:674)\r\n        ... 20 more\r\nCaused by: java.lang.ClassNotFoundException: com.google.auth.Credentials\r\n        at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n        at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n        ... 28 more\r\nFound 20 items\r\n----------   1 root admin          0 2025-08-16 19:44 file:///.file\r\n...\r\n{code}\r\n", "Hi [~stevel@apache.org]. Not fully caught up on this one, but there is a {{fs.gs.impl}} entry in core-default.xml now. Was that all we needed?\r\n\r\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml#L4508"], "labels": [], "summary": "Surfaced during HADOOP-19696 and work with all the cloud connectors on the class", "qna": [{"question": "What is the issue title?", "answer": "google gs connector registration failing"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19696", "project": "HADOOP", "title": "hadoop binary distribution to move cloud connectors to hadoop common/lib", "status": "Open", "reporter": "Steve Loughran", "created": "2025-09-17T13:36:12.000+0000", "description": "Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed.\r\n\r\n* filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. \r\n* Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. \r\n\r\nThis adds a lot more stuff into the distribution, so I'm doing the following design\r\n* all hadoop-* modules in common/lib\r\n* minimal dependencies for hadoop-azure and hadoop-gcs (once we get those right!)\r\n* hadoop-aws: everything except bundle.jar\r\n* other connectors: only included with explicit profiles.\r\n\r\nASF releases will support azure out the box, the others once you add the dependencies. And anyone can build their own release with everything\r\n\r\nOne concern here, we make hadoop-cloud-storage artifact incomplete at pulling in things when depended on. We may need a separate module for the distro setup.\r\n\r\nNoticed during this that the hadoop-tos component is shaded and includes stuff (httpclient5) that we need under control. Filed HADOOP-19708 and incorporating here. \r\n\r\n\r\n", "comments": ["steveloughran opened a new pull request, #7980:\nURL: https://github.com/apache/hadoop/pull/7980\n\n   \r\n   \r\n   * new assembly for hadoop cloud storage\r\n   * hadoop-cloud-storage does the assembly on -Pdist\r\n   * layout stitching to move into share/hadoop/common/lib\r\n   * remove connectors from hadoop-tools-dist\r\n   * cut old jackson version from huawaei cloud dependency -even though it was being upgraded by our own artifacts, it was a complication.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Manual build, review, storediag, hadoop fs commands\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [=] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303127263\n\n   \r\n   * This puts the hadoop-azure, hadoop-aws &c binaries into common/lib and so on the classpath everywhere\r\n   * some problem with gcs instantiation during enum (will file later, as while it surfaces here, I think it's unrelated)\r\n   * my local builds end up (today) with some versioned jars as well as the -SNAPSHOT. I think this is from me tainting my maven repo, would like to see what others see\r\n   \r\n   ```\r\n   total 1401704\r\n   -rw-r--r--@ 1 stevel  staff     106151 Sep 17 12:57 aliyun-java-core-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff     194215 Sep 17 12:57 aliyun-java-sdk-core-4.5.10.jar\r\n   -rw-r--r--@ 1 stevel  staff     163698 Sep 17 12:57 aliyun-java-sdk-kms-2.11.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     220800 Sep 17 12:57 aliyun-java-sdk-ram-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     928456 Sep 17 12:57 aliyun-sdk-oss-3.18.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    2470776 Sep 17 12:57 analyticsaccelerator-s3-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      27006 Sep 17 13:11 aopalliance-repackaged-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      20891 Sep 17 13:11 audience-annotations-0.12.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     651391 Sep 17 13:11 avro-1.11.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     113966 Sep 17 12:57 azure-data-lake-store-sdk-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff      10288 Sep 17 12:57 azure-keyvault-core-1.0.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     815331 Sep 17 12:57 azure-storage-7.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    8324412 Sep 17 13:11 bcprov-jdk18on-1.78.1.jar\r\n   -rw-r--r--@ 1 stevel  staff  641534749 Sep 17 12:57 bundle-2.29.52.jar\r\n   -rw-r--r--@ 1 stevel  staff     223979 Sep 17 13:11 checker-qual-3.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      75479 Sep 17 13:11 commons-cli-1.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     353793 Sep 17 13:11 commons-codec-1.15.jar\r\n   -rw-r--r--@ 1 stevel  staff     751914 Sep 17 13:11 commons-collections4-4.4.jar\r\n   -rw-r--r--@ 1 stevel  staff    1079377 Sep 17 13:11 commons-compress-1.26.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     657516 Sep 17 13:11 commons-configuration2-2.10.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      24239 Sep 17 13:11 commons-daemon-1.0.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     508826 Sep 17 13:11 commons-io-2.16.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     673587 Sep 17 13:11 commons-lang3-3.17.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      70816 Sep 17 13:11 commons-logging-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    2213560 Sep 17 13:11 commons-math3-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     316431 Sep 17 13:11 commons-net-3.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     238400 Sep 17 13:11 commons-text-1.10.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    8661164 Sep 17 12:57 cos_api-bundle-5.6.19.jar\r\n   -rw-r--r--@ 1 stevel  staff    2983237 Sep 17 13:11 curator-client-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     336384 Sep 17 13:11 curator-framework-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     315569 Sep 17 13:11 curator-recipes-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     583996 Sep 17 13:11 dnsjava-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     324655 Sep 17 12:57 dom4j-2.1.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     670059 Sep 17 12:57 esdk-obs-java-3.20.4.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4617 Sep 17 13:11 failureaccess-1.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     249277 Sep 17 13:11 gson-2.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    3037368 Sep 17 13:11 guava-32.0.1-jre.jar\r\n   -rw-r--r--@ 1 stevel  staff      94013 Sep 17 12:57 hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      14456 Sep 17 13:11 hadoop-annotations-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     114335 Sep 17 13:11 hadoop-auth-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     930516 Sep 17 12:57 hadoop-aws-3.5.0-20250916.124028-686.jar\r\n   -rw-r--r--@ 1 stevel  staff     827349 Sep 17 12:57 hadoop-azure-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      33363 Sep 17 12:57 hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      70007 Sep 17 12:57 hadoop-cos-3.5.0-20250916.124028-683.jar\r\n   -rw-r--r--@ 1 stevel  staff     138447 Sep 17 12:57 hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     142274 Sep 17 12:57 hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff    3519516 Sep 17 13:11 hadoop-shaded-guava-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1952967 Sep 17 13:11 hadoop-shaded-protobuf_3_25-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    4019589 Sep 17 12:57 hadoop-tos-3.5.0-20250916.124028-202.jar\r\n   -rw-r--r--@ 1 stevel  staff     200223 Sep 17 13:11 hk2-api-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     203358 Sep 17 13:11 hk2-locator-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     131590 Sep 17 13:11 hk2-utils-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     780321 Sep 17 13:11 httpclient-4.5.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     328593 Sep 17 13:11 httpcore-4.4.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     102220 Sep 17 12:57 ini4j-0.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      29807 Sep 17 13:11 istack-commons-runtime-3.0.12.jar\r\n   -rw-r--r--@ 1 stevel  staff       9301 Sep 17 13:11 j2objc-annotations-2.8.jar\r\n   -rw-r--r--@ 1 stevel  staff      76636 Sep 17 13:11 jackson-annotations-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     473081 Sep 17 13:11 jackson-core-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff    1617187 Sep 17 13:11 jackson-databind-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      68453 Sep 17 13:11 jakarta.activation-1.2.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      44399 Sep 17 13:11 jakarta.activation-api-1.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      25058 Sep 17 13:11 jakarta.annotation-api-1.3.5.jar\r\n   -rw-r--r--@ 1 stevel  staff      18140 Sep 17 13:11 jakarta.inject-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      82973 Sep 17 13:11 jakarta.servlet-api-4.0.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      53683 Sep 17 13:11 jakarta.servlet.jsp-api-2.3.6.jar\r\n   -rw-r--r--@ 1 stevel  staff      91930 Sep 17 13:11 jakarta.validation-api-2.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     140376 Sep 17 13:11 jakarta.ws.rs-api-2.1.6.jar\r\n   -rw-r--r--@ 1 stevel  staff     115638 Sep 17 13:11 jakarta.xml.bind-api-2.3.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       7771 Sep 17 12:57 java-trace-api-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff      18432 Sep 17 12:57 java-xmlbuilder-1.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     794714 Sep 17 13:11 javassist-3.30.2-GA.jar\r\n   -rw-r--r--@ 1 stevel  staff      95806 Sep 17 13:11 javax.servlet-api-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1019097 Sep 17 13:11 jaxb-runtime-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff       4722 Sep 17 13:11 jcip-annotations-1.0-1.jar\r\n   -rw-r--r--@ 1 stevel  staff     327806 Sep 17 12:57 jdom2-2.0.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     311826 Sep 17 13:11 jersey-client-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff    1267957 Sep 17 13:11 jersey-common-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      32929 Sep 17 13:11 jersey-container-servlet-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      75742 Sep 17 13:11 jersey-container-servlet-core-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      80272 Sep 17 13:11 jersey-hk2-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff     964550 Sep 17 13:11 jersey-server-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      90184 Sep 17 12:57 jettison-1.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     249911 Sep 17 13:11 jetty-http-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     183011 Sep 17 13:11 jetty-io-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     118496 Sep 17 13:11 jetty-security-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     739348 Sep 17 13:11 jetty-server-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     146064 Sep 17 13:11 jetty-servlet-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     588962 Sep 17 13:11 jetty-util-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      66643 Sep 17 13:11 jetty-util-ajax-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     140308 Sep 17 13:11 jetty-webapp-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      68894 Sep 17 13:11 jetty-xml-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     282591 Sep 17 13:11 jsch-0.1.55.jar\r\n   -rw-r--r--@ 1 stevel  staff      19936 Sep 17 13:11 jsr305-3.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4519 Sep 17 13:11 jul-to-slf4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff     223129 Sep 17 13:11 kerb-core-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     115065 Sep 17 13:11 kerb-crypto-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      36361 Sep 17 13:11 kerb-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     100095 Sep 17 13:11 kerby-asn1-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      30190 Sep 17 13:11 kerby-config-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     200581 Sep 17 13:11 kerby-pkix-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      40787 Sep 17 13:11 kerby-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       2199 Sep 17 13:11 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar\r\n   -rw-r--r--@ 1 stevel  staff     136314 Sep 17 13:11 metrics-core-3.2.4.jar\r\n   -rw-r--r--@ 1 stevel  staff       4554 Sep 17 13:11 netty-all-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     339045 Sep 17 13:11 netty-buffer-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     355199 Sep 17 13:11 netty-codec-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      67192 Sep 17 13:11 netty-codec-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37789 Sep 17 13:11 netty-codec-haproxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     674362 Sep 17 13:11 netty-codec-http-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     490985 Sep 17 13:11 netty-codec-http2-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      44736 Sep 17 13:11 netty-codec-memcache-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     113699 Sep 17 13:11 netty-codec-mqtt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      46015 Sep 17 13:11 netty-codec-redis-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      21344 Sep 17 13:11 netty-codec-smtp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     121032 Sep 17 13:11 netty-codec-socks-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      34636 Sep 17 13:11 netty-codec-stomp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19823 Sep 17 13:11 netty-codec-xml-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     719225 Sep 17 13:11 netty-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     580162 Sep 17 13:11 netty-handler-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25650 Sep 17 13:11 netty-handler-proxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      26833 Sep 17 13:11 netty-handler-ssl-ocsp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37842 Sep 17 13:11 netty-resolver-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     188360 Sep 17 13:11 netty-resolver-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff       9145 Sep 17 13:11 netty-resolver-dns-classes-macos-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19825 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      19629 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff     521428 Sep 17 13:11 netty-transport-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     147621 Sep 17 13:11 netty-transport-classes-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     108558 Sep 17 13:11 netty-transport-classes-kqueue-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      42321 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      36594 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar\r\n   -rw-r--r--@ 1 stevel  staff      40644 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff       6193 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25741 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      25170 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      44157 Sep 17 13:11 netty-transport-native-unix-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      18241 Sep 17 13:11 netty-transport-rxtx-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      50814 Sep 17 13:11 netty-transport-sctp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      32189 Sep 17 13:11 netty-transport-udt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     779369 Sep 17 13:11 nimbus-jose-jwt-9.37.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     425763 Sep 17 12:57 okhttp-3.14.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      91980 Sep 17 12:57 okio-1.17.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     141734 Sep 17 12:57 opentelemetry-api-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      47252 Sep 17 12:57 opentelemetry-context-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      18189 Sep 17 12:57 opentracing-api-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      10542 Sep 17 12:57 opentracing-noop-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff       7504 Sep 17 12:57 opentracing-util-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     281989 Sep 17 12:57 org.jacoco.agent-0.8.5-runtime.jar\r\n   -rw-r--r--@ 1 stevel  staff      19479 Sep 17 13:11 osgi-resource-locator-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     128414 Sep 17 13:11 re2j-1.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      11369 Sep 17 12:57 reactive-streams-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     332398 Sep 17 13:11 reload4j-1.2.22.jar\r\n   -rw-r--r--@ 1 stevel  staff      41125 Sep 17 13:11 slf4j-api-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff       9824 Sep 17 13:11 slf4j-reload4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff    2112099 Sep 17 13:11 snappy-java-1.1.10.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     195909 Sep 17 13:11 stax2-api-4.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      72007 Sep 17 13:11 txw2-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff     443788 Sep 17 12:57 wildfly-openssl-2.1.4.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     522679 Sep 17 13:11 woodstox-core-5.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1323991 Sep 17 13:11 zookeeper-3.8.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     254932 Sep 17 13:11 zookeeper-jute-3.8.4.jar\r\n   \r\n   ```\r\n   \n\n\n", "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303133677\n\n   for contrast, here's 3.4.2\r\n   ```\r\n   ../hadoop-3.4.2/:\r\n   total 272\r\n   drwxr-xr-x@ 13 stevel  staff    416 Aug  7 12:58 bin\r\n   -rw-r--r--@  1 stevel  staff    824 Aug 27 16:58 binding.xml\r\n   drwxr-xr-x@  3 stevel  staff     96 Aug 25 17:09 downloads\r\n   drwxr-xr-x@  3 stevel  staff     96 Aug  7 12:16 etc\r\n   drwxr-xr-x@  7 stevel  staff    224 Aug  7 12:58 include\r\n   drwxr-xr-x@  3 stevel  staff     96 Aug  7 12:58 lib\r\n   drwxr-xr-x@ 14 stevel  staff    448 Aug  7 12:58 libexec\r\n   -rw-r--r--@  1 stevel  staff  23682 Aug  7 10:40 LICENSE-binary\r\n   -rw-r--r--@  1 stevel  staff  15791 Aug  7 10:39 LICENSE.txt\r\n   drwxr-xr-x@ 45 stevel  staff   1440 Aug  7 12:58 licenses-binary\r\n   -rw-r--r--@  1 stevel  staff  45514 Aug 27 17:12 log.txt\r\n   -rw-r--r--@  1 stevel  staff  27373 Aug  7 10:39 NOTICE-binary\r\n   -rw-r--r--@  1 stevel  staff   1541 Aug  7 10:39 NOTICE.txt\r\n   -rw-r--r--@  1 stevel  staff    175 Aug  7 10:39 README.txt\r\n   drwxr-xr-x@ 29 stevel  staff    928 Aug  7 12:16 sbin\r\n   -rw-r--r--@  1 stevel  staff    438 Aug 25 16:59 secrets.bin\r\n   drwxr-xr-x@  4 stevel  staff    128 Aug  7 13:23 share\r\n   -rw-r--r--@  1 stevel  staff    275 Aug 27 17:12 system.properties\r\n   \r\n   share/hadoop/common/lib:\r\n   total 1401704\r\n   -rw-r--r--@ 1 stevel  staff     106151 Sep 17 12:57 aliyun-java-core-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff     194215 Sep 17 12:57 aliyun-java-sdk-core-4.5.10.jar\r\n   -rw-r--r--@ 1 stevel  staff     163698 Sep 17 12:57 aliyun-java-sdk-kms-2.11.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     220800 Sep 17 12:57 aliyun-java-sdk-ram-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     928456 Sep 17 12:57 aliyun-sdk-oss-3.18.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    2470776 Sep 17 12:57 analyticsaccelerator-s3-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      27006 Sep 17 13:11 aopalliance-repackaged-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      20891 Sep 17 13:11 audience-annotations-0.12.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     651391 Sep 17 13:11 avro-1.11.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     113966 Sep 17 12:57 azure-data-lake-store-sdk-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff      10288 Sep 17 12:57 azure-keyvault-core-1.0.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     815331 Sep 17 12:57 azure-storage-7.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    8324412 Sep 17 13:11 bcprov-jdk18on-1.78.1.jar\r\n   -rw-r--r--@ 1 stevel  staff  641534749 Sep 17 12:57 bundle-2.29.52.jar\r\n   -rw-r--r--@ 1 stevel  staff     223979 Sep 17 13:11 checker-qual-3.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      75479 Sep 17 13:11 commons-cli-1.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     353793 Sep 17 13:11 commons-codec-1.15.jar\r\n   -rw-r--r--@ 1 stevel  staff     751914 Sep 17 13:11 commons-collections4-4.4.jar\r\n   -rw-r--r--@ 1 stevel  staff    1079377 Sep 17 13:11 commons-compress-1.26.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     657516 Sep 17 13:11 commons-configuration2-2.10.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      24239 Sep 17 13:11 commons-daemon-1.0.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     508826 Sep 17 13:11 commons-io-2.16.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     673587 Sep 17 13:11 commons-lang3-3.17.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      70816 Sep 17 13:11 commons-logging-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    2213560 Sep 17 13:11 commons-math3-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     316431 Sep 17 13:11 commons-net-3.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     238400 Sep 17 13:11 commons-text-1.10.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    8661164 Sep 17 12:57 cos_api-bundle-5.6.19.jar\r\n   -rw-r--r--@ 1 stevel  staff    2983237 Sep 17 13:11 curator-client-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     336384 Sep 17 13:11 curator-framework-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     315569 Sep 17 13:11 curator-recipes-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     583996 Sep 17 13:11 dnsjava-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     324655 Sep 17 12:57 dom4j-2.1.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     670059 Sep 17 12:57 esdk-obs-java-3.20.4.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4617 Sep 17 13:11 failureaccess-1.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     249277 Sep 17 13:11 gson-2.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    3037368 Sep 17 13:11 guava-32.0.1-jre.jar\r\n   -rw-r--r--@ 1 stevel  staff      94013 Sep 17 12:57 hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      14456 Sep 17 13:11 hadoop-annotations-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     114335 Sep 17 13:11 hadoop-auth-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     930516 Sep 17 12:57 hadoop-aws-3.5.0-20250916.124028-686.jar\r\n   -rw-r--r--@ 1 stevel  staff     827349 Sep 17 12:57 hadoop-azure-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      33363 Sep 17 12:57 hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      70007 Sep 17 12:57 hadoop-cos-3.5.0-20250916.124028-683.jar\r\n   -rw-r--r--@ 1 stevel  staff     138447 Sep 17 12:57 hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     142274 Sep 17 12:57 hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff    3519516 Sep 17 13:11 hadoop-shaded-guava-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1952967 Sep 17 13:11 hadoop-shaded-protobuf_3_25-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    4019589 Sep 17 12:57 hadoop-tos-3.5.0-20250916.124028-202.jar\r\n   -rw-r--r--@ 1 stevel  staff     200223 Sep 17 13:11 hk2-api-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     203358 Sep 17 13:11 hk2-locator-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     131590 Sep 17 13:11 hk2-utils-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     780321 Sep 17 13:11 httpclient-4.5.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     328593 Sep 17 13:11 httpcore-4.4.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     102220 Sep 17 12:57 ini4j-0.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      29807 Sep 17 13:11 istack-commons-runtime-3.0.12.jar\r\n   -rw-r--r--@ 1 stevel  staff       9301 Sep 17 13:11 j2objc-annotations-2.8.jar\r\n   -rw-r--r--@ 1 stevel  staff      76636 Sep 17 13:11 jackson-annotations-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     473081 Sep 17 13:11 jackson-core-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff    1617187 Sep 17 13:11 jackson-databind-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      68453 Sep 17 13:11 jakarta.activation-1.2.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      44399 Sep 17 13:11 jakarta.activation-api-1.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      25058 Sep 17 13:11 jakarta.annotation-api-1.3.5.jar\r\n   -rw-r--r--@ 1 stevel  staff      18140 Sep 17 13:11 jakarta.inject-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      82973 Sep 17 13:11 jakarta.servlet-api-4.0.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      53683 Sep 17 13:11 jakarta.servlet.jsp-api-2.3.6.jar\r\n   -rw-r--r--@ 1 stevel  staff      91930 Sep 17 13:11 jakarta.validation-api-2.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     140376 Sep 17 13:11 jakarta.ws.rs-api-2.1.6.jar\r\n   -rw-r--r--@ 1 stevel  staff     115638 Sep 17 13:11 jakarta.xml.bind-api-2.3.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       7771 Sep 17 12:57 java-trace-api-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff      18432 Sep 17 12:57 java-xmlbuilder-1.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     794714 Sep 17 13:11 javassist-3.30.2-GA.jar\r\n   -rw-r--r--@ 1 stevel  staff      95806 Sep 17 13:11 javax.servlet-api-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1019097 Sep 17 13:11 jaxb-runtime-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff       4722 Sep 17 13:11 jcip-annotations-1.0-1.jar\r\n   -rw-r--r--@ 1 stevel  staff     327806 Sep 17 12:57 jdom2-2.0.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     311826 Sep 17 13:11 jersey-client-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff    1267957 Sep 17 13:11 jersey-common-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      32929 Sep 17 13:11 jersey-container-servlet-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      75742 Sep 17 13:11 jersey-container-servlet-core-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      80272 Sep 17 13:11 jersey-hk2-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff     964550 Sep 17 13:11 jersey-server-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      90184 Sep 17 12:57 jettison-1.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     249911 Sep 17 13:11 jetty-http-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     183011 Sep 17 13:11 jetty-io-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     118496 Sep 17 13:11 jetty-security-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     739348 Sep 17 13:11 jetty-server-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     146064 Sep 17 13:11 jetty-servlet-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     588962 Sep 17 13:11 jetty-util-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      66643 Sep 17 13:11 jetty-util-ajax-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     140308 Sep 17 13:11 jetty-webapp-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      68894 Sep 17 13:11 jetty-xml-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     282591 Sep 17 13:11 jsch-0.1.55.jar\r\n   -rw-r--r--@ 1 stevel  staff      19936 Sep 17 13:11 jsr305-3.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4519 Sep 17 13:11 jul-to-slf4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff     223129 Sep 17 13:11 kerb-core-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     115065 Sep 17 13:11 kerb-crypto-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      36361 Sep 17 13:11 kerb-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     100095 Sep 17 13:11 kerby-asn1-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      30190 Sep 17 13:11 kerby-config-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     200581 Sep 17 13:11 kerby-pkix-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      40787 Sep 17 13:11 kerby-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       2199 Sep 17 13:11 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar\r\n   -rw-r--r--@ 1 stevel  staff     136314 Sep 17 13:11 metrics-core-3.2.4.jar\r\n   -rw-r--r--@ 1 stevel  staff       4554 Sep 17 13:11 netty-all-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     339045 Sep 17 13:11 netty-buffer-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     355199 Sep 17 13:11 netty-codec-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      67192 Sep 17 13:11 netty-codec-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37789 Sep 17 13:11 netty-codec-haproxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     674362 Sep 17 13:11 netty-codec-http-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     490985 Sep 17 13:11 netty-codec-http2-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      44736 Sep 17 13:11 netty-codec-memcache-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     113699 Sep 17 13:11 netty-codec-mqtt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      46015 Sep 17 13:11 netty-codec-redis-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      21344 Sep 17 13:11 netty-codec-smtp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     121032 Sep 17 13:11 netty-codec-socks-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      34636 Sep 17 13:11 netty-codec-stomp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19823 Sep 17 13:11 netty-codec-xml-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     719225 Sep 17 13:11 netty-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     580162 Sep 17 13:11 netty-handler-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25650 Sep 17 13:11 netty-handler-proxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      26833 Sep 17 13:11 netty-handler-ssl-ocsp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37842 Sep 17 13:11 netty-resolver-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     188360 Sep 17 13:11 netty-resolver-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff       9145 Sep 17 13:11 netty-resolver-dns-classes-macos-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19825 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      19629 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff     521428 Sep 17 13:11 netty-transport-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     147621 Sep 17 13:11 netty-transport-classes-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     108558 Sep 17 13:11 netty-transport-classes-kqueue-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      42321 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      36594 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar\r\n   -rw-r--r--@ 1 stevel  staff      40644 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff       6193 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25741 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      25170 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      44157 Sep 17 13:11 netty-transport-native-unix-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      18241 Sep 17 13:11 netty-transport-rxtx-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      50814 Sep 17 13:11 netty-transport-sctp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      32189 Sep 17 13:11 netty-transport-udt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     779369 Sep 17 13:11 nimbus-jose-jwt-9.37.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     425763 Sep 17 12:57 okhttp-3.14.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      91980 Sep 17 12:57 okio-1.17.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     141734 Sep 17 12:57 opentelemetry-api-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      47252 Sep 17 12:57 opentelemetry-context-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      18189 Sep 17 12:57 opentracing-api-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      10542 Sep 17 12:57 opentracing-noop-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff       7504 Sep 17 12:57 opentracing-util-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     281989 Sep 17 12:57 org.jacoco.agent-0.8.5-runtime.jar\r\n   -rw-r--r--@ 1 stevel  staff      19479 Sep 17 13:11 osgi-resource-locator-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     128414 Sep 17 13:11 re2j-1.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      11369 Sep 17 12:57 reactive-streams-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     332398 Sep 17 13:11 reload4j-1.2.22.jar\r\n   -rw-r--r--@ 1 stevel  staff      41125 Sep 17 13:11 slf4j-api-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff       9824 Sep 17 13:11 slf4j-reload4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff    2112099 Sep 17 13:11 snappy-java-1.1.10.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     195909 Sep 17 13:11 stax2-api-4.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      72007 Sep 17 13:11 txw2-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff     443788 Sep 17 12:57 wildfly-openssl-2.1.4.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     522679 Sep 17 13:11 woodstox-core-5.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1323991 Sep 17 13:11 zookeeper-3.8.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     254932 Sep 17 13:11 zookeeper-jute-3.8.4.jar\r\n   ```\r\n   \n\n\n", "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303139199\n\n   by contrast: 3.4.2\r\n   ```\r\n   total 98048\r\n   -rw-r--r--@ 1 stevel  staff     3448 Aug  7 12:16 animal-sniffer-annotations-1.17.jar\r\n   -rw-r--r--@ 1 stevel  staff    20891 Aug  7 12:16 audience-annotations-0.12.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   651391 Aug  7 12:16 avro-1.11.4.jar\r\n   -rw-r--r--@ 1 stevel  staff  8324412 Aug  7 12:15 bcprov-jdk18on-1.78.1.jar\r\n   -rw-r--r--@ 1 stevel  staff   193322 Aug  7 12:16 checker-qual-2.5.2.jar\r\n   -rw-r--r--@ 1 stevel  staff    75479 Aug  7 12:16 commons-cli-1.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   353793 Aug  7 12:16 commons-codec-1.15.jar\r\n   -rw-r--r--@ 1 stevel  staff   751914 Aug  7 12:16 commons-collections4-4.4.jar\r\n   -rw-r--r--@ 1 stevel  staff  1079377 Aug  7 12:16 commons-compress-1.26.1.jar\r\n   -rw-r--r--@ 1 stevel  staff   657516 Aug  7 12:16 commons-configuration2-2.10.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    24239 Aug  7 12:16 commons-daemon-1.0.13.jar\r\n   -rw-r--r--@ 1 stevel  staff   508826 Aug  7 12:16 commons-io-2.16.1.jar\r\n   -rw-r--r--@ 1 stevel  staff   673587 Aug  7 12:16 commons-lang3-3.17.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    70816 Aug  7 12:16 commons-logging-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff  2213560 Aug  7 12:16 commons-math3-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff   316431 Aug  7 12:16 commons-net-3.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   238400 Aug  7 12:16 commons-text-1.10.0.jar\r\n   -rw-r--r--@ 1 stevel  staff  2983237 Aug  7 12:16 curator-client-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   336384 Aug  7 12:16 curator-framework-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   315569 Aug  7 12:16 curator-recipes-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   583996 Aug  7 12:16 dnsjava-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     3727 Aug  7 12:16 failureaccess-1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   249277 Aug  7 12:16 gson-2.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff  2747878 Aug  7 12:16 guava-27.0-jre.jar\r\n   -rw-r--r--@ 1 stevel  staff    25517 Aug  7 12:16 hadoop-annotations-3.4.2.jar\r\n   -rw-r--r--@ 1 stevel  staff   110106 Aug  7 12:16 hadoop-auth-3.4.2.jar\r\n   -rw-r--r--@ 1 stevel  staff   810477 Aug  7 12:39 hadoop-azure-3.4.2.jar\r\n   -rw-r--r--@ 1 stevel  staff  3519516 Aug  7 12:16 hadoop-shaded-guava-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff  1952967 Aug  7 12:16 hadoop-shaded-protobuf_3_25-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   780321 Aug  7 12:16 httpclient-4.5.13.jar\r\n   -rw-r--r--@ 1 stevel  staff   328593 Aug  7 12:16 httpcore-4.4.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     8782 Aug  7 12:16 j2objc-annotations-1.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    75705 Aug  7 12:16 jackson-annotations-2.12.7.jar\r\n   -rw-r--r--@ 1 stevel  staff   365538 Aug  7 12:16 jackson-core-2.12.7.jar\r\n   -rw-r--r--@ 1 stevel  staff  1512418 Aug  7 12:16 jackson-databind-2.12.7.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    44399 Aug  7 12:16 jakarta.activation-api-1.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    95806 Aug  7 12:16 javax.servlet-api-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   102244 Aug  7 12:16 jaxb-api-2.2.11.jar\r\n   -rw-r--r--@ 1 stevel  staff   890168 Aug  7 12:16 jaxb-impl-2.2.3-1.jar\r\n   -rw-r--r--@ 1 stevel  staff     4722 Aug  7 12:16 jcip-annotations-1.0-1.jar\r\n   -rw-r--r--@ 1 stevel  staff   436731 Aug  7 12:16 jersey-core-1.19.4.jar\r\n   -rw-r--r--@ 1 stevel  staff   158890 Aug  7 12:16 jersey-json-1.22.0.jar\r\n   -rw-r--r--@ 1 stevel  staff   705276 Aug  7 12:16 jersey-server-1.19.4.jar\r\n   -rw-r--r--@ 1 stevel  staff   128990 Aug  7 12:16 jersey-servlet-1.19.4.jar\r\n   -rw-r--r--@ 1 stevel  staff    90184 Aug  7 12:16 jettison-1.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff   249911 Aug  7 12:16 jetty-http-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff   183011 Aug  7 12:16 jetty-io-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff   118496 Aug  7 12:16 jetty-security-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff   739348 Aug  7 12:16 jetty-server-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff   146064 Aug  7 12:16 jetty-servlet-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff   588962 Aug  7 12:16 jetty-util-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff    66643 Aug  7 12:16 jetty-util-ajax-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff   140308 Aug  7 12:16 jetty-webapp-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff    68894 Aug  7 12:16 jetty-xml-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff   282591 Aug  7 12:16 jsch-0.1.55.jar\r\n   -rw-r--r--@ 1 stevel  staff   100636 Aug  7 12:15 jsp-api-2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    19936 Aug  7 12:16 jsr305-3.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff    46367 Aug  7 12:16 jsr311-api-1.1.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     4519 Aug  7 12:16 jul-to-slf4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff   223129 Aug  7 12:16 kerb-core-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff   115065 Aug  7 12:16 kerb-crypto-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff    36361 Aug  7 12:16 kerb-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff   100095 Aug  7 12:16 kerby-asn1-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff    30190 Aug  7 12:16 kerby-config-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff   200581 Aug  7 12:16 kerby-pkix-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff    40787 Aug  7 12:16 kerby-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     2199 Aug  7 12:16 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar\r\n   -rw-r--r--@ 1 stevel  staff   136314 Aug  7 12:16 metrics-core-3.2.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     4554 Aug  7 12:15 netty-all-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   339045 Aug  7 12:16 netty-buffer-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   355199 Aug  7 12:16 netty-codec-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    67192 Aug  7 12:15 netty-codec-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    37789 Aug  7 12:15 netty-codec-haproxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   674362 Aug  7 12:15 netty-codec-http-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   490985 Aug  7 12:15 netty-codec-http2-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    44736 Aug  7 12:15 netty-codec-memcache-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   113699 Aug  7 12:15 netty-codec-mqtt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    46015 Aug  7 12:15 netty-codec-redis-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    21344 Aug  7 12:15 netty-codec-smtp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   121032 Aug  7 12:15 netty-codec-socks-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    34636 Aug  7 12:15 netty-codec-stomp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    19823 Aug  7 12:15 netty-codec-xml-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   719225 Aug  7 12:16 netty-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   580162 Aug  7 12:16 netty-handler-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    25650 Aug  7 12:15 netty-handler-proxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    26833 Aug  7 12:15 netty-handler-ssl-ocsp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    37842 Aug  7 12:16 netty-resolver-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   188360 Aug  7 12:15 netty-resolver-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     9145 Aug  7 12:15 netty-resolver-dns-classes-macos-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    19825 Aug  7 12:15 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff    19629 Aug  7 12:15 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff   521428 Aug  7 12:16 netty-transport-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   147621 Aug  7 12:16 netty-transport-classes-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   108558 Aug  7 12:15 netty-transport-classes-kqueue-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    42321 Aug  7 12:15 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff    36594 Aug  7 12:15 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar\r\n   -rw-r--r--@ 1 stevel  staff    40644 Aug  7 12:15 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff     6193 Aug  7 12:16 netty-transport-native-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    25741 Aug  7 12:15 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff    25170 Aug  7 12:15 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff    44157 Aug  7 12:16 netty-transport-native-unix-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    18241 Aug  7 12:15 netty-transport-rxtx-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    50814 Aug  7 12:15 netty-transport-sctp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff    32189 Aug  7 12:15 netty-transport-udt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff   779369 Aug  7 12:16 nimbus-jose-jwt-9.37.2.jar\r\n   -rw-r--r--@ 1 stevel  staff   128414 Aug  7 12:16 re2j-1.1.jar\r\n   -rw-r--r--@ 1 stevel  staff   332398 Aug  7 12:16 reload4j-1.2.22.jar\r\n   -rw-r--r--@ 1 stevel  staff    41125 Aug  7 12:15 slf4j-api-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff     9824 Aug  7 12:15 slf4j-reload4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff  2112099 Aug  7 12:16 snappy-java-1.1.10.4.jar\r\n   -rw-r--r--@ 1 stevel  staff   195909 Aug  7 12:16 stax2-api-4.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff   522679 Aug  7 12:16 woodstox-core-5.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff  1323991 Aug  7 12:16 zookeeper-3.8.4.jar\r\n   -rw-r--r--@ 1 stevel  staff   254932 Aug  7 12:16 zookeeper-jute-3.8.4.jar\r\n   ```\r\n   \n\n\n", "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303435683\n\n   Having audited the files coming off the cloud connectors, we have about a dozen whose licenses aren't in the binary\r\n   \r\n   ```\r\n   analyticsaccelerator-s3-1.3.0.jar\r\n   cos_api-bundle-5.6.19.jar\r\n   dom4j-2.1.4.jar\r\n   esdk-obs-java-3.20.4.2.jar\r\n   java-trace-api-0.2.11-beta.jar\r\n   java-xmlbuilder-1.2.jar\r\n   opentracing-api-0.33.0.jar\r\n   opentracing-noop-0.33.0.jar\r\n   opentracing-util-0.33.0.jar\r\n   reactive-streams-1.0.3.jar\r\n   ve-tos-java-sdk-hadoop-2.8.9.jar\r\n   ```\r\n   \r\n   the analyticsaccelerator is @ahmarsuhail 's work to add to the license, not sure about the others. \r\n   \r\n   Proposed: identify which connector the unacknowledged artifacts are coming from, create homework for each team. \r\n   \n\n\n", "hadoop-yetus commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303936232\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  13m  4s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  43m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m 16s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 53s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 43s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 35s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 58s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 51s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   2m 47s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 41s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 39s |  |  hadoop-assemblies in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 41s |  |  hadoop-tools-dist in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 44s |  |  hadoop-huaweicloud in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 41s |  |  hadoop-cloud-storage in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 214m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7980 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint |\r\n   | uname | Linux 604f3bcd050e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c59e35149ea17b7cea37be9203a265dcbff118fe |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/1/testReport/ |\r\n   | Max. process+thread count | 548 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-assemblies hadoop-tools/hadoop-tools-dist hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-cloud-storage U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "ahmarsuhail commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3311933628\n\n   not very familiar with how the packaging stuff works, so finding this a bit difficult to review. \r\n   \r\n   How are testing the packaging, I just ran `mvn package -Pdist -DskipTests -Dmaven.javadoc.skip=true  -DskipShade`, but the outputs in :\r\n   \r\n   `hadoop-cloud-storage-project/hadoop-cloud-storage/target/hadoop-cloud-storage-3.5.0-SNAPSHOT/share/hadoop/common/lib`, \r\n   \r\n   `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/tools/lib`\r\n   \r\n   `hadoop/hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib` \r\n   \r\n   \r\n   are all the same before and after your changes, so I must be doing something wrong.\n\n\n", "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3312222391\n\n   did you do a `mvn clean package`?\r\n   \r\n   `hadoop-cloud-storage-project/hadoop-cloud-storage/target/hadoop-cloud-storage-3.5.0-SNAPSHOT/share/hadoop/common/lib`  -new, contains all cloud stuff we want in; should cut stuff already going to be there just to reduce copying\r\n   \r\n   `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/tools/lib`\r\n   should remove hadoop-azure, hadoop-aws, hadoop-gcs, bundle.jar...\r\n   \r\n   the big distro created under `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/` is what is shipped.\r\n   \n\n\n", "hadoop-yetus commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3327784078\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m 12s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 27s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m  7s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 50s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 11s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  18m  3s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 35s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 25s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 56s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  18m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   9m  9s | [/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 10 new + 5526 unchanged - 10 fixed = 5536 total (was 5536)  |\r\n   | -1 :x: |  javadoc  |   7m 34s | [/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/artifact/out/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 10 new + 1418 unchanged - 10 fixed = 1428 total (was 1428)  |\r\n   | -1 :x: |  shadedclient  |  19m 38s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 800m  7s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 46s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1046m 13s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7980 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint |\r\n   | uname | Linux 271d5e8aaf6a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0aaa6ce66e8a8fc7fc04fa4e2650badf956d531a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/testReport/ |\r\n   | Max. process+thread count | 4938 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-assemblies hadoop-tools/hadoop-tools-dist hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-cloud-storage . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "I've just discovered quite how many things google-cloud-storage jar pulls in if you don't build a shaded release.\r\n\r\nNowhere as big as the aws sdk, but it it is still significant.\r\n\r\n# I'm going to exclude hadoop-gcp dependencies by default in a build, so if you build hadoop distro with -DskipShade you don't get these in common-lib unless you have a -Dhadoop-gcp-package. \r\n# Most of these aren't in our published LICENSE-binary file. some are, but not all.\r\n# the opentelemetry/census artifacts are newer than those from one of the other projects; build both and you get conflict (joy!).\r\n# a protobuf 2.5 comes in from somewhere\r\n\r\nI think for now I'd say \"don't make issues 2-3 blockers on merging the PR\" because they're independent. But ideally the gcs imports should be tuned down and we should go for consistent opentelemetry/opencensus versions wherever imported.\r\n\r\n\r\n{code}\r\n3.0K animal-sniffer-annotations-1.24.jar\r\n3.0K annotations-4.1.1.4.jar\r\n 49K api-common-2.47.2.jar\r\n7.3K auto-value-annotations-1.11.0.jar\r\n232K checker-qual-3.49.0.jar\r\n4.3M conscrypt-openjdk-uber-2.5.2.jar\r\n 18K detector-resources-support-0.33.0.jar\r\n 19K error_prone_annotations-2.36.0.jar\r\n 39K exporter-metrics-0.33.0.jar\r\n4.6K failureaccess-1.0.2.jar\r\n 52K gapic-google-cloud-storage-v2-2.52.0.jar\r\n424K gax-2.64.2.jar\r\n154K gax-grpc-2.64.2.jar\r\n162K gax-httpjson-2.64.2.jar\r\n295K google-api-client-2.7.2.jar\r\n252K google-api-services-storage-v1-rev20250420-2.0.0.jar\r\n8.2K google-auth-library-credentials-1.33.1.jar\r\n294K google-auth-library-oauth2-http-1.33.1.jar\r\n137K google-cloud-core-2.54.2.jar\r\n 16K google-cloud-core-grpc-2.54.2.jar\r\n 15K google-cloud-core-http-2.54.2.jar\r\n249K google-cloud-monitoring-3.52.0.jar\r\n1.3M google-cloud-storage-2.52.0.jar\r\n289K google-http-client-1.46.3.jar\r\n 11K google-http-client-apache-v2-1.46.3.jar\r\n 19K google-http-client-appengine-1.46.3.jar\r\n 13K google-http-client-gson-1.46.3.jar\r\n9.4K google-http-client-jackson2-1.46.3.jar\r\n 80K google-oauth-client-1.37.0.jar\r\n316K grpc-alts-1.70.0.jar\r\n316K grpc-api-1.70.0.jar\r\n 14K grpc-auth-1.70.0.jar\r\n293B grpc-context-1.70.0.jar\r\n639K grpc-core-1.70.0.jar\r\n 30K grpc-google-cloud-storage-v2-2.52.0.jar\r\n 15K grpc-googleapis-1.70.0.jar\r\n175K grpc-grpclb-1.70.0.jar\r\n 39K grpc-inprocess-1.70.0.jar\r\n9.3M grpc-netty-shaded-1.70.0.jar\r\n 67K grpc-opentelemetry-1.70.0.jar\r\n5.2K grpc-protobuf-1.70.0.jar\r\n7.7K grpc-protobuf-lite-1.70.0.jar\r\n248K grpc-rls-1.70.0.jar\r\n928K grpc-services-1.70.0.jar\r\n 59K grpc-stub-1.70.0.jar\r\n 98K grpc-util-1.70.0.jar\r\n9.4M grpc-xds-1.70.0.jar\r\n243K gson-2.9.0.jar\r\n2.9M guava-33.4.8-jre.jar\r\n 12K j2objc-annotations-3.0.0.jar\r\n462K jackson-core-2.14.3.jar\r\n 26K javax.annotation-api-1.3.2.jar\r\n3.7K jspecify-1.0.0.jar\r\n 19K jsr305-3.0.2.jar\r\n2.1K listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar\r\n347K opencensus-api-0.31.1.jar\r\n 23K opencensus-contrib-http-util-0.31.1.jar\r\n155K opentelemetry-api-1.47.0.jar\r\n 48K opentelemetry-context-1.47.0.jar\r\n8.1K opentelemetry-gcp-resources-1.37.0-alpha.jar\r\n6.6K opentelemetry-sdk-1.47.0.jar\r\n 54K opentelemetry-sdk-common-1.47.0.jar\r\n 20K opentelemetry-sdk-extension-autoconfigure-spi-1.47.0.jar\r\n 53K opentelemetry-sdk-logs-1.47.0.jar\r\n322K opentelemetry-sdk-metrics-1.47.0.jar\r\n129K opentelemetry-sdk-trace-1.47.0.jar\r\n 73K opentelemetry-semconv-1.29.0-alpha.jar\r\n6.8K perfmark-api-0.27.0.jar\r\n1.9M proto-google-cloud-monitoring-v3-3.52.0.jar\r\n980K proto-google-cloud-storage-v2-2.52.0.jar\r\n2.6M proto-google-common-protos-2.55.2.jar\r\n182K proto-google-iam-v1-1.50.2.jar\r\n521K protobuf-java-2.5.0.jar\r\n 71K protobuf-java-util-3.25.5.jar\r\n125K re2j-1.1.jar\r\n 91K shared-resourcemapping-0.33.0.jar\r\n 40K slf4j-api-1.7.36.jar\r\n503K threetenbp-1.7.0.jar\r\n{code}\r\n", "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3387174385\n\n   Latest build generates stack traces from gcs and obs filesystem incomplete CP in service loader. Both need to move to core-default.xml *only* which is faster anyway.\r\n   \r\n   ```\r\n   \r\n   2025-10-09 20:06:44,452 [main] WARN  fs.FileSystem (FileSystem.java:loadFileSystems(3539)) - Cannot load filesystem\r\n   java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.gs.GoogleHadoopFileSystem could not be instantiated    \r\n           at java.util.ServiceLoader.fail(ServiceLoader.java:232)                                                                                            \r\n           at java.util.ServiceLoader.access$100(ServiceLoader.java:185)                                                                                      \r\n           at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)                                                                        \r\n           at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)                                                                               \r\n           at java.util.ServiceLoader$1.next(ServiceLoader.java:480)                                                                                          \r\n           at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3525)                                                                           \r\n           at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3562)                                                                        \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.probeForFileSystemClass(StoreDiag.java:671)                                                           \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.run(StoreDiag.java:223)                                                                               \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.run(StoreDiag.java:170)                                                                               \r\n           at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)                                                                                       \r\n           at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)                                                                                       \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.exec(StoreDiag.java:1255)                                                                             \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.main(StoreDiag.java:1264)                                                                             \r\n           at storediag.main(storediag.java:25)                                                                                                               \r\n           at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                     \r\n           at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)                                                                   \r\n           at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)                                                           \r\n           at java.lang.reflect.Method.invoke(Method.java:498)                                                                                                \r\n           at org.apache.hadoop.util.RunJar.run(RunJar.java:333)                                                                                              \r\n           at org.apache.hadoop.util.RunJar.main(RunJar.java:254)                                                                                             \r\n   Caused by: java.lang.NoClassDefFoundError: com/google/auth/Credentials                                                                                     \r\n           at java.lang.Class.getDeclaredConstructors0(Native Method)                                                                                         \r\n           at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)                                                                                 \r\n           at java.lang.Class.getConstructor0(Class.java:3075)                                                                                                \r\n           at java.lang.Class.newInstance(Class.java:412)                                                                                                     \r\n           at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)                                                                        \r\n           ... 18 more                                                                                                                                        \r\n   Caused by: java.lang.ClassNotFoundException: com.google.auth.Credentials                                                                                   \r\n           at java.net.URLClassLoader.findClass(URLClassLoader.java:387)                                                                                      \r\n           at java.lang.ClassLoader.loadClass(ClassLoader.java:419)                                                                                           \r\n           at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)                                                                                   \r\n           at java.lang.ClassLoader.loadClass(ClassLoader.java:352)                                                                                           \r\n           ... 23 more                                                                                                                                        \r\n   2025-10-09 20:06:44,456 [main] WARN  fs.FileSystem (FileSystem.java:loadFileSystems(3539)) - Cannot load filesystem                                        \r\n   java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.obs.OBSFileSystem could not be instantiated            \r\n           at java.util.ServiceLoader.fail(ServiceLoader.java:232)                                                                                            \r\n           at java.util.ServiceLoader.access$100(ServiceLoader.java:185)                                                                                      \r\n           at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)                                                                        \r\n           at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)                                                                               \r\n           at java.util.ServiceLoader$1.next(ServiceLoader.java:480)                                                                                          \r\n           at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3525)                                                                           \r\n           at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3562)                                                                        \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.probeForFileSystemClass(StoreDiag.java:671)                                                           \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.run(StoreDiag.java:223)                                                                               \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.run(StoreDiag.java:170)                                                                               \r\n           at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)                                                                                       \r\n           at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)                                                                                       \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.exec(StoreDiag.java:1255)                                                                             \r\n           at org.apache.hadoop.fs.store.diag.StoreDiag.main(StoreDiag.java:1264)                                                                             \r\n           at storediag.main(storediag.java:25)                                                                                                               \r\n           at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                     \r\n           at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)                                                                   \r\n           at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)                                                           \r\n           at java.lang.reflect.Method.invoke(Method.java:498)                                                                                                \r\n           at org.apache.hadoop.util.RunJar.run(RunJar.java:333)                                                                                              \r\n           at org.apache.hadoop.util.RunJar.main(RunJar.java:254)                                                                                             \r\n   Caused by: java.lang.NoClassDefFoundError: com/obs/services/exception/ObsException                                                                         \r\n           at java.lang.Class.getDeclaredConstructors0(Native Method)                                                                                         \r\n           at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)                                                                                 \r\n           at java.lang.Class.getConstructor0(Class.java:3075)                                                                                                \r\n           at java.lang.Class.newInstance(Class.java:412)                                                                                                     \r\n           at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)                                                                        \r\n           ... 18 more                                                                                                                                        \r\n   Caused by: java.lang.ClassNotFoundException: com.obs.services.exception.ObsException                                                                       \r\n           at java.net.URLClassLoader.findClass(URLClassLoader.java:387)                                                                                      \r\n           at java.lang.ClassLoader.loadClass(ClassLoader.java:419)                                                                                           \r\n           at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)                                                                                   \r\n           at java.lang.ClassLoader.loadClass(ClassLoader.java:352)                                                                                           \r\n           ... 23 more                                                                                                                                        \r\n   FileSystem for s3a:// is: org.apache.hadoop.fs.s3a.S3AFileSystem                                                                                           \r\n   Loaded from: file:/Users/stevel/Projects/Releases/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/hadoop-aws-3.5.0-SNAPSHOT.jar via sun.misc.Launcher$AppClassLoader@41906a77                                                                                                                                           \r\n   ```\r\n   \n\n\n", "hadoop-yetus commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3387715974\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  9s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m  3s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m  3s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m  6s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  21m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m  0s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 29s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  49m 55s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 40s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  41m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  18m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   9m 58s | [/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 10 new + 5526 unchanged - 10 fixed = 5536 total (was 5536)  |\r\n   | -1 :x: |  javadoc  |   8m  2s | [/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/artifact/out/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 10 new + 1418 unchanged - 10 fixed = 1428 total (was 1428)  |\r\n   | +1 :green_heart: |  shadedclient  |  63m 31s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 22s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 32s |  |  ASF License check generated no output?  |\r\n   |  |   | 309m 40s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7980 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint |\r\n   | uname | Linux 01b725e22dc7 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 76b4eae78d738ee73cf68118a87c9204d120d752 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/testReport/ |\r\n   | Max. process+thread count | 699 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-assemblies hadoop-tools/hadoop-tools-dist hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-tos hadoop-cloud-storage-project/hadoop-cloud-storage hadoop-cloud-storage-project/hadoop-cloud-storage-dist hadoop-cloud-storage-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/console |\r\n   | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "and full jars if you pull in everything. aws bundle.jar dominates; the rest adds up. \r\n{code}\r\n104K aliyun-java-core-0.2.11-beta.jar\r\n190K aliyun-java-sdk-core-4.5.10.jar\r\n160K aliyun-java-sdk-kms-2.11.0.jar\r\n216K aliyun-java-sdk-ram-3.1.0.jar\r\n907K aliyun-sdk-oss-3.18.1.jar\r\n2.4M analyticsaccelerator-s3-1.3.0.jar\r\n3.0K animal-sniffer-annotations-1.24.jar\r\n3.0K annotations-4.1.1.4.jar\r\n 49K api-common-2.47.2.jar\r\n7.3K auto-value-annotations-1.11.0.jar\r\n111K azure-data-lake-store-sdk-2.3.9.jar\r\n 10K azure-keyvault-core-1.0.0.jar\r\n796K azure-storage-7.0.1.jar\r\n612M bundle-2.29.52.jar\r\n232K checker-qual-3.49.0.jar\r\n346K commons-codec-1.15.jar\r\n 69K commons-logging-1.3.0.jar\r\n4.3M conscrypt-openjdk-uber-2.5.2.jar\r\n8.3M cos_api-bundle-5.6.19.jar\r\n 18K detector-resources-support-0.33.0.jar\r\n317K dom4j-2.1.4.jar\r\n 19K error_prone_annotations-2.36.0.jar\r\n 39K exporter-metrics-0.33.0.jar\r\n4.6K failureaccess-1.0.2.jar\r\n 52K gapic-google-cloud-storage-v2-2.52.0.jar\r\n424K gax-2.64.2.jar\r\n154K gax-grpc-2.64.2.jar\r\n162K gax-httpjson-2.64.2.jar\r\n295K google-api-client-2.7.2.jar\r\n252K google-api-services-storage-v1-rev20250420-2.0.0.jar\r\n8.2K google-auth-library-credentials-1.33.1.jar\r\n294K google-auth-library-oauth2-http-1.33.1.jar\r\n137K google-cloud-core-2.54.2.jar\r\n 16K google-cloud-core-grpc-2.54.2.jar\r\n 15K google-cloud-core-http-2.54.2.jar\r\n249K google-cloud-monitoring-3.52.0.jar\r\n1.3M google-cloud-storage-2.52.0.jar\r\n289K google-http-client-1.46.3.jar\r\n 11K google-http-client-apache-v2-1.46.3.jar\r\n 19K google-http-client-appengine-1.46.3.jar\r\n 13K google-http-client-gson-1.46.3.jar\r\n9.4K google-http-client-jackson2-1.46.3.jar\r\n 80K google-oauth-client-1.37.0.jar\r\n316K grpc-alts-1.70.0.jar\r\n316K grpc-api-1.70.0.jar\r\n 14K grpc-auth-1.70.0.jar\r\n293B grpc-context-1.70.0.jar\r\n639K grpc-core-1.70.0.jar\r\n 30K grpc-google-cloud-storage-v2-2.52.0.jar\r\n 15K grpc-googleapis-1.70.0.jar\r\n175K grpc-grpclb-1.70.0.jar\r\n 39K grpc-inprocess-1.70.0.jar\r\n9.3M grpc-netty-shaded-1.70.0.jar\r\n 67K grpc-opentelemetry-1.70.0.jar\r\n5.2K grpc-protobuf-1.70.0.jar\r\n7.7K grpc-protobuf-lite-1.70.0.jar\r\n248K grpc-rls-1.70.0.jar\r\n928K grpc-services-1.70.0.jar\r\n 59K grpc-stub-1.70.0.jar\r\n 98K grpc-util-1.70.0.jar\r\n9.4M grpc-xds-1.70.0.jar\r\n243K gson-2.9.0.jar\r\n2.9M guava-33.4.8-jre.jar\r\n 92K hadoop-aliyun-3.5.0-SNAPSHOT.jar\r\n910K hadoop-aws-3.5.0-SNAPSHOT.jar\r\n810K hadoop-azure-3.5.0-SNAPSHOT.jar\r\n 33K hadoop-azure-datalake-3.5.0-SNAPSHOT.jar\r\n 68K hadoop-cos-3.5.0-SNAPSHOT.jar\r\n135K hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n142K hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n250K hadoop-tos-3.5.0-SNAPSHOT.jar\r\n762K httpclient-4.5.13.jar\r\n933K httpclient5-5.5.jar\r\n321K httpcore-4.4.13.jar\r\n888K httpcore5-5.3.6.jar\r\n236K httpcore5-h2-5.3.4.jar\r\n100K ini4j-0.5.4.jar\r\n 12K j2objc-annotations-3.0.0.jar\r\n462K jackson-core-2.14.3.jar\r\n7.6K java-trace-api-0.2.11-beta.jar\r\n 26K javax.annotation-api-1.3.2.jar\r\n320K jdom2-2.0.6.1.jar\r\n 88K jettison-1.5.4.jar\r\n575K jetty-util-9.4.57.v20241219.jar\r\n 65K jetty-util-ajax-9.4.57.v20241219.jar\r\n3.7K jspecify-1.0.0.jar\r\n 19K jsr305-3.0.2.jar\r\n2.1K listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar\r\n347K opencensus-api-0.31.1.jar\r\n 23K opencensus-contrib-http-util-0.31.1.jar\r\n155K opentelemetry-api-1.47.0.jar\r\n 48K opentelemetry-context-1.47.0.jar\r\n8.1K opentelemetry-gcp-resources-1.37.0-alpha.jar\r\n6.6K opentelemetry-sdk-1.47.0.jar\r\n 54K opentelemetry-sdk-common-1.47.0.jar\r\n 20K opentelemetry-sdk-extension-autoconfigure-spi-1.47.0.jar\r\n 53K opentelemetry-sdk-logs-1.47.0.jar\r\n322K opentelemetry-sdk-metrics-1.47.0.jar\r\n129K opentelemetry-sdk-trace-1.47.0.jar\r\n 73K opentelemetry-semconv-1.29.0-alpha.jar\r\n275K org.jacoco.agent-0.8.5-runtime.jar\r\n6.8K perfmark-api-0.27.0.jar\r\n1.9M proto-google-cloud-monitoring-v3-3.52.0.jar\r\n980K proto-google-cloud-storage-v2-2.52.0.jar\r\n2.6M proto-google-common-protos-2.55.2.jar\r\n182K proto-google-iam-v1-1.50.2.jar\r\n521K protobuf-java-2.5.0.jar\r\n 71K protobuf-java-util-3.25.5.jar\r\n125K re2j-1.1.jar\r\n 11K reactive-streams-1.0.3.jar\r\n 91K shared-resourcemapping-0.33.0.jar\r\n 40K slf4j-api-1.7.36.jar\r\n503K threetenbp-1.7.0.jar\r\n980K ve-tos-java-sdk-hadoop-2.8.9.jar\r\n433K wildfly-openssl-2.1.4.Final.jar\r\n{code}\r\n\r\nThe default settings will produce something a lot leaner\r\n{code}\r\n2.4M analyticsaccelerator-s3-1.3.0.jar\r\n 10K azure-keyvault-core-1.0.0.jar\r\n796K azure-storage-7.0.1.jar\r\n346K commons-codec-1.15.jar\r\n 69K commons-logging-1.3.0.jar\r\n910K hadoop-aws-3.5.0-SNAPSHOT.jar\r\n810K hadoop-azure-3.5.0-SNAPSHOT.jar\r\n 33K hadoop-azure-datalake-3.5.0-SNAPSHOT.jar\r\n 68K hadoop-cos-3.5.0-SNAPSHOT.jar\r\n135K hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n142K hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n250K hadoop-tos-3.5.0-SNAPSHOT.jar\r\n762K httpclient-4.5.13.jar\r\n321K httpcore-4.4.13.jar\r\n575K jetty-util-9.4.57.v20241219.jar\r\n 65K jetty-util-ajax-9.4.57.v20241219.jar\r\n433K wildfly-openssl-2.1.4.Final.jar\r\n{code}\r\n", "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3402138222\n\n   shaded test failures\r\n   * some in yarn which are presumably unrelated...let's see\r\n   * lots of more skipped tests in hadoop-azure, hadoop-azuredatalake, such as `TestAbfsInputStreamStatistics` which is skipping  .... Looks like maven is back to running these tests and skipping where they don't have the credentials\n\n\n", "hadoop-yetus commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3405523026\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 30s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 15s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  35m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 40s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |  11m  2s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 44s |  |  trunk passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  branch/hadoop-assemblies no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |   1m 19s | [/branch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 32s | [/branch-spotbugs-hadoop-tools_hadoop-gcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-tools_hadoop-gcp.txt) |  hadoop-gcp in trunk failed.  |\r\n   | +0 :ok: |  spotbugs  |   0m 23s |  |  branch/hadoop-tools/hadoop-tools-dist no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |   0m 32s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt) |  hadoop-huaweicloud in trunk failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 37s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in trunk failed.  |\r\n   | +0 :ok: |  spotbugs  |   0m 23s |  |  branch/hadoop-cloud-storage-project/hadoop-cloud-storage no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |   0m 31s | [/branch-spotbugs-hadoop-cloud-storage-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-cloud-storage-project.txt) |  hadoop-cloud-storage-project in trunk failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 30s | [/branch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  30m 50s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  43m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  17m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 39s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   7m 36s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   9m 41s | [/results-javadoc-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/results-javadoc-javadoc-root.txt) |  root generated 30 new + 42985 unchanged - 30 fixed = 43015 total (was 43015)  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-assemblies has no data from spotbugs  |\r\n   | -1 :x: |  spotbugs  |   1m 19s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-tools/hadoop-tools-dist has no data from spotbugs  |\r\n   | -1 :x: |  spotbugs  |   0m 33s | [/patch-spotbugs-hadoop-tools_hadoop-gcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-tools_hadoop-gcp.txt) |  hadoop-gcp in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 32s | [/patch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt) |  hadoop-huaweicloud in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 36s | [/patch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in the patch failed.  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-cloud-storage-project/hadoop-cloud-storage has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 23s |  |  hadoop-cloud-storage-project/hadoop-cloud-storage-dist has no data from spotbugs  |\r\n   | -1 :x: |  spotbugs  |   0m 31s | [/patch-spotbugs-hadoop-cloud-storage-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-cloud-storage-project.txt) |  hadoop-cloud-storage-project in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 30s | [/patch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  31m 14s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 801m 10s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   1m 45s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   | 1072m 30s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.yarn.service.TestYarnNativeServices |\r\n   |   | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.fs.tosfs.object.TestObjectOutputStream |\r\n   |   | hadoop.fs.tosfs.commit.TestMagicOutputStream |\r\n   |   | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider |\r\n   |   | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain |\r\n   |   | hadoop.fs.tosfs.object.TestObjectRangeInputStream |\r\n   |   | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7980 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle |\r\n   | uname | Linux 209e4e5576fc 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0254bb430e1623793f30744d7b32a37202f2d586 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/testReport/ |\r\n   | Max. process+thread count | 3665 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-assemblies hadoop-common-project/hadoop-common hadoop-tools/hadoop-tools-dist hadoop-tools/hadoop-gcp hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-tos hadoop-cloud-storage-project/hadoop-cloud-storage hadoop-cloud-storage-project/hadoop-cloud-storage-dist hadoop-cloud-storage-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/console |\r\n   | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "cnauroth commented on code in PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#discussion_r2446383325\n\n\n##########\nhadoop-cloud-storage-project/pom.xml:\n##########\n@@ -34,6 +34,7 @@\n     <module>hadoop-cos</module>\n     <module>hadoop-huaweicloud</module>\n     <module>hadoop-tos</module>\n+    <module>hadoop-cloud-storage-dist</module>\n\nReview Comment:\n   It seems like we would never need to enter execution of `hadoop-cloud-storage-dist` unless we are building a distro (activating `-Pdist`). Should we also wrap inclusion of the sub-module here behind activation of the `dist` profile?\n\n\n\n##########\nBUILDING.txt:\n##########\n@@ -388,6 +388,58 @@ Create a local staging version of the website (in /tmp/hadoop-site)\n \n Note that the site needs to be built in a second pass after other artifacts.\n \n+----------------------------------------------------------------------------------\n+Including Cloud Connector Dependencies in Distributions:\n+\n+Hadoop distributions include the hadoop modules need to work with data and services\n\nReview Comment:\n   Nitpick: \"modules needed\".\n\n\n\n", "steveloughran commented on code in PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#discussion_r2447670994\n\n\n##########\nhadoop-cloud-storage-project/pom.xml:\n##########\n@@ -34,6 +34,7 @@\n     <module>hadoop-cos</module>\n     <module>hadoop-huaweicloud</module>\n     <module>hadoop-tos</module>\n+    <module>hadoop-cloud-storage-dist</module>\n\nReview Comment:\n   valid point. Will do, as it'll save on disk space as well as time.\n\n\n\n", "hadoop-yetus commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3425948823\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 16s |  |  https://github.com/apache/hadoop/pull/7980 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7980 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/10/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "cnauroth commented on code in PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#discussion_r2449857037\n\n\n##########\nBUILDING.txt:\n##########\n@@ -388,6 +388,57 @@ Create a local staging version of the website (in /tmp/hadoop-site)\n \n Note that the site needs to be built in a second pass after other artifacts.\n \n+----------------------------------------------------------------------------------\n+Including Cloud Connector Dependencies in Distributions:\n+\n+Hadoop distributions include the hadoop modules needed to work with data and services\n+on cloud infrastructure\n+\n+However, dependencies are omitted for all cloud connectors except hadoop-azure\n+(abfs:// and wasb://) and possibly hadoop-gcp (gs://) and hadoop-tos (tos://).\n+For the latter two modules, it depends on shading options.\n+\n+For hadoop-aws the AWS SDK bundle.jar is omitted, but everything else is included.\n+\n+Excluding the extra binaries:\n+* Keeps release artifact size below the limit of the ASF distribution network.\n+* Reduces download and size overhead in docker usage.\n+* Reduces the CVE attack surface and audit-related complaints about those same ScVES.\n\nReview Comment:\n   Nitpick: \"CVEs.\"\n\n\n\n", "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3516563186\n\n   @cnauroth do you see anything else which needs to go in here? I think I'm done at least as a merge to trunk goes.\n\n\n", "steveloughran merged PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980\n\n\n", "steveloughran opened a new pull request, #8094:\nURL: https://github.com/apache/hadoop/pull/8094\n\n   \r\n   \r\n   This moves all the cloud connector libraries to common/lib There are specific build options to control which libraries to include The hadoop-* JARs of the modules are includes, but dependencies are only included when the build-time options specify it.\r\n   \r\n     Available package profiles:\r\n       hadoop-aliyun-package\r\n       hadoop-aws-package\r\n       hadoop-azure-datalake-package\r\n       hadoop-cos-package\r\n       hadoop-huaweicloud-package\r\n   \r\n   This means that by default AWS bundle.jar is no longer included in the distribution: to add it users must drop their chosen version of the SDK into share/hadoop/common/lib\r\n   \r\n   Anyone building their own release now has a choice of which connectors to bundle. The ASF ones will stay fairly lean to reduce the CVE attack surface as well as keep package size under control.\r\n   \r\n   This is the branch-3.4 variant which cuts out connector that are not present (tos, gcp).\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Manual builds; another in progress.\r\n   \r\n   LICENSE-binary validated by looking at dependencie of hadoop-cloud-storage, making sure the needed ones were there and deleting some which didn't appear any more. \r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran commented on code in PR #8094:\nURL: https://github.com/apache/hadoop/pull/8094#discussion_r2543140022\n\n\n##########\nLICENSE-binary:\n##########\n@@ -536,3 +549,8 @@ Public Domain\n -------------\n \n aopalliance:aopalliance:1.0\n+\n\nReview Comment:\n   cut\n\n\n\n##########\nhadoop-cloud-storage-project/hadoop-cloud-storage-dist/pom.xml:\n##########\n@@ -0,0 +1,281 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+ Licensed under the Apache License, Version 2.0 (the \"License\");\n+ you may not use this file except in compliance with the License.\n+ You may obtain a copy of the License at\n+\n+   https://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing, software\n+ distributed under the License is distributed on an \"AS IS\" BASIS,\n+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ See the License for the specific language governing permissions and\n+ limitations under the License. See accompanying LICENSE file.\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.hadoop</groupId>\n+    <artifactId>hadoop-project</artifactId>\n+    <version>3.4.3-SNAPSHOT</version>\n+    <relativePath>../../hadoop-project</relativePath>\n+  </parent>\n+  <artifactId>hadoop-cloud-storage-dist</artifactId>\n+  <version>3.4.3-SNAPSHOT</version>\n+  <packaging>jar</packaging>\n+\n+  <description>Apache Hadoop Cloud Storage Distribution</description>\n+  <name>Apache Hadoop Cloud Storage Distribution</name>\n+\n+  <!--\n+  This pulls in all the artifacts to copy into common/lib and so put into\n+  the Hadoop distro and onto the classpath.\n+\n+  The assembly file /hadoop-assemblies/src/main/resources/assemblies/hadoop-cloud-storage.xml\n+  is processed to define the layout and to add extra files alongside\n+  the Jars.\n+\n+  By default, while hadoop-* artifacts are all included, dependencies\n+  are omitted for all cloud connectors except hadoop-azure and\n+  possibly hadoop-gcp and hadoop-tos modules.\n+  For hadoop-aws the AWS SDK bundle.jar omitted, but everything else is included.\n+\n+   * This keeps binary release size below the limit of apache distributions\n+   * Reduces download and size overhead in docker usage.\n+   * Reduces the CVE attack surface\n+   * Reduces the risk of classpath conflict.\n+\n+  To produce a build with the specific desired dependencies, the build must be executed\n+  with the relevant profile of ${module}-package.\n+\n+  For example, a build with the hadoop-aws and hadoop-azure-datalake dependencies,\n+  build with -Dhadoop-aws-package -Dhadoop-azure-datalake-package\n+\n+  Available package profiles:\n+    hadoop-aws-package\n\nReview Comment:\n   restore hadoop-aliyun-package docs\n\n\n\n##########\nBUILDING.txt:\n##########\n@@ -385,6 +385,49 @@ Create a local staging version of the website (in /tmp/hadoop-site)\n \n Note that the site needs to be built in a second pass after other artifacts.\n \n+----------------------------------------------------------------------------------\n+Including Cloud Connector Dependencies in Distributions:\n+\n+Hadoop distributions include the hadoop modules needed to work with data and services\n+on cloud infrastructure\n+\n+However, dependencies are omitted for all cloud connectors except hadoop-azure\n+(abfs:// and wasb://) and possibly hadoop-gcp (gs://) and hadoop-tos (tos://).\n+For the latter two modules, it depends on shading options.\n+\n+For hadoop-aws the AWS SDK bundle.jar is omitted, but everything else is included.\n+\n+Excluding the extra binaries:\n+* Keeps release artifact size below the limit of the ASF distribution network.\n+* Reduces download and size overhead in docker usage.\n+* Reduces the CVE attack surface and audit-related complaints about those same CVEs.\n+* Reduces the risk of classpath conflict.\n+\n+To produce a build with the specific desired dependencies, the build must be executed\n+with the relevant profile of ${module}-package alongside the -Pdist profile.\n+\n+For example, a build with the hadoop-aws and hadoop-azure-datalake dependencies,\n+run with\n+\n+ mvn package -Pdist -DskipTests -Dhadoop-aws-package -Dhadoop-azure-datalake-package\n+\n+Available package profiles:\n+  hadoop-aws-package\n\nReview Comment:\n   restore hadoop-aliyun-package \n\n\n\n", "hadoop-yetus commented on PR #8094:\nURL: https://github.com/apache/hadoop/pull/8094#issuecomment-3555833003\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 58s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   2m 12s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  21m 55s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |   9m  5s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m  0s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 56s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  14m 38s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   4m 47s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 39s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 13s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 11s |  |  branch/hadoop-assemblies no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |  16m 43s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8094/1/artifact/out/branch-spotbugs-root-warnings.html) |  root in branch-3.4 has 1 extant spotbugs warnings.  |\r\n   | +0 :ok: |  spotbugs  |   0m 15s |  |  branch/hadoop-tools/hadoop-tools-dist no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  18m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 23s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  18m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 45s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 12s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   8m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  11m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   4m 42s | [/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8094/1/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 10 new + 5675 unchanged - 10 fixed = 5685 total (was 5685)  |\r\n   | -1 :x: |  javadoc  |   4m 40s | [/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8094/1/artifact/out/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 10 new + 1429 unchanged - 10 fixed = 1439 total (was 1439)  |\r\n   | +0 :ok: |  spotbugs  |   0m 14s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 13s |  |  hadoop-assemblies has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 14s |  |  hadoop-tools/hadoop-tools-dist has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 16s |  |  hadoop-cloud-storage-project/hadoop-cloud-storage-dist has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  18m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 634m 43s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8094/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 53s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8094/1/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   | 845m 29s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.gridmix.TestSleepJob |\r\n   |   | hadoop.mapred.gridmix.TestGridmixSubmission |\r\n   |   | hadoop.mapred.gridmix.TestLoadJob |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.52 ServerAPI=1.52 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8094/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8094 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle |\r\n   | uname | Linux 5698d08ecfea 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / 026c9ba4efeb3484a05537e552a6565895d700e1 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8094/1/testReport/ |\r\n   | Max. process+thread count | 4336 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-assemblies hadoop-common-project/hadoop-common hadoop-tools/hadoop-tools-dist hadoop-tools/hadoop-aws hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project . hadoop-cloud-storage-project/hadoop-cloud-storage-dist U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8094/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #8094:\nURL: https://github.com/apache/hadoop/pull/8094#issuecomment-3556042432\n\n   Hmm... I feel this is a surprising change for branch-3.4\n\n\n", "steveloughran commented on PR #8094:\nURL: https://github.com/apache/hadoop/pull/8094#issuecomment-3559789024\n\n   @pan3793 I understand your concerns but this is actually a packaging improvement\r\n   \r\n   The current build\r\n   * contains a tools/lib/bundle.jar for aws, which is too big for distribution\r\n   * doesn't put anyone elses cloud connector on the classpath. People doing their own distros end up moving them or doing other classpath stuff.\r\n   * forces a release process where we have to take the full tar, untar it, remove that bundle.tar, rebuild and sign again: https://github.com/apache/hadoop-release-support/blob/main/build.xml#L1315\r\n   \r\n   moving the hadoop-* cloud libs into common-lib but leaving out dependencies means the stuff is in the right place, provided users manually add the dependencies (which I'm not going to build with except for hadoop-azure as that's the httpclient and wildfly libs we use elsewhere.\r\n   \r\n   This makes releasing _easier_, and it makes adding the dependencies easier as the current setup requires a user to add the specific bundle-jar version the release was built with, same for the other components.\r\n   \n\n\n", "steveloughran opened a new pull request, #8098:\nURL: https://github.com/apache/hadoop/pull/8098\n\n   \r\n   Replace all uses of the word \"Li2cense\" with \"License\" in hadoop-assemblies\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8098:\nURL: https://github.com/apache/hadoop/pull/8098#issuecomment-3562328921\n\n   as mentioned in #8097\n\n\n", "pan3793 commented on PR #8094:\nURL: https://github.com/apache/hadoop/pull/8094#issuecomment-3562392587\n\n   @steveloughran I understand it's a little bit tricky for creating the lean tarball (should be similar to aarch tarball?), given that it already has a working script for it, I don't think it's a blocker for the 3.4 patching releases. TBH, I think Hadoop is currently kind of an abuse of patch releases, the recent patch releases contain more features than bug fixes, and even breaking changes.\n\n\n", "steveloughran commented on PR #8094:\nURL: https://github.com/apache/hadoop/pull/8094#issuecomment-3563608680\n\n   > I think Hadoop is currently kind of an abuse of patch releases, the recent patch releases contain more features than bug fixes, and even breaking changes.\r\n   \r\n   I've been trying to keep 3.4.3 low diff-wise to 3.4.2, balancing out the need for a lot of those transient CVE fixes. Other than anything related to avro updates, everything api-wise shouldn't be causing regressions.\r\n   \r\n   I'd like 3.4.3 to be the last java8 release, though I suspect we may need some dependency update releases next year. It's got a stabilisation of the aws analytics reader, but the only breaking change there is we change the default to \"on\"...people can switch back.\r\n   \r\n   maybe we should discuss this on common-dev?\n\n\n", "slfan1989 commented on PR #8098:\nURL: https://github.com/apache/hadoop/pull/8098#issuecomment-3567942469\n\n   LGTM.\n\n\n", "pan3793 commented on PR #8094:\nURL: https://github.com/apache/hadoop/pull/8094#issuecomment-3568750661\n\n   @steveloughran, I may not have enough background to comment on all the changes. Specific to this PR, I wonder \r\n   \r\n   - How does it affect `hadoop classpath`? As far as I know, my downstream projects require adding the output of `hadoop classpath` to their classpath.\r\n   - I suppose it has no effect on `hadoop-client-*` jars, right?\r\n   - Which maven `profiles` will be enabled by the Apache binary release? in other words, how to reproduce it from the source release locally?\r\n   \r\n   Compile/release is one-time stuff, while install/setup is more frequent, I would rather not have such a change in a **patch** release if it is not a blocker, but only makes the release process easier. Anyway, I don't have a binding veto, and I trust your authority in the Hadoop project.\n\n\n"], "labels": ["pull-request-available"], "summary": "Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/co", "qna": [{"question": "What is the issue title?", "answer": "hadoop binary distribution to move cloud connectors to hadoop common/lib"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19695", "project": "HADOOP", "title": "Add dual-stack/IPv6 Support to HttpServer2", "status": "Resolved", "reporter": "Ferenc Erdelyi", "created": "2025-09-17T11:21:23.000+0000", "description": "To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant.\r\n\r\nTo enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host.\r\nWhen the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6.\r\nWhen java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors.\r\nTo disable IPv4, you need to configure the OS at the system level.\r\n ", "comments": ["ferdelyi opened a new pull request, #7979:\nURL: https://github.com/apache/hadoop/pull/7979\n\n   To enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host. When the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6. When java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors. To disable IPv4, you need to configure the OS at the system level.\r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3303626071\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 55s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  1s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 40s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 14s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 1 new + 68 unchanged - 0 fixed = 69 total (was 68)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 44s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  42m 42s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 33s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 251m 12s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 69abc2152550 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 05144e7770fd55d4ed400b5ee8d71ea02d4d37b6 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/testReport/ |\r\n   | Max. process+thread count | 3098 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3304428637\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 48s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  0s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 57s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 43s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  42m 52s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 37s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 251m 40s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2f758f82d727 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4e384cfeaf21c2674e992b6ec288d9403a8d1adb |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/2/testReport/ |\r\n   | Max. process+thread count | 2679 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "brumi1024 commented on code in PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#discussion_r2359223851\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java:\n##########\n@@ -549,25 +550,50 @@ public HttpServer2 build() throws IOException {\n       }\n \n       for (URI ep : endpoints) {\n-        final ServerConnector connector;\n+        //\n+        // To enable dual-stack or IPv6 support, use InetAddress\n+        // .getAllByName(hostname) to resolve the IP addresses of a host.\n+        // When the system property java.net.preferIPv4Stack is set to true,\n+        // only IPv4 addresses are returned, and any IPv6 addresses are\n+        // ignored, so no extra check is needed to exclude IPv6.\n+        // When java.net.preferIPv4Stack is false, both IPv4 and IPv6\n+        // addresses may be returned, and any IPv6 addresses will also be\n+        // added as connectors.\n+        // To disable IPv4, you need to configure the OS at the system level.\n+        //\n+        InetAddress[] addresses = InetAddress.getAllByName(ep.getHost());\n+        server = addConnectors(\n+            ep, addresses, server, httpConfig, backlogSize, idleTimeout);\n+      }\n+      server.loadListeners();\n+      return server;\n+    }\n+\n+    @VisibleForTesting\n+    HttpServer2 addConnectors(\n+        URI ep, InetAddress[] addresses, HttpServer2 server,\n+        HttpConfiguration httpConfig, int backlogSize, int idleTimeout){\n+      for (InetAddress addr : addresses) {\n+        ServerConnector connector;\n         String scheme = ep.getScheme();\n         if (HTTP_SCHEME.equals(scheme)) {\n-          connector = createHttpChannelConnector(server.webServer,\n-              httpConfig);\n+          connector = createHttpChannelConnector(\n+              server.webServer, httpConfig);\n         } else if (HTTPS_SCHEME.equals(scheme)) {\n-          connector = createHttpsChannelConnector(server.webServer,\n-              httpConfig);\n+          connector = createHttpsChannelConnector(\n+              server.webServer, httpConfig);\n         } else {\n           throw new HadoopIllegalArgumentException(\n               \"unknown scheme for endpoint:\" + ep);\n         }\n-        connector.setHost(ep.getHost());\n+        LOG.info(\"Adding connector to WebServer for address {}\",\n\nReview Comment:\n   Do we need info level for this? Debug might be enough.\n\n\n\n", "ferdelyi commented on code in PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#discussion_r2359742305\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java:\n##########\n@@ -549,25 +550,50 @@ public HttpServer2 build() throws IOException {\n       }\n \n       for (URI ep : endpoints) {\n-        final ServerConnector connector;\n+        //\n+        // To enable dual-stack or IPv6 support, use InetAddress\n+        // .getAllByName(hostname) to resolve the IP addresses of a host.\n+        // When the system property java.net.preferIPv4Stack is set to true,\n+        // only IPv4 addresses are returned, and any IPv6 addresses are\n+        // ignored, so no extra check is needed to exclude IPv6.\n+        // When java.net.preferIPv4Stack is false, both IPv4 and IPv6\n+        // addresses may be returned, and any IPv6 addresses will also be\n+        // added as connectors.\n+        // To disable IPv4, you need to configure the OS at the system level.\n+        //\n+        InetAddress[] addresses = InetAddress.getAllByName(ep.getHost());\n+        server = addConnectors(\n+            ep, addresses, server, httpConfig, backlogSize, idleTimeout);\n+      }\n+      server.loadListeners();\n+      return server;\n+    }\n+\n+    @VisibleForTesting\n+    HttpServer2 addConnectors(\n+        URI ep, InetAddress[] addresses, HttpServer2 server,\n+        HttpConfiguration httpConfig, int backlogSize, int idleTimeout){\n+      for (InetAddress addr : addresses) {\n+        ServerConnector connector;\n         String scheme = ep.getScheme();\n         if (HTTP_SCHEME.equals(scheme)) {\n-          connector = createHttpChannelConnector(server.webServer,\n-              httpConfig);\n+          connector = createHttpChannelConnector(\n+              server.webServer, httpConfig);\n         } else if (HTTPS_SCHEME.equals(scheme)) {\n-          connector = createHttpsChannelConnector(server.webServer,\n-              httpConfig);\n+          connector = createHttpsChannelConnector(\n+              server.webServer, httpConfig);\n         } else {\n           throw new HadoopIllegalArgumentException(\n               \"unknown scheme for endpoint:\" + ep);\n         }\n-        connector.setHost(ep.getHost());\n+        LOG.info(\"Adding connector to WebServer for address {}\",\n\nReview Comment:\n   @brumi1024 thank you for your review! I've pushed a new commit with the suggested change.\n\n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3309202536\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 54s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 14s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 55s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 33s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 45s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  42m 49s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 40s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 253m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d2fb039015c7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 792d7c033b10af7f44ef638aa249d7594e5f3bcd |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/3/testReport/ |\r\n   | Max. process+thread count | 1273 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3319347658\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m 12s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  54m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 55s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 29s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  44m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  4s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  4s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 33s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 48s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  43m 28s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m  7s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 273m 25s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b753ac7f62a8 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7d394e5b895e03fa9e42057aa4ea2a74b3d0d034 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/4/testReport/ |\r\n   | Max. process+thread count | 3098 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "ferdelyi commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3324196841\n\n   Earlier shadedclient failures: [ERROR]   ITUseMiniCluster.clusterUp:78 \u00bb IO Problem starting http server\r\n   Latest failure: ERROR: Failed to write github status. Token expired or missing repo:status write?\n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3325156959\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m 16s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  4s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 19s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 59s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 23s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 44s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  42m 44s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 31s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  7s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 273m 56s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux afe6e7544141 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 48a180410434866489d88bb6cfda181f2bcc602d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/5/testReport/ |\r\n   | Max. process+thread count | 3100 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3333947403\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 59s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  53m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  17m 43s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 55s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 45s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  42m  1s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 35s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 248m 51s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 04cfc8fa6705 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2b42d965a1cda228b2307bf26b0444c9b047d8c8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/6/testReport/ |\r\n   | Max. process+thread count | 1249 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/6/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "ferdelyi commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3335426490\n\n   shadedclient was failing on this:\r\n   \r\n   [INFO] Running org.apache.hadoop.example.ITUseMiniCluster\r\n   [ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 19.60 s <<< FAILURE! ", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3336661768\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  4s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 37s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  47m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  8s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 47s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   3m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 40s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   6m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 57s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   3m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 54s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   6m 22s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  42m 14s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 39s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 276m 18s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 552m  3s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ed92b822cf7a 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8aa88fc3e70c245050e33aab0a585dd5a9606340 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/7/testReport/ |\r\n   | Max. process+thread count | 3098 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/7/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "ferdelyi commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3338485084\n\n   In the patch-shadedclient.txt the exception now is:\r\n   Caused by: java.lang.IllegalStateException: Insufficient configured threads: required=5 < max=5 for QueuedThreadPool[qtp164052991]@9c73fff{STARTED,5<=5<=5,i=3,r=-1,q=0}[ReservedThreadExecutor@29be997f{reserved=0/1,pending=0}]\r\n   \r\n   I've increased HTTP_MAX_THREADS by one, and the required number of threads also increased by one. Just out of curiosity will puch e.g. 10 and see if it keeps increasing.\n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3340611486\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 44s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  47m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  6s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 38s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 50s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   6m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 35s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  6s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 23s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   3m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 47s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   6m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m  6s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 42s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 278m  6s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 549m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2ff86f039bb0 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e14a395fa6450d1aeb2f0054197292b5af1540dc |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/8/testReport/ |\r\n   | Max. process+thread count | 2198 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/8/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3375718964\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  30m 10s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 52s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  45m 43s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  11m 43s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |  10m 13s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 14s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 29s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   5m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m  9s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |  11m 28s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |  11m 28s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |  10m  9s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |  10m  9s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  3s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   5m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 22s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 470m 28s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 741m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 83d811e5f434 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8e2cd7f590381ff77458ec6a44f0d3b83779eb53 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/testReport/ |\r\n   | Max. process+thread count | 2216 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3377795853\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 32s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  46m 25s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  11m 42s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   9m 59s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 12s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   5m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 36s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m 29s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |  11m 52s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |  11m 52s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   9m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   9m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  3s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   5m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 25s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 279m 19s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 54s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 521m 50s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d9d0c21591bb 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7adcb39cbe58f85adc3d03fa0ad751c64b87624e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/testReport/ |\r\n   | Max. process+thread count | 3098 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3386925666\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 36s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m  2s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  41m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 50s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 55s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 33s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 53s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   5m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 24s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 45s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 51s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   6m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 43s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 50s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 289m 16s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 557m 31s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/12/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 22968369ed69 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 87e9944f809d56e1c99a5ff5138c93377e99f464 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/12/testReport/ |\r\n   | Max. process+thread count | 3152 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/12/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "brumi1024 merged PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979\n\n\n"], "labels": ["pull-request-available"], "summary": "To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp ", "qna": [{"question": "What is the issue title?", "answer": "Add dual-stack/IPv6 Support to HttpServer2"}, {"question": "Who reported this issue?", "answer": "Ferenc Erdelyi"}]}
{"key": "HADOOP-19694", "project": "HADOOP", "title": "Bump guava to  33.4.8-jre due to EOL", "status": "Resolved", "reporter": "Rohit Kumar", "created": "2025-09-17T11:01:22.000+0000", "description": "We can use the latest 33.4.8-jre version as the current one is quite old.\r\n \r\nCherrypicking note: include the followup patch!", "comments": ["When I do a thirdparty build I now get a warning of duplicate module 9 info.\r\n{code}\r\n[INFO] No artifact matching filter org.checkerframework:checker-qual\r\n[WARNING] error_prone_annotations-2.36.0.jar, guava-33.4.8-jre.jar, failureaccess-1.0.3.jar, jspecify-1.0.0.jar, j2objc-annotations-3.0.0.jar define 1 overlapping classes: \r\n[WARNING]   - META-INF.versions.9.module-info\r\n[WARNING] maven-shade-plugin has detected that some class files are\r\n[WARNING] present in two or more JARs. When this happens, only one\r\n[WARNING] single version of the class is copied to the uber jar.\r\n[WARNING] Usually this is not harmful and you can skip these warnings,\r\n[WARNING] otherwise try to manually exclude artifacts based on\r\n[WARNING] mvn dependency:tree -Ddetail=true and the above output.\r\n[WARNING] See http://maven.apache.org/plugins/maven-shade-plugin/\r\n{code}\r\nThis is new.\r\n\r\ngiven we want this to work on java17+ , we need to come up with a way of resolving the conflict, at the very least by making the guava one dominant.\r\n\r\n\r\n\r\n", "ahh, the later versions of guava exclude a dependency on checkerframework. So any references in our code (there are four) fail. Which means we have to manually add it if we want a drop in replacement. PITA."], "labels": ["pull-request-available"], "summary": "We can use the latest 33", "qna": [{"question": "What is the issue title?", "answer": "Bump guava to  33.4.8-jre due to EOL"}, {"question": "Who reported this issue?", "answer": "Rohit Kumar"}]}
{"key": "HADOOP-19693", "project": "HADOOP", "title": "Update Java 24 to 25 in docker images", "status": "Resolved", "reporter": "Istvan Toth", "created": "2025-09-17T07:57:36.000+0000", "description": "Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images.", "comments": ["The ubuntu packages have been released.", "stoty opened a new pull request, #7991:\nURL: https://github.com/apache/hadoop/pull/7991\n\n   ### Description of PR\r\n   \r\n   Update Java 24 to 25 in docker images\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Built ubuntu_20 and ubuntu_24 x64 images, and ran java 25 in them.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "stoty commented on PR #7991:\nURL: https://github.com/apache/hadoop/pull/7991#issuecomment-3322708226\n\n   PTAL @slfan1989 \n\n\n", "hadoop-yetus commented on PR #7991:\nURL: https://github.com/apache/hadoop/pull/7991#issuecomment-3322950127\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  31m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 51s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  53m 17s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 46s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 20s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 58s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 130m 13s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7991/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7991 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux 7aa6647a89a6 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / edaa6e0885eeb0c0db357ef86e5defa0dfcc28d8 |\r\n   | Max. process+thread count | 525 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7991/1/console |\r\n   | versions | git=2.43.7 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7991:\nURL: https://github.com/apache/hadoop/pull/7991#issuecomment-3323181520\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  27m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 27s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  shadedclient  |   8m 53s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 24s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  31m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 32s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  70m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7991/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7991 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux 242b70b5bd76 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / edaa6e0885eeb0c0db357ef86e5defa0dfcc28d8 |\r\n   | Max. process+thread count | 538 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7991/1/console |\r\n   | versions | git=2.30.2 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7991:\nURL: https://github.com/apache/hadoop/pull/7991#issuecomment-3332430073\n\n   @pan3793 Could you please review this PR? Thank you very much! Pan has some experience with higher versions of JDK.\n\n\n", "stoty commented on PR #7991:\nURL: https://github.com/apache/hadoop/pull/7991#issuecomment-3332517143\n\n   Not much to review here @slfan1989 .\r\n   \r\n   This is just the first step to being able to test with JDK25.\r\n   \r\n   TBH I don't see much value in supporting or testing for JDK24, the real goal is JDK25, the stable release.\r\n   \r\n   Generally, I would test with the supported stable Java releases, plus the latest supported non-stable Java release.\r\n   \r\n   i.e when JDK 26 is released it, then keep Java 25 and add Java 26, then keep replacing Java 26 with 27,28,29,29... until the next stable Java is released (with the optimistic assumption that Hadoop is going to keep up with the non-stable Java releases, and we won't have to do any more big bang updates for 10+ java releases)\n\n\n", "slfan1989 merged PR #7991:\nURL: https://github.com/apache/hadoop/pull/7991\n\n\n", "slfan1989 commented on PR #7991:\nURL: https://github.com/apache/hadoop/pull/7991#issuecomment-3369570388\n\n   @stoty Thanks for the contribution!\n\n\n"], "labels": ["Java25", "pull-request-available"], "summary": "Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25", "qna": [{"question": "What is the issue title?", "answer": "Update Java 24 to 25 in docker images"}, {"question": "Who reported this issue?", "answer": "Istvan Toth"}]}
{"key": "HADOOP-19692", "project": "HADOOP", "title": "Exclude junit 4 transitive dependency", "status": "Resolved", "reporter": "Tsz-wo Sze", "created": "2025-09-16T18:18:40.000+0000", "description": "HADOOP-19617 removed direct junit 4 dependency.  However, junit 4 is still pulled transitively by other dependencies.", "comments": ["{code}\r\n[INFO] ------------------< org.apache.hadoop:hadoop-common >-------------------\r\n[INFO] Building Apache Hadoop Common 3.5.0-SNAPSHOT                    [11/117]\r\n[INFO]   from hadoop-common-project/hadoop-common/pom.xml\r\n...\r\n[INFO] +- com.squareup.okhttp3:mockwebserver:jar:4.11.0:test\r\n[INFO] |  +- com.squareup.okhttp3:okhttp:jar:4.11.0:test\r\n[INFO] |  |  \\- com.squareup.okio:okio:jar:3.2.0:test\r\n[INFO] |  |     \\- com.squareup.okio:okio-jvm:jar:3.2.0:test\r\n[INFO] |  \\- junit:junit:jar:4.13:test\r\n[INFO] |     \\- org.hamcrest:hamcrest-core:jar:1.3:test\r\n{code}", "{code}\r\n[INFO] ----------------< org.apache.hadoop:hadoop-hdfs-httpfs >----------------\r\n[INFO] Building Apache Hadoop HttpFS 3.5.0-SNAPSHOT                    [19/117]\r\n[INFO]   from hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml\r\n...\r\n[INFO] +- com.googlecode.json-simple:json-simple:jar:1.1.1:compile\r\n[INFO] | \u00a0\\- junit:junit:jar:4.10:compile\r\n{code}", "{code}\r\n[INFO] -----< org.apache.hadoop:hadoop-yarn-applications-catalog-webapp >------\r\n[INFO] Building Apache Hadoop YARN Application Catalog Webapp 3.5.0-SNAPSHOT [63/117]\r\n[INFO]   from hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml\r\n...\r\n[INFO] +- org.apache.solr:solr-test-framework:jar:8.11.2:test\r\n[INFO] |  +- org.apache.lucene:lucene-test-framework:jar:8.11.2:test\r\n[INFO] |  +- com.carrotsearch.randomizedtesting:junit4-ant:jar:2.7.2:test\r\n[INFO] |  +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.7.2:test\r\n[INFO] |  +- io.opentracing:opentracing-mock:jar:0.33.0:test\r\n[INFO] |  +- junit:junit:jar:4.13.1:test\r\n{code}", "{code}\r\n[INFO] --< org.apache.hadoop.applications.mawo:hadoop-yarn-applications-mawo >--\r\n[INFO] Building Apache Hadoop YARN Application MaWo 3.5.0-SNAPSHOT     [65/117]\r\n[INFO]   from hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/pom.xml\r\n...\r\n[INFO] +- com.googlecode.json-simple:json-simple:jar:1.1.1:compile\r\n[INFO] |  \\- junit:junit:jar:4.10:compile\r\n[INFO] |     \\- org.hamcrest:hamcrest-core:jar:1.1:compile\r\n{code}", "szetszwo opened a new pull request, #7978:\nURL: https://github.com/apache/hadoop/pull/7978\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   HADOOP-19692\r\n   \r\n   The direct junit 4 dependency was removed by HADOOP-19617. However, junit 4 is still pulled transitively by other dependencies.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   By the pull request action.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [NA] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [NA] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [NA] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3300529165\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  12m 11s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  47m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 49s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 15s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   4m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 58s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 146m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   0m 53s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  compile  |   1m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   1m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   1m 13s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   1m 13s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   0m 59s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   2m  9s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 42s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  10m  7s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 14s |  |  hadoop-project in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 59s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  unit  |   5m 32s |  |  hadoop-hdfs-httpfs in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 48s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 20s |  |  hadoop-yarn-applications-mawo-core in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 32s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 173m 47s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.appcatalog.application.TestAppCatalogSolrClient |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7978 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux b2d417db0dc5 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 613024d63ccccb96c39f1fab428aa1356b68d2d4 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/testReport/ |\r\n   | Max. process+thread count | 863 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "szetszwo commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3300605779\n\n   It turns out that we have not completely removed junit tests.\r\n   ```\r\n   ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7978/ubuntu-focal/src/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/http/TestHttpFileSystem.java:[57,5] cannot access org.junit.rules.ExternalResource\r\n     class file for org.junit.rules.ExternalResource not found\r\n   ```\n\n\n", "szetszwo commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3300697473\n\n   > ... we have not completely removed junit tests.\r\n   \r\n   The reason is that `mockwebserver` uses junit 4.  We should replace it with `mockwebserver3-junit5`.\n\n\n", "hadoop-yetus commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3301302828\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 57s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  49m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 59s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   4m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 54s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 27s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 41s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 39s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  9s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 21s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   4m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 55s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 35s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 36s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  22m 35s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   5m 55s |  |  hadoop-hdfs-httpfs in the patch passed.  |\r\n   | -1 :x: |  unit  |   1m 10s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/3/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 42s |  |  hadoop-yarn-applications-mawo-core in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  7s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 298m 25s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.appcatalog.application.TestAppCatalogSolrClient |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7978 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 19a387ca4ff9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 715044a0192f83d7661d00e9ede87d1067004e52 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/3/testReport/ |\r\n   | Max. process+thread count | 1294 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "szetszwo commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3304127134\n\n   ```\r\n   [ERROR] org.apache.hadoop.yarn.appcatalog.application.TestAppCatalogSolrClient.testNotFoundSearch ", "hadoop-yetus commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3304842636\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 48s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  11m 52s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  48m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 56s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   4m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 58s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 42s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 30s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 37s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 58s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   4m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 56s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 37s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 36s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  22m 38s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   5m 54s |  |  hadoop-hdfs-httpfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 15s |  |  hadoop-yarn-applications-catalog-webapp in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 42s |  |  hadoop-yarn-applications-mawo-core in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 297m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7978 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux ded193a6470d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 90ed7ecdaeec392fff06e28edc555a5420794f66 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/4/testReport/ |\r\n   | Max. process+thread count | 2987 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "szetszwo commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3304863270\n\n   > The junit 4 Assert is in LuceneTestCase. Not sure if updating the solr-test-framework version 8.11.2 could fix it.\r\n   \r\n   The current code in LuceneTestCase still use JUnit 4.\r\n   https://github.com/apache/lucene/blob/13a7e1e53d0e69233e775f2fb241b86c3ac0e527/lucene/test-framework/src/java/org/apache/lucene/tests/util/LuceneTestCase.java#L202C1-L217C3\n\n\n", "szetszwo commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3308469834\n\n   Filed HADOOP-19699  for TestAppCatalogSolrClient\n\n\n", "szetszwo merged PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978\n\n\n", "szetszwo commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3308483383\n\n   Thanks @cnauroth  and @slfan1989 for reviewing this!\n\n\n", "The pull request was merged.", "slfan1989 commented on PR #7978:\nURL: https://github.com/apache/hadoop/pull/7978#issuecomment-3310127385\n\n   > Thanks @cnauroth and @slfan1989 for reviewing this!\r\n   \r\n   @szetszwo Thank you for the contribution!\n\n\n"], "labels": ["pull-request-available"], "summary": "HADOOP-19617 removed direct junit 4 dependency", "qna": [{"question": "What is the issue title?", "answer": "Exclude junit 4 transitive dependency"}, {"question": "Who reported this issue?", "answer": "Tsz-wo Sze"}]}
{"key": "HADOOP-19691", "project": "HADOOP", "title": "[JDK17] Disallow JUnit4 Imports After JUnit5 Migration", "status": "Resolved", "reporter": "Shilun Fan", "created": "2025-09-16T03:05:49.000+0000", "description": "As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies.\r\n\r\nThis task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to ban future usage.", "comments": ["slfan1989 commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3297850797\n\n   @TaoYang526 We are currently working on upgrading the project to JUnit 5, and I have added a validation rule to prevent users from reintroducing JUnit 4 dependencies. During the review, I found that the testAsyncScheduleThreadExit method used JUnit 4 features, so I made some modifications. Could you please take a look and let me know if the changes are reasonable?\n\n\n", "hadoop-yetus commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3300118538\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 33s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  47m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 57s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 16s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 32s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 51s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  36m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  73m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 45s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  48m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 25s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 47s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  37m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  73m 40s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 503m 15s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   2m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 930m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7976 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 946810b75b42 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 78ed6e9e068b8c35d3eb372e09440128bf4fd0d8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/testReport/ |\r\n   | Max. process+thread count | 3137 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3300985791\n\n   > +1. Thanks @slfan1989 .\r\n   > \r\n   > I was going to suggest also banning `org.hamcrest`, but it looks like there is still a tiny amount of hamcrest remaining in YARN. Maybe this is a topic for a different PR.\r\n   > \r\n   > ```\r\n   > > grep -r --include '*.java' 'org.hamcrest' *\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppListControllerTest.java:import static org.hamcrest.MatcherAssert.assertThat;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppListControllerTest.java:import static org.hamcrest.core.Is.is;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppDetailsControllerTest.java:import static org.hamcrest.MatcherAssert.assertThat;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppDetailsControllerTest.java:import static org.hamcrest.core.Is.is;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppStoreControllerTest.java:import static org.hamcrest.MatcherAssert.assertThat;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppStoreControllerTest.java:import static org.hamcrest.core.Is.is;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java:import static org.hamcrest.MatcherAssert.assertThat;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java:import static org.hamcrest.core.IsInstanceOf.instanceOf;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java:import static org.hamcrest.core.IsSame.sameInstance;\r\n   > ```\r\n   \r\n   @cnauroth Thank you for reviewing the code! I\u2019ll submit a separate PR to replace the usage of `org.hamcrest.`.\n\n\n", "hadoop-yetus commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3312878887\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  11m 55s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  47m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  6s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 15s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 19s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 43s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  36m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  72m 58s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 45s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  48m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 26s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  20m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 22s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 54s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  37m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  73m  1s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 509m 47s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/2/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 937m 20s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7976 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux b7280a78b909 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b0d0b609f7714de5a35a2e356b8eca12903f5d3b |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/2/testReport/ |\r\n   | Max. process+thread count | 2676 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3328740044\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  54m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m  7s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  | 130m 22s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 43s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 32s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  39m 36s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 503m 30s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/4/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 16s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 778m 24s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7976 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 0b5b9083f652 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c161c9b215d2498c3c9a2d060a0be1701444b768 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/4/testReport/ |\r\n   | Max. process+thread count | 3135 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/4/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3328991107\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 40s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  54m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  9s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 13s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 49s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  | 130m 34s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 24s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 46s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  39m 49s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 538m 55s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/3/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 47s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 815m 59s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.TestRollingUpgrade |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7976 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 8c48d2e66c3e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c161c9b215d2498c3c9a2d060a0be1701444b768 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/3/testReport/ |\r\n   | Max. process+thread count | 3137 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3330876058\n\n   This PR enforces import restrictions to prohibit the use of JUnit4. The previously reported shade error has already been resolved in #7995. Given the lengthy compilation time, I will not re-trigger the build for this PR. I will continue to investigate and address the YARN crash issue separately.\r\n   \r\n   @cnauroth Thanks for the review!\n\n\n", "slfan1989 merged PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976\n\n\n"], "labels": ["pull-request-available"], "summary": "As our project has fully migrated to JUnit5, we should now enforce a rule that p", "qna": [{"question": "What is the issue title?", "answer": "[JDK17] Disallow JUnit4 Imports After JUnit5 Migration"}, {"question": "Who reported this issue?", "answer": "Shilun Fan"}]}
{"key": "HADOOP-19690", "project": "HADOOP", "title": "Bump commons-lang3 to 3.18.0 due to CVE-2025-48924", "status": "Resolved", "reporter": "PJ Fanning", "created": "2025-09-15T17:53:25.000+0000", "description": "https://www.cve.org/CVERecord?id=CVE-2025-48924\r\n\r\nWill update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.  ", "comments": ["pjfanning opened a new pull request, #7970:\nURL: https://github.com/apache/hadoop/pull/7970\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   CVE-2025-48924\r\n   \r\n   See https://issues.apache.org/jira/browse/HADOOP-19690 for reason to upgrade commons-text too.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7970:\nURL: https://github.com/apache/hadoop/pull/7970#issuecomment-3305413646\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 19s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  24m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  14m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 58s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  31m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 23s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  22m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 11s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  11m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  6s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  32m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 963m 33s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1144m 26s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7970 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux e242a07f343d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 82dae1282632826d8977c56821441f5b32ee1ec8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/testReport/ |\r\n   | Max. process+thread count | 4334 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7970:\nURL: https://github.com/apache/hadoop/pull/7970\n\n\n", "slfan1989 commented on PR #7970:\nURL: https://github.com/apache/hadoop/pull/7970#issuecomment-3310116432\n\n   @pjfanning Thanks for the contribution! The unit test errors are unrelated to this PR. Could you please take a look at the branch-3.4? I think this PR should also be backported there.\n\n\n", "pjfanning opened a new pull request, #7985:\nURL: https://github.com/apache/hadoop/pull/7985\n\n   * relates to #7970 \r\n   \r\n   * HADOOP-19690. bump commons-lang3 to 3.18.0 due to CVE-2025-48924\r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7985:\nURL: https://github.com/apache/hadoop/pull/7985#issuecomment-3311953658\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   7m 12s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   2m 18s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  22m 13s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |   8m 32s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 43s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  13m 58s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   4m 40s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 52s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  28m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 21s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  20m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 42s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 47s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   8m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   4m 44s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 54s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  25m 52s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  asflicense  |   0m 19s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/artifact/out/results-asflicense.txt) |  The patch generated 296 ASF License warnings.  |\r\n   |  |   | 197m 56s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.metrics2.source.TestJvmMetrics |\r\n   |   | hadoop.ipc.TestCallQueueManager |\r\n   |   | hadoop.fs.shell.TestHdfsTextCommand |\r\n   |   | hadoop.hdfs.util.TestByteArrayManager |\r\n   |   | hadoop.hdfs.web.TestWebHDFSOAuth2 |\r\n   |   | hadoop.hdfs.web.TestWebHdfsContentLength |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7985 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 71fdfc267120 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / fa8656b628557693f6cd66800a5d84bebd80501c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/testReport/ |\r\n   | Max. process+thread count | 685 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7985:\nURL: https://github.com/apache/hadoop/pull/7985\n\n\n", "slfan1989 commented on PR #7985:\nURL: https://github.com/apache/hadoop/pull/7985#issuecomment-3322481127\n\n   @pjfanning Thanks for the contribution! Merged into branch-3.4.\n\n\n"], "labels": ["pull-request-available"], "summary": "https://www", "qna": [{"question": "What is the issue title?", "answer": "Bump commons-lang3 to 3.18.0 due to CVE-2025-48924"}, {"question": "Who reported this issue?", "answer": "PJ Fanning"}]}
{"key": "HADOOP-19689", "project": "HADOOP", "title": "Bump netty to 4.1.127 due to CVE-2025-58057", "status": "Resolved", "reporter": "PJ Fanning", "created": "2025-09-15T17:45:00.000+0000", "description": "https://www.cve.org/CVERecord?id=CVE-2025-58057\r\n\r\nfixed in 4.1.125 but no harm upgrading to latest", "comments": ["pjfanning opened a new pull request, #7969:\nURL: https://github.com/apache/hadoop/pull/7969\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Upgrade netty due to CVE-2025-58057\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7969:\nURL: https://github.com/apache/hadoop/pull/7969\n\n\n", "slfan1989 commented on PR #7969:\nURL: https://github.com/apache/hadoop/pull/7969#issuecomment-3310122835\n\n   @pjfanning Thanks for the contribution! Merged into trunk. The branch-3.4 should also be taken into consideration.\n\n\n", "pjfanning opened a new pull request, #7984:\nURL: https://github.com/apache/hadoop/pull/7984\n\n   * HADOOP-19689: bump netty to 4.1.127.Final due to CVE-2025-58057\r\n   \r\n   Signed-off-by: Shilun Fan <slfan1989@apache.org>\r\n   \r\n   Update LICENSE-binary\r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7984:\nURL: https://github.com/apache/hadoop/pull/7984#issuecomment-3314414435\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m 42s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   2m 19s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m  3s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |  19m  9s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  17m 20s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  23m  6s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   8m 58s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 19s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  54m 54s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 38s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 43s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 20s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  16m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  18m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   8m 22s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 52s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  51m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 738m 57s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7984/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1050m  3s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.gridmix.TestGridmixSubmission |\r\n   |   | hadoop.mapred.gridmix.TestLoadJob |\r\n   |   | hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2 |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7984/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7984 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux df829c2020d7 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / 80ba81ead32289cc5ee5aceb75950ca4e5a9c50e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7984/1/testReport/ |\r\n   | Max. process+thread count | 3744 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7984/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7984:\nURL: https://github.com/apache/hadoop/pull/7984\n\n\n", "slfan1989 commented on PR #7984:\nURL: https://github.com/apache/hadoop/pull/7984#issuecomment-3322482594\n\n   @pjfanning Thanks for the contribution! Merged into branch-3.4.\n\n\n"], "labels": ["pull-request-available"], "summary": "https://www", "qna": [{"question": "What is the issue title?", "answer": "Bump netty to 4.1.127 due to CVE-2025-58057"}, {"question": "Who reported this issue?", "answer": "PJ Fanning"}]}
{"key": "HADOOP-19688", "project": "HADOOP", "title": "S3A: ITestS3ACommitterMRJob failing on Junit5", "status": "Resolved", "reporter": "Steve Loughran", "created": "2025-09-15T17:38:39.000+0000", "description": "NPE in test200 of ITestS3ACommitterMRJob.\r\n\r\nCause is\r\n* test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir\r\n* somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs.\r\n\r\nFix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong. ", "comments": ["steveloughran opened a new pull request, #7968:\nURL: https://github.com/apache/hadoop/pull/7968\n\n   \r\n   Adds extra logging as to what is happening, passes in actual test dir set at class level.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   s3 london\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "steveloughran commented on PR #7968:\nURL: https://github.com/apache/hadoop/pull/7968#issuecomment-3293282933\n\n   ```\r\n   [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob\r\n   [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob\r\n   [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.97 s ", "steveloughran commented on PR #7968:\nURL: https://github.com/apache/hadoop/pull/7968#issuecomment-3293285487\n\n   fyi @slfan1989 @ahmarsuhail @mukund-thakur\r\n   one of the final nits of junit5 migration\n\n\n", "hadoop-yetus commented on PR #7968:\nURL: https://github.com/apache/hadoop/pull/7968#issuecomment-3294044978\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  11m 17s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  41m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 54s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   3m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 28s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 35s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 37s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m 17s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7968/1/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 1 new + 45 unchanged - 1 fixed = 46 total (was 46)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 40s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 38s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 11s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   9m 58s |  |  hadoop-mapreduce-client-core in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 47s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  8s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 230m 49s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7968/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7968 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 0d73de768dd4 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a86c300286ffb8cd8884923b5d6a0deaf9095d63 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7968/1/testReport/ |\r\n   | Max. process+thread count | 1567 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7968/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7968:\nURL: https://github.com/apache/hadoop/pull/7968#issuecomment-3294374360\n\n   LGTM\n\n\n", "steveloughran merged PR #7968:\nURL: https://github.com/apache/hadoop/pull/7968\n\n\n"], "labels": ["pull-request-available"], "summary": "NPE in test200 of ITestS3ACommitterMRJob", "qna": [{"question": "What is the issue title?", "answer": "S3A: ITestS3ACommitterMRJob failing on Junit5"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19687", "project": "HADOOP", "title": "Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864", "status": "Resolved", "reporter": "Rohit Kumar", "created": "2025-09-15T11:29:00.000+0000", "description": "*CVE-2025-53864:*\r\n\r\nConnect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson.\r\n\r\nSeverity: 6.9 (medium)", "comments": ["rohit-kb opened a new pull request, #7965:\nURL: https://github.com/apache/hadoop/pull/7965\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   1. Bumping nimbus-jose-jwt to 10.4 due to CVEs\r\n   2. com.github.stephenc.jcip:jcip-annotations is being shaded and is no more a transitive dependency from nimbus starting from versions 9.38, so we can add it as an explicit dependency.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "duplicate of HADOOP-19632 for which there are already PRs", "rohit-kb commented on PR #7965:\nURL: https://github.com/apache/hadoop/pull/7965#issuecomment-3302275652\n\n   Hi @pjfanning, can we use this patch instead of the original one as I don't see any progress on that? We need this upgrade in the downstream soon. \r\n   \r\n   Also, it seems like the original patch hasn't handled the shading of com.github.stephenc.jcip:jcip-annotations in later nimbus versions. Thanks\n\n\n", "pjfanning commented on PR #7965:\nURL: https://github.com/apache/hadoop/pull/7965#issuecomment-3302315128\n\n   Could you rebase this to force a new CI run? The tests crashed in the last run.\r\n   \n\n\n", "pjfanning commented on PR #7965:\nURL: https://github.com/apache/hadoop/pull/7965#issuecomment-3302343034\n\n   Could you change the name of the PR and the git commit to use [HADOOP-19632](https://issues.apache.org/jira/browse/HADOOP-19632)?\n\n\n"], "labels": ["pull-request-available"], "summary": "*CVE-2025-53864:*\r\n\r\nConnect2id Nimbus JOSE + JWT before 10", "qna": [{"question": "What is the issue title?", "answer": "Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864"}, {"question": "Who reported this issue?", "answer": "Rohit Kumar"}]}
{"key": "HADOOP-19685", "project": "HADOOP", "title": "Clover breaks on double semicolon", "status": "Resolved", "reporter": "Michael Smith", "created": "2025-09-12T20:38:19.000+0000", "description": "Building with {{-Pclover}} fails with\r\n{code}\r\n[INFO] Instrumentation error\r\ncom.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;\r\n...\r\nFailed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory\r\n{code}\r\n\r\nIt doesn't seem to like a double semicolon in ITestS3APutIfMatchAndIfNoneMatch.java that was added in HADOOP-19256.", "comments": ["MikaelSmith opened a new pull request, #7956:\nURL: https://github.com/apache/hadoop/pull/7956\n\n   ### Description of PR\r\n   \r\n   Removes the extra semicolon after an import that causes `-Pclover` to fail with\r\n   \r\n       com.atlassian.clover.api.CloverException: hadoop/hadoop-tools/\r\n       hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/\r\n       ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;`\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   ```\r\n   mvn -e -Pclover install -DskipTests -DskipShade --projects 'hadoop-tools/hadoop-aws'\r\n   ```\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7956:\nURL: https://github.com/apache/hadoop/pull/7956#issuecomment-3287077606\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m 13s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 21s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7956 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d1403c764b02 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9fc528eeb6112dd9b3209b648fb33da720214dff |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/testReport/ |\r\n   | Max. process+thread count | 709 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "cnauroth closed pull request #7956: HADOOP-19685. Fix double semicolon breaking clover\nURL: https://github.com/apache/hadoop/pull/7956\n\n\n", "cnauroth commented on PR #7956:\nURL: https://github.com/apache/hadoop/pull/7956#issuecomment-3287154332\n\n   I committed this to trunk and branch-3.4. Thank you for the patch @MikaelSmith !\n\n\n"], "labels": ["pull-request-available"], "summary": "Building with {{-Pclover}} fails with\r\n{code}\r\n[INFO] Instrumentation error\r\ncom", "qna": [{"question": "What is the issue title?", "answer": "Clover breaks on double semicolon"}, {"question": "Who reported this issue?", "answer": "Michael Smith"}]}
{"key": "HADOOP-19684", "project": "HADOOP", "title": "Add JDK 21 to Ubuntu 20.04 docker development images", "status": "Resolved", "reporter": "Istvan Toth", "created": "2025-09-10T14:00:14.000+0000", "description": "We want to support JDK21, we better have it available in the development image for testing.\r\n", "comments": ["stoty opened a new pull request, #7947:\nURL: https://github.com/apache/hadoop/pull/7947\n\n   ### Description of PR\r\n   \r\n   Add JDK 21 to Ubuntu 20.04 and 24.04  docker development images\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Built the default image locally and started JDK21.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3275227945\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   1m 32s |  |  Docker failed to build run-specific yetus/hadoop:tp-5188}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7947 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on code in PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#discussion_r2340621524\n\n\n##########\ndev-support/docker/pkg-resolver/packages.json:\n##########\n@@ -268,17 +268,20 @@\n     ],\n     \"ubuntu:focal\": [\n       \"temurin-24-jdk\",\n+      \"temurin-21-jdk\",\n\nReview Comment:\n   the ubuntu official apt repo provides `openjdk-21-jdk`, it's unnecessary to install from 3rd party. let's use the official one and put it at the end of the list.\n\n\n\n", "pan3793 commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3280408907\n\n   The Jenkins failure should be fixed by https://github.com/apache/hadoop/pull/7938, @slfan1989 can you help merging that to unblock this patch?\n\n\n", "stoty commented on code in PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#discussion_r2340811930\n\n\n##########\ndev-support/docker/pkg-resolver/packages.json:\n##########\n@@ -268,17 +268,20 @@\n     ],\n     \"ubuntu:focal\": [\n       \"temurin-24-jdk\",\n+      \"temurin-21-jdk\",\n\nReview Comment:\n   Thanks. Done @pan3793 .\n\n\n\n", "hadoop-yetus commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3280638248\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   1m 21s |  |  Docker failed to build run-specific yetus/hadoop:tp-1035}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7947 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3282893605\n\n   > The Jenkins failure should be fixed by #7938, @slfan1989 can you help merge that to unblock this patch?\r\n   \r\n   @stoty I\u2019ve already merged #7938 into the trunk branch, so we can move forward with this PR.\n\n\n", "stoty commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3283557198\n\n   restarted CI\n\n\n", "hadoop-yetus commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3283756503\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  25m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 48s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  46m 30s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 45s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  36m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 56s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 111m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7947 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux 68e70612efec 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4ed7fc2da756cc4332d3f9ecc8cce291d094b50d |\r\n   | Max. process+thread count | 708 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/console |\r\n   | versions | git=2.43.7 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3283820061\n\n   The CI results meet expectations. I will proceed with merging the change shortly.\n\n\n", "hadoop-yetus commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3284002090\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 27s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 35s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 30s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  25m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  74m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7947 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux 26f820c8794f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4ed7fc2da756cc4332d3f9ecc8cce291d094b50d |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/console |\r\n   | versions | git=2.30.2 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3284223003\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 55s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  71m 51s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7947 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux cedf7863bb64 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4ed7fc2da756cc4332d3f9ecc8cce291d094b50d |\r\n   | Max. process+thread count | 567 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947\n\n\n", "slfan1989 commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3289299477\n\n   @stoty Thanks for the contribution! @pan3793 Thanks for the review!\n\n\n"], "labels": ["pull-request-available"], "summary": "We want to support JDK21, we better have it available in the development image f", "qna": [{"question": "What is the issue title?", "answer": "Add JDK 21 to Ubuntu 20.04 docker development images"}, {"question": "Who reported this issue?", "answer": "Istvan Toth"}]}
{"key": "HADOOP-19682", "project": "HADOOP", "title": "Fix incorrect link from current3 of hadoop-site", "status": "Resolved", "reporter": "Xiaoqiao He", "created": "2025-09-09T03:15:33.000+0000", "description": "Fix hadoop site incorrect link which reported by https://lists.apache.org/thread/ozvkh0x7qdr8d3vr0tbm5g7jx1jprbct.", "comments": [], "labels": ["pull-request-available"], "summary": "Fix hadoop site incorrect link which reported by https://lists", "qna": [{"question": "What is the issue title?", "answer": "Fix incorrect link from current3 of hadoop-site"}, {"question": "Who reported this issue?", "answer": "Xiaoqiao He"}]}
{"key": "HADOOP-19681", "project": "HADOOP", "title": "Fix S3A failing to initialize S3 buckets having namespace with dot followed by number", "status": "Open", "reporter": "Syed Shameerur Rahman", "created": "2025-09-08T08:51:08.000+0000", "description": "S3A fails to initialize when S3 bucket namespace is having dot followed by a number.\u00a0\r\n\r\n{*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's\r\nURI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null.\r\n\r\n\u00a0\r\n\r\n{{}}\r\n{code:java}\r\nhadoop dfs -ls s3a://bucket-v1.1-us-east-1/\r\n\r\nWARNING: Use of this script to execute dfs is deprecated.\r\nWARNING: Attempting to execute replacement \"hdfs dfs\" instead.\r\n\r\n2025-09-08 06:13:06,670 WARN fs.FileSystem: Failed to initialize filesystem s3://bucket-v1.1-us-east-1/: java.lang.IllegalArgumentException: bucket is null/empty\r\n-ls: bucket is null/empty{code}\r\n\u00a0\r\n\r\n{*}Please Note{*}: Although there has been discussion on not allowing S3 buckets with such a namespace ([https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/]) , Amazon S3 still allows you to create a bucket with such a namespace.", "comments": ["shameersss1 opened a new pull request, #7942:\nURL: https://github.com/apache/hadoop/pull/7942\n\n   ### Description of PR\r\n   \r\n   S3A fails to initialize when S3 bucket namespace is having dot followed by a number. \r\n   \r\n   Specific Problem: URI parsing fails when S3 bucket names contain a dot followed by a number (like bucket-v1.1-us-east-1). Java's\r\n   URI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Tested in us-east-1 with bucket having namespace with dot followed by a number.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "shameersss1 commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3267057573\n\n   Test `ITestBucketTool,ITestS3ACommitterMRJob` are failing even without the change.\n\n\n", "shameersss1 commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3267074576\n\n   @steveloughran  : Could you please review the changes.\n\n\n", "hadoop-yetus commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3267937541\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m  0s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 25s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 48s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 57s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 10s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  6s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 48s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 10s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 46s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  22m 55s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 41s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  9s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 253m 43s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7942 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3434362c830f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 82914829acf34ecd0b05c57af673c1e4095acb30 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/1/testReport/ |\r\n   | Max. process+thread count | 3056 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3269845775\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 40s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 52s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 48s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 51s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 14s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 50s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 48s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  8s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 47s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 53s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 49s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  9s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 237m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7942 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b5e9b0d84482 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ea1308a4c1fdf443562e752e09d43ae0a7f5cd8b |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/2/testReport/ |\r\n   | Max. process+thread count | 1271 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on code in PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#discussion_r2348701684\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AConfiguration.java:\n##########\n@@ -594,13 +595,28 @@ private static <T> T getField(Object target, Class<T> fieldType,\n   public void testConfOptionPropagationToFS() throws Exception {\n     Configuration config = new Configuration();\n     String testFSName = config.getTrimmed(TEST_FS_S3A_NAME, \"\");\n-    String bucket = new URI(testFSName).getHost();\n+    URI uri = new URI(testFSName);\n+    String bucket = uri.getHost();\n+    if (bucket == null) {\n+      bucket = uri.getAuthority();\n+    }\n     setBucketOption(config, bucket, \"propagation\", \"propagated\");\n     fs = S3ATestUtils.createTestFileSystem(config);\n     Configuration updated = fs.getConf();\n     assertOptionEquals(updated, \"fs.s3a.propagation\", \"propagated\");\n   }\n \n+  @Test\n+  public void testBucketNameWithDotAndNumber() throws Exception {\n+    Configuration config = new Configuration();\n+    Path path = new Path(\"s3a://test-bucket-v1.1\");\n+    try (FileSystem fs = path.getFileSystem(config)) {\n+      assertThat(fs instanceof S3AFileSystem)\n\nReview Comment:\n   use whatever assertj assertion is about instanceof, so you get a better error.\n\n\n\n", "raphaelazzolini commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3293303278\n\n   > Oh, this complicates things. I've been happily treating all issues related to buckets with . in them as WONTFIX. Storediag tells people off too.\r\n   \r\n   @steveloughran, in HADOOP-17241, you referenced this announcement as one of the reasons to not support this buckets with dot in the name: https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/\r\n   \r\n   However, AWS have since revised their stance. AWS has confirmed they will continue supporting buckets with dots in their names through virtual hosted-style URLs due to customer feedback and compatibility requirements.\r\n   \r\n   > We have also heard feedback from customers that virtual hosted-style URLs should support buckets that have dots in their names for compatibility reasons, so we\u2019re working on developing that support.\r\n   \r\n   So I guess it makes sense to add support for it in S3A.\n\n\n", "hadoop-yetus commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3298148674\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 59s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  40m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m  1s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 53s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 15s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 47s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 35s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 39s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 49s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  14m 49s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/artifact/out/blanks-eol.txt) |  The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   4m 12s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 1 new + 63 unchanged - 0 fixed = 64 total (was 63)  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 46s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 39s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 44s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  8s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 261m 30s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7942 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux 8e602b48d6d4 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5014c326331ebcc9e8cb30cf2632774733c6ce56 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/testReport/ |\r\n   | Max. process+thread count | 1302 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3299506183\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  11m  7s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  42m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m  8s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 59s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 13s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 48s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  6s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 49s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 49s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/4/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  8s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 47s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m  4s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 45s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  9s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 251m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7942 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux 2d2f97096065 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f86ae95b09e52b9c71c1d9d937b060aa9409c096 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/4/testReport/ |\r\n   | Max. process+thread count | 1270 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "shameersss1 commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3303302768\n\n   Thanks @steveloughran  for the review. I have addressed your comments.\n\n\n", "steveloughran commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3329432114\n\n   @raphaelazzolini \r\n   > AWS has confirmed they will continue supporting buckets with dots in their names through virtual hosted-style URLs due to customer feedback and compatibility requirements.\r\n   \r\n   OK. main issue is that there may be existing code which cares about hostname and may not look at FQDN for differencing names. I can't think of any right now -bucket names which don't map to valid hostnames are more a pain point\n\n\n", "steveloughran commented on code in PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#discussion_r2376222445\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java:\n##########\n@@ -573,8 +573,11 @@ private static void addDeprecatedKeys() {\n    */\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n-    // get the host; this is guaranteed to be non-null, non-empty\n+    // get the host; fallback to authority if getHost() returns null\n     bucket = name.getHost();\n+    if (bucket == null) {\n\nReview Comment:\n   pull this out, stick it in `S3AUtils`, add unit tests that now try to break things. Use everywhere\n\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/AbstractFileSystem.java:\n##########\n@@ -335,7 +335,7 @@ private URI getUri(URI uri, String supportedScheme,\n     int port = uri.getPort();\n     port = (port == -1 ? defaultPort : port);\n     if (port == -1) { // no port supplied and default port is not specified\n-      return new URI(supportedScheme, authority, \"/\", null);\n+      return URI.create(supportedScheme + \"://\" + authority + \"/\");\n\nReview Comment:\n   why this change?\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md:\n##########\n@@ -31,6 +31,12 @@ before 2021.\n Consult [S3A and Directory Markers](directory_markers.html) for\n full details.\n \n+### <a name=\"bucket-name-compatibility\"></a> S3 Bucket Name Compatibility\n+\n+This release adds support for S3 bucket names containing dots followed by numbers\n+(e.g., `my-bucket-v1.1`, `data-store.v2.3`). Previous versions of the Hadoop S3A\n+client failed to initialize such buckets due to URI parsing limitations.\n+\n\nReview Comment:\n   * highlight that per-bucket settings do not work for dotted buckets (they don't, do they?), so the ability to use them is still very much downgraded.\r\n   * Explain that AWS do not recommend dotted buckets for anything other than web site serving\r\n   * highlight that path style access is needed to access (correct? never tried)\n\n\n\n"], "labels": ["pull-request-available"], "summary": "S3A fails to initialize when S3 bucket namespace is having dot followed by a num", "qna": [{"question": "What is the issue title?", "answer": "Fix S3A failing to initialize S3 buckets having namespace with dot followed by number"}, {"question": "Who reported this issue?", "answer": "Syed Shameerur Rahman"}]}
{"key": "HADOOP-19680", "project": "HADOOP", "title": "Update non-thirdparty Guava version to 32.0.1", "status": "Resolved", "reporter": "Istvan Toth", "created": "2025-09-08T04:59:06.000+0000", "description": "Guava has been already updated to 32.0.1 in hadoop-thirdparty.\r\nHowever, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners.\r\n\r\nSync the non-thirdparty Guava version to the thirdparty one.", "comments": ["stoty opened a new pull request, #7940:\nURL: https://github.com/apache/hadoop/pull/7940\n\n   same as the current thirdparty Guava version\r\n   \r\n   ### Description of PR\r\n   \r\n   Guava has been already updated to 32.0.1 in hadoop-thirdparty.\r\n   However, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners.\r\n   \r\n   Sync the non-thirdparty Guava version to the thirdparty one.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Test suite in CI (on this PR)\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3264930118\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 57s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  87m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 17s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7940 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux fe8aa1f6455e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8794a7e5b6cf955551d93f3a15effbd83d5174b7 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3280459780\n\n   cc @cnauroth \n\n\n", "slfan1989 commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3289303390\n\n   > +1 Thank you @stoty . I'm good with this change. Ideally, I would like to see at least one more committer +1, just in case there are some downstream impacts I haven't thought of.\r\n   > \r\n   > CC: @steveloughran , @ayushtkn , @mukund-thakur\r\n   \r\n   @cnauroth Thanks for the review! I think we still need to carefully consider this change and should wait for Steve's confirmation before making a decision. \r\n   \r\n   cc: @steveloughran \n\n\n", "stoty commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3289335562\n\n   For what it's worth:\r\n   \r\n   I've fought a LOT with guava versions in the last six years, but I haven't seen any issue (apart from Google adding new annotation libraries which throw off some shading tests)  when upgrading from 27 to a newer version.\r\n   \n\n\n", "steveloughran commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3291722519\n\n   I've been away. let's do it an update in release notes that  you can change the version without breaking any hadoop code.\n\n\n", "stoty commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3291915610\n\n   > I've been away. let's do it an update in release notes that you can change the version without breaking any hadoop code.\r\n   \r\n   Thanks @steveloughran .\r\n   I'm not sure I understand your comment. I have added a release note that explains the change to the JIRA.\n\n\n", "steveloughran merged PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940\n\n\n", "steveloughran commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3293320945\n\n   thanks...your release note is good, added something in the commit too.\r\n   \r\n   can you do a backport PR to branch-3.4; I think we should be looking at a \"dependency update\" release there with minimal actual code changes\n\n\n", "slfan1989 merged PR #7977:\nURL: https://github.com/apache/hadoop/pull/7977\n\n\n", "slfan1989 commented on PR #7977:\nURL: https://github.com/apache/hadoop/pull/7977#issuecomment-3310107461\n\n   @stoty Sorry for the late reply, thanks for the contribution!\n\n\n"], "labels": ["pull-request-available"], "summary": "Guava has been already updated to 32", "qna": [{"question": "What is the issue title?", "answer": "Update non-thirdparty Guava version to 32.0.1"}, {"question": "Who reported this issue?", "answer": "Istvan Toth"}]}
{"key": "HADOOP-19679", "project": "HADOOP", "title": "Maven site task fails with Java 17", "status": "Resolved", "reporter": "Michael Smith", "created": "2025-09-05T22:47:46.000+0000", "description": "If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report\r\n{code}\r\n$ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site\r\n...\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report: IllegalArgumentException -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:347)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\nCaused by: org.apache.maven.plugin.MojoExecutionException: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:153)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\nCaused by: org.apache.maven.doxia.siterenderer.RendererException: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:247)\r\n    at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:349)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:194)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:143)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\nCaused by: java.lang.IllegalArgumentException\r\n    at org.objectweb.asm.ClassReader.<init> (Unknown Source)\r\n    at org.objectweb.asm.ClassReader.<init> (Unknown Source)\r\n    at org.objectweb.asm.ClassReader.<init> (Unknown Source)\r\n    at org.apache.maven.shared.dependency.analyzer.asm.DependencyClassFileVisitor.visitClass (DependencyClassFileVisitor.java:65)\r\n    at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.visitClass (ClassFileVisitorUtils.java:163)\r\n    at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.acceptDirectory (ClassFileVisitorUtils.java:143)\r\n    at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.accept (ClassFileVisitorUtils.java:71)\r\n    at org.apache.maven.shared.dependency.analyzer.asm.ASMDependencyAnalyzer.analyze (ASMDependencyAnalyzer.java:50)\r\n    at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.buildDependencyClasses (DefaultProjectDependencyAnalyzer.java:211)\r\n    at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.buildDependencyClasses (DefaultProjectDependencyAnalyzer.java:198)\r\n    at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.analyze (DefaultProjectDependencyAnalyzer.java:74)\r\n    at org.apache.maven.plugins.dependency.analyze.AnalyzeReportMojo.executeReport (AnalyzeReportMojo.java:138)\r\n    at org.apache.maven.reporting.AbstractMavenReport.generate (AbstractMavenReport.java:255)\r\n    at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:226)\r\n    at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:349)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:194)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:143)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\n[ERROR] \r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\r\n[ERROR] \r\n[ERROR] After correcting the problems, you can resume the build with the command\r\n[ERROR]   mvn <args> -rf :hadoop-annotations\r\n{code}\r\n\r\nUpdating to the latest maven-dependency-plugin version (3.8.1) fixes it for me.", "comments": ["MikaelSmith opened a new pull request, #7936:\nURL: https://github.com/apache/hadoop/pull/7936\n\n   ### Description of PR\r\n   Updates maven-dependency-plugin to 3.8.1 to fix an IllegalArgumentException when running the `site:site` Maven task with Java 17.\r\n   \r\n   ### How was this patch tested?\r\n   Ran `mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site` locally with openjdk-17 and it succeeded.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7936:\nURL: https://github.com/apache/hadoop/pull/7936#issuecomment-3262110832\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   9m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  15m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   6m  7s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  94m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  19m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m  3s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 20s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   8m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   9m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m  5s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 21s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 663m 19s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 840m 24s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapreduce.v2.TestUberAM |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7936 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 33f1385abb00 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e89a211da8c19aed6f68dbd9ea4418f6e657d615 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/testReport/ |\r\n   | Max. process+thread count | 3913 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "szetszwo merged PR #7936:\nURL: https://github.com/apache/hadoop/pull/7936\n\n\n", "The pull request is now merged.  Thanks, [~MikaelSmith]!", "Thanks! PR was merged against trunk, so I think current Fix Version would be 3.5.0.", "You are right -- updated it to 3.5.0."], "labels": ["pull-request-available"], "summary": "If I try to build site with Java 17, I get an IllegalArgumentException from anal", "qna": [{"question": "What is the issue title?", "answer": "Maven site task fails with Java 17"}, {"question": "Who reported this issue?", "answer": "Michael Smith"}]}
{"key": "HADOOP-19678", "project": "HADOOP", "title": "[JDK17] Remove powermock dependency", "status": "Resolved", "reporter": "Tsz-wo Sze", "created": "2025-09-05T18:22:53.000+0000", "description": "The powermock dependency is specified in\r\n- hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml\r\n- hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml\r\n\r\nbut not used anywhere.  We should remove it.", "comments": ["Will include this in HADOOP-19677.", "Reopen for removing unused powermock dependencies.\r\n\r\n", "szetszwo opened a new pull request, #7939:\nURL: https://github.com/apache/hadoop/pull/7939\n\n   ### Description of PR\r\n   \r\n   HADOOP-19678\r\n   \r\n   The powermock dependency is specified in\r\n   \r\n   - hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml\r\n   - hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml\r\n   \r\n   but not used anywhere. We should remove it.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   By existing tests.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [NA] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [NA ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [NA ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7939:\nURL: https://github.com/apache/hadoop/pull/7939#issuecomment-3264013057\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m  3s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 24s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 39s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 43s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 33s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 111m  0s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  3s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 40s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   1m  4s |  |  hadoop-yarn-server-timelineservice-documentstore in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 44s |  |  hadoop-huaweicloud in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 201m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7939/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7939 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 9f779bd63474 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / dd72444841a0ec35affd2cf1e058030ef5fe99d2 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7939/1/testReport/ |\r\n   | Max. process+thread count | 708 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-cloud-storage-project/hadoop-huaweicloud U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7939/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7939:\nURL: https://github.com/apache/hadoop/pull/7939\n\n\n", "szetszwo commented on PR #7939:\nURL: https://github.com/apache/hadoop/pull/7939#issuecomment-3267073155\n\n   @slfan1989 , @pan3793 , thanks a lot for reviewing this!\n\n\n"], "labels": ["pull-request-available"], "summary": "The powermock dependency is specified in\r\n- hadoop-cloud-storage-project/hadoop-", "qna": [{"question": "What is the issue title?", "answer": "[JDK17] Remove powermock dependency"}, {"question": "Who reported this issue?", "answer": "Tsz-wo Sze"}]}
{"key": "HADOOP-19677", "project": "HADOOP", "title": "[JDK17] Remove mockito-all 1.10.19 and powermock", "status": "Resolved", "reporter": "Tsz-wo Sze", "created": "2025-09-05T17:51:30.000+0000", "description": "- The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11.\r\n\r\n - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17.\r\n -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml\r\n\r\n\u00a0", "comments": ["szetszwo opened a new pull request, #7935:\nURL: https://github.com/apache/hadoop/pull/7935\n\n   ### Description of PR\r\n   \r\n   The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   By updating existing tests\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [NA] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [NA] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [NA] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3260267569\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 33s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  41m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  19m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  17m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 10s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   4m 40s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |   0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   5m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   4m 41s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 59s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 17s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 18s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 18s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 33s |  |  hadoop-auth in the patch passed.  |\r\n   | -1 :x: |  unit  | 118m 19s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in the patch failed.  |\r\n   | -1 :x: |  unit  | 141m 25s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 50s |  |  hadoop-yarn-server-timelineservice-documentstore in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 23s |  |  hadoop-yarn-server-globalpolicygenerator in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m  5s |  |  hadoop-yarn-applications-catalog-webapp in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 20s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt) |  hadoop-yarn-ui in the patch failed.  |\r\n   | +1 :green_heart: |  unit  |   3m 12s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 47s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 541m 38s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7935 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 21c4e19a37e1 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7dadba11fdeaf702d291642f8b2d59c197ec9ecb |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/testReport/ |\r\n   | Max. process+thread count | 1139 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3260284208\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  0s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m  4s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  46m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  20m  8s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 16s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  9s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   0m 38s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 16s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 16s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 15s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 15s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 46s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 39s | [/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   4m 50s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 11s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 15s |  |  hadoop-project has no data from spotbugs  |\r\n   | -1 :x: |  spotbugs  |   0m 34s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | +0 :ok: |  spotbugs  |   0m 17s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 20s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 29s |  |  hadoop-auth in the patch passed.  |\r\n   | -1 :x: |  unit  | 118m 15s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in the patch failed.  |\r\n   | -1 :x: |  unit  | 139m 46s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 51s |  |  hadoop-yarn-server-timelineservice-documentstore in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 15s |  |  hadoop-yarn-server-globalpolicygenerator in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 48s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | -1 :x: |  unit  |   0m 21s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt) |  hadoop-yarn-ui in the patch failed.  |\r\n   | +1 :green_heart: |  unit  |   3m  4s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 50s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 517m 52s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7935 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 1742a3955488 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1d5fb3ced9d16f8c847312dfd39d90d69a3f608e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/testReport/ |\r\n   | Max. process+thread count | 1134 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3262843831\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 42s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  5s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 54s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   6m 15s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 23s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 34s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   0m 34s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | -1 :x: |  compile  |  12m 54s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |  12m 54s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |  11m 40s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |  11m 40s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   5m 19s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 40s | [/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 24s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  9s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 35s |  |  hadoop-project has no data from spotbugs  |\r\n   | -1 :x: |  spotbugs  |   0m 39s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 31s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 22s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 28s |  |  hadoop-auth in the patch passed.  |\r\n   | -1 :x: |  unit  | 117m 33s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in the patch failed.  |\r\n   | -1 :x: |  unit  | 141m 22s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 55s |  |  hadoop-yarn-server-timelineservice-documentstore in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 14s |  |  hadoop-yarn-server-globalpolicygenerator in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 47s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | +1 :green_heart: |  unit  |   2m 10s |  |  hadoop-yarn-ui in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 19s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  9s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 542m 48s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapreduce.v2.TestUberAM |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7935 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 2cc4b9991723 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5eec2aa52aaafe54638d427ab3943557b7660e21 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/testReport/ |\r\n   | Max. process+thread count | 1143 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3262979489\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 37s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  40m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  19m  1s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 31s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   5m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 40s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 23s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  43m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   4m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  18m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 58s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  16m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 43s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 24s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 39s |  |  hadoop-auth in the patch passed.  |\r\n   | -1 :x: |  unit  | 118m 36s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in the patch failed.  |\r\n   | -1 :x: |  unit  | 142m 36s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 55s |  |  hadoop-yarn-server-timelineservice-documentstore in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 16s |  |  hadoop-yarn-server-globalpolicygenerator in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 10s |  |  hadoop-yarn-applications-catalog-webapp in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m  2s |  |  hadoop-yarn-ui in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 18s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 17s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 557m 11s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapreduce.v2.TestUberAM |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7935 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux fd48f302fc58 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2d02b1f3029f4fa6964b28470c69a5d603dd995a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/testReport/ |\r\n   | Max. process+thread count | 1130 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "szetszwo commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3263174197\n\n   The failed tests seem not related.\n\n\n", "slfan1989 commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3263559296\n\n   @szetszwo Thank you for your contribution! LGTM.\n\n\n", "szetszwo merged PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935\n\n\n", "szetszwo commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3263885757\n\n   @slfan1989 , thanks a lot for reviewing this!\n\n\n", "The pull request was merged.  Resolving ..."], "labels": ["pull-request-available"], "summary": "- The mockito-all 1", "qna": [{"question": "What is the issue title?", "answer": "[JDK17] Remove mockito-all 1.10.19 and powermock"}, {"question": "Who reported this issue?", "answer": "Tsz-wo Sze"}]}
{"key": "HADOOP-19676", "project": "HADOOP", "title": "ABFS: Enhancing ABFS Driver Metrics for Analytical Usability", "status": "Open", "reporter": "Manish Bhatt", "created": "2025-09-04T16:09:30.000+0000", "description": null, "comments": [], "labels": [], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "ABFS: Enhancing ABFS Driver Metrics for Analytical Usability"}, {"question": "Who reported this issue?", "answer": "Manish Bhatt"}]}
{"key": "HADOOP-19675", "project": "HADOOP", "title": "Close stale PRs updated over 100 days ago.", "status": "Resolved", "reporter": "Xiaoqiao He", "created": "2025-09-04T07:59:48.000+0000", "description": "Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh", "comments": ["Hexiaoqiao opened a new pull request, #7930:\nURL: https://github.com/apache/hadoop/pull/7930\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   This PR adds a GitHub workflow to automatically close stale PRs which have no activity over 100 days.\r\n   \r\n   ### How was this patch tested?\r\n   No.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [Y] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [N] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [N] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [Y] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3252923998\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  yamllint  |   0m  0s |  |  yamllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  44m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/artifact/out/blanks-eol.txt) |  The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  88m 49s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7930 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets yamllint |\r\n   | uname | Linux 8be7cf7a0c05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5e03454c04c43efa24691ef3d9245d9333a46e70 |\r\n   | Max. process+thread count | 536 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3253473455\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  yamllint  |   0m  0s |  |  yamllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  44m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  87m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7930 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets yamllint |\r\n   | uname | Linux 013fcfcc6e1b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bae392d8dfb7f2edb9de956122a07626d8802b20 |\r\n   | Max. process+thread count | 531 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "Hexiaoqiao commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3257436978\n\n   Thanks @slfan1989 .\r\n   \r\n   ping @ayushtkn @KeeProMise @ahmarsuhail and other guys, any more suggestions here? Thanks.\n\n\n", "ahmarsuhail commented on code in PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#discussion_r2324404204\n\n\n##########\n.github/workflows/stale.yml:\n##########\n@@ -0,0 +1,44 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+name: Close stale PRs\n+on:\n+  schedule:\n+    - cron: \"0 0 * * *\"\n+\n+jobs:\n+  stale:\n+    if: github.repository == 'apache/hadoop'\n+    runs-on: ubuntu-latest\n+    steps:\n+      - uses: actions/stale@v1\n+        with:\n+          stale-pr-message: >\n+            We're closing this stale PR because it has been open 100 days with\n\nReview Comment:\n   nit: open for 100 days\n\n\n\n", "hadoop-yetus commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3264681771\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 59s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  yamllint  |   0m  0s |  |  yamllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  46m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 46s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  89m 40s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7930 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets yamllint |\r\n   | uname | Linux c705d2f588b2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fbadafe3b09613b1566c16d9c0b7518ad1ec53e0 |\r\n   | Max. process+thread count | 525 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "Hexiaoqiao commented on code in PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#discussion_r2329238358\n\n\n##########\n.github/workflows/stale.yml:\n##########\n@@ -0,0 +1,44 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+name: Close stale PRs\n+on:\n+  schedule:\n+    - cron: \"0 0 * * *\"\n+\n+jobs:\n+  stale:\n+    if: github.repository == 'apache/hadoop'\n+    runs-on: ubuntu-latest\n+    steps:\n+      - uses: actions/stale@v1\n+        with:\n+          stale-pr-message: >\n+            We're closing this stale PR because it has been open 100 days with\n\nReview Comment:\n   Thanks. Fixed.\n\n\n\n", "Hexiaoqiao merged PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930\n\n\n", "Hexiaoqiao commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3264769501\n\n   Committed. Thanks all.\n\n\n", "Hexiaoqiao opened a new pull request, #7943:\nURL: https://github.com/apache/hadoop/pull/7943\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   This PR adds a GitHub workflow to automatically close stale PRs which have no activity over 100 days, linked https://github.com/apache/hadoop/pull/7930.\r\n   \r\n   Addendum. add github token.\r\n   \r\n   ### How was this patch tested?\r\n   No.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7943:\nURL: https://github.com/apache/hadoop/pull/7943#issuecomment-3268690888\n\n   LGTM\n\n\n", "KeeProMise commented on PR #7943:\nURL: https://github.com/apache/hadoop/pull/7943#issuecomment-3268702731\n\n   LGTM.\n\n\n", "hadoop-yetus commented on PR #7943:\nURL: https://github.com/apache/hadoop/pull/7943#issuecomment-3268837255\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  yamllint  |   0m  0s |  |  yamllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  45m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 53s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  88m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7943/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7943 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets yamllint |\r\n   | uname | Linux 5223952659a9 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 612de4cfeddb2b582bed25abb524eec88f366245 |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7943/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "Hexiaoqiao merged PR #7943:\nURL: https://github.com/apache/hadoop/pull/7943\n\n\n", "Hexiaoqiao commented on PR #7943:\nURL: https://github.com/apache/hadoop/pull/7943#issuecomment-3268849533\n\n   Committed. Thanks @slfan1989 and @KeeProMise .\n\n\n", "ahmarsuhail commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3298194311\n\n   Hey @Hexiaoqiao, just curious, number of open PRs is still 1.1K, is that expected?\n\n\n", "Hexiaoqiao commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3298596092\n\n   Thanks @ahmarsuhail , Yes, it works fine from my side. ref: https://github.com/apache/hadoop/actions/workflows/stale.yml\r\n   It will keep for a long time because rate limit. I didn't dig if the rate limit could be tuning or disable. Any thought?\n\n\n"], "labels": ["pull-request-available"], "summary": "Close stale PRs on GitHub which updated over 100 days ago, base on the voting th", "qna": [{"question": "What is the issue title?", "answer": "Close stale PRs updated over 100 days ago."}, {"question": "Who reported this issue?", "answer": "Xiaoqiao He"}]}
{"key": "HADOOP-19674", "project": "HADOOP", "title": "[JDK 17] Implementation of JAXB-API has not been found on module path or classpath", "status": "Resolved", "reporter": "Bence Kosztolnik", "created": "2025-09-03T11:30:33.000+0000", "description": "When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors:\r\n\r\n{noformat}\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n{noformat}\r\n\r\nRepro steps:\r\n- run: ./start-build-env.sh ubuntu_24\r\n- in docker run: mvn clean install -Pjdk17+ -T 32 -DskipTests \r\n- in hadoop-yarn-common modul run: mvn test -Dtest=org.apache.hadoop.yarn.webapp.TestWebApp\r\n\r\nI found a similar error here:\r\nhttps://issues.apache.org/jira/browse/HDDS-5068\r\n\r\nBased on my understanding the problem is the JDK11+ environments does not have the JAXB runtime, so we have to explicit provide them. Maybe this is just a temporal solution, till the whole Jakarta upgrade can be done.\r\n\r\n{panel:title=Full error log}\r\n{code}\r\n[ERROR] testRobotsText  Time elapsed: 0.064 s  <<< ERROR!\r\norg.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\r\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:506)\r\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:492)\r\n\tat org.apache.hadoop.yarn.webapp.TestWebApp.testRobotsText(TestWebApp.java:324)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)\r\n\tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\r\n\tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\r\n\tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\r\n\tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\r\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\r\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\r\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\r\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\r\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\r\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)\r\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\r\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\r\n\tat org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\r\n\tat org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)\r\n\tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142)\r\n\tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:113)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\nCaused by: java.io.IOException: Unable to initialize WebAppContext\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1453)\r\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:503)\r\n\t... 71 more\r\nCaused by: javax.servlet.ServletException: org.glassfish.jersey.servlet.ServletContainer-640d604==org.glassfish.jersey.servlet.ServletContainer@f679d7ba{jsp=null,order=-1,inst=true,async=true,src=EMBEDDED:null,STARTED}\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:650)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initialize(ServletHolder.java:415)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:750)\r\n\tat java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)\r\n\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)\r\n\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\r\n\tat java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)\r\n\tat java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)\r\n\tat java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)\r\n\tat org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1449)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1414)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)\r\n\tat org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:524)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)\r\n\tat org.eclipse.jetty.server.handler.StatisticsHandler.doStart(StatisticsHandler.java:264)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)\r\n\tat org.eclipse.jetty.server.Server.start(Server.java:423)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)\r\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:387)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1416)\r\n\t... 72 more\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n\r\n\tat org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:368)\r\n\tat org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)\r\n\tat org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:59)\r\n\tat org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:47)\r\n\tat org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture$1.call(Cache.java:74)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture.run(Cache.java:131)\r\n\tat org.glassfish.hk2.utilities.cache.Cache.compute(Cache.java:176)\r\n\tat org.jvnet.hk2.internal.SingletonContext.findOrCreate(SingletonContext.java:98)\r\n\tat org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)\r\n\tat org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetAllServiceHandles(ServiceLocatorImpl.java:1481)\r\n\tat org.jvnet.hk2.internal.ServiceLocatorImpl.getAllServices(ServiceLocatorImpl.java:799)\r\n\tat org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getAllInstances(AbstractHk2InjectionManager.java:171)\r\n\tat org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getAllInstances(ImmediateHk2InjectionManager.java:30)\r\n\tat org.glassfish.jersey.internal.ContextResolverFactory$ContextResolversConfigurator.postInit(ContextResolverFactory.java:69)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.lambda$initialize$2(ApplicationHandler.java:353)\r\n\tat java.base/java.util.Arrays$ArrayList.forEach(Arrays.java:4204)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.initialize(ApplicationHandler.java:353)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.lambda$initialize$1(ApplicationHandler.java:297)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\r\n\tat org.glassfish.jersey.internal.Errors.processWithException(Errors.java:232)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.initialize(ApplicationHandler.java:296)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.<init>(ApplicationHandler.java:261)\r\n\tat org.glassfish.jersey.servlet.WebComponent.<init>(WebComponent.java:314)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:154)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:360)\r\n\tat javax.servlet.GenericServlet.init(GenericServlet.java:244)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:632)\r\n\t... 104 more\r\nCaused by: javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n - with linked exception:\r\n[java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory]\r\n\tat javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:232)\r\n\tat javax.xml.bind.ContextFinder.find(ContextFinder.java:375)\r\n\tat javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:691)\r\n\tat javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:632)\r\n\tat org.glassfish.jersey.jettison.JettisonJaxbContext.<init>(JettisonJaxbContext.java:73)\r\n\tat org.glassfish.jersey.jettison.JettisonJaxbContext.<init>(JettisonJaxbContext.java:54)\r\n\tat org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver.<init>(MyTestJAXBContextResolver.java:45)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat org.glassfish.hk2.utilities.reflection.ReflectionHelper.makeMe(ReflectionHelper.java:1356)\r\n\tat org.jvnet.hk2.internal.ClazzCreator.createMe(ClazzCreator.java:248)\r\n\tat org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:342)\r\n\t... 132 more\r\nCaused by: java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory\r\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:538)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat javax.xml.bind.ServiceLoaderUtil.nullSafeLoadClass(ServiceLoaderUtil.java:92)\r\n\tat javax.xml.bind.ServiceLoaderUtil.safeLoadClass(ServiceLoaderUtil.java:125)\r\n\tat javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:230)\r\n\t... 146 more\r\n{code}\r\n{panel}\r\n\r\n", "comments": ["Thank you. I was able to repro the problem\r\n\r\nDo you plan to provide a patch [~bkosztolnik] ?", "K0K0V0K opened a new pull request, #7928:\nURL: https://github.com/apache/hadoop/pull/7928\n\n   https://issues.apache.org/jira/browse/HADOOP-19674\r\n   \r\n   ### Description of PR\r\n   \r\n   When we try to create an instance of `JettisonJaxbContext` a `JAXBContext.newInstance` call will happen. With my understanding JAXB is removed since JDK11 ([source](https://docs.oracle.com/en/java/javase/24/migrate/removed-tools-and-components.html#GUID-11F78105-D735-430D-92DD-6C37958FCBC3)) and if we would like to access JAXB, then we should explicit include them to the jars. So a new **jaxb-runtime** dependency is included in the code base. I used test scope where ever it was possible but for example in the `MRClientService` the `JAXBContextResolver` is in use, so i left the test scope there.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Unit tests\r\n   - I created a JDK8 build and run the `org.apache.hadoop.yarn.webapp.TestWebApp` test\r\n   - I created a JDK17 build and run the` org.apache.hadoop.yarn.webapp.TestWebApp` test\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "Hi [~stoty]!\r\n\r\nYes, just now.\r\nI am not 100% sure this is a good solution in point of architecture, security or long term maintenance, so i am grateful to every review.\r\nThanks!  ", "I have looked into this a bit:\r\n\r\nIn 3.4, jaxb-impl is a transitive dependency of jersey-json:\r\n\r\n{noformat}\r\n[INFO] +- com.github.pjfanning:jersey-json:jar:1.22.0:compile\r\n[INFO] |  \\- com.sun.xml.bind:jaxb-impl:jar:2.2.3-1:compile\r\n[INFO] |     \\- javax.xml.bind:jaxb-api:jar:2.2.11:compile\r\n{noformat}\r\n\r\nIn 3.5 Jersey has been upgraded, and we've lost the transitive dependency.\r\n\r\nHadoop does use java.xml.bind, so in the modules where it's used it should be added as a compile / test dependency (depending on where it is used).\r\n\r\nIt is also required by Jetty (and possibly other libraries) so it may need to be added to some modules that do not directly use javax.bind.\r\n\r\nYes, at some point we should update to Jakarta, but that is a future issue, IMO adding the dependency now is fine.", "stoty commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2319677719\n\n\n##########\nhadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/pom.xml:\n##########\n@@ -144,6 +144,11 @@\n       <artifactId>jersey-media-jaxb</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n\nReview Comment:\n   How did you decide where to add jaxb-imp and with what scope ?\n\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator/pom.xml:\n##########\n@@ -122,6 +122,12 @@\n       <artifactId>jersey-test-framework-provider-jetty</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.glassfish.jaxb</groupId>\n+      <artifactId>jaxb-runtime</artifactId>\n+      <version>${jaxb.version}</version>\n\nReview Comment:\n   The version should be set in once in the hadoop-project pom dependencyManagement section\n\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml:\n##########\n@@ -208,6 +208,12 @@\n       <artifactId>jersey-media-json-jettison</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.glassfish.jaxb</groupId>\n+      <artifactId>jaxb-runtime</artifactId>\n+      <version>${jaxb.version}</version>\n+      <scope>test</scope>\n\nReview Comment:\n   This is needed by Jetty. (at least)\r\n   This should be compile scope, as javax.xml.bind is directly used in the main code.\r\n   \n\n\n\n", "K0K0V0K commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2319820574\n\n\n##########\nhadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/pom.xml:\n##########\n@@ -144,6 +144,11 @@\n       <artifactId>jersey-media-jaxb</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n\nReview Comment:\n   I checked the usage of the `JettisonJaxbContext` in the projects where only used in the tests i added with test scope, otherwise i set no scope.\n\n\n\n", "K0K0V0K commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2319871051\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml:\n##########\n@@ -208,6 +208,12 @@\n       <artifactId>jersey-media-json-jettison</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.glassfish.jaxb</groupId>\n+      <artifactId>jaxb-runtime</artifactId>\n+      <version>${jaxb.version}</version>\n+      <scope>test</scope>\n\nReview Comment:\n   I was not sure this needed in every case for the Jetty.\r\n   But in that case if i see well we will need this every where where we are using the HttpServer2.\r\n   So if i see well this will be needed in HDFS NN also.\r\n   \r\n   In that case shall we add this to the just to the `hadoop-common` project and let others use it transitively?\n\n\n\n", "K0K0V0K commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2319871051\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml:\n##########\n@@ -208,6 +208,12 @@\n       <artifactId>jersey-media-json-jettison</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.glassfish.jaxb</groupId>\n+      <artifactId>jaxb-runtime</artifactId>\n+      <version>${jaxb.version}</version>\n+      <scope>test</scope>\n\nReview Comment:\n   I was not sure this needed in every case for the Jetty.\r\n   But in that case if i see well we will need this every where where we are using the HttpServer2.\r\n   So if i see well this will be needed in HDFS NN also.\r\n   \r\n   In that case shall we add this to the `hadoop-common` project only and let others use it transitively?\n\n\n\n", "susheelgupta7 commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3252006029\n\n   Build issue observed on JDK 8\r\n   The Maven enforcer plugin failed due to a dependency convergence conflict for jaxb-runtime:\r\n   \r\n   ```java\r\n   [ERROR] Rule 0: org.apache.maven.enforcer.rules.dependency.DependencyConvergence failed with message:\r\n   [ERROR] Failed while enforcing releasability.\r\n   [ERROR] \r\n   [ERROR] Dependency convergence error for org.glassfish.jaxb:jaxb-runtime:jar:2.3.1 paths to dependency are:\r\n   [ERROR] +-org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.5.0-SNAPSHOT\r\n   [ERROR]   +-org.apache.hadoop:hadoop-yarn-server-web-proxy:jar:3.5.0-SNAPSHOT:compile\r\n   [ERROR]     +-org.apache.hadoop:hadoop-yarn-server-common:jar:3.5.0-SNAPSHOT:compile\r\n   [ERROR]       +-org.ehcache:ehcache:jar:3.8.2:compile\r\n   [ERROR]         +-org.glassfish.jaxb:jaxb-runtime:jar:2.3.1:compile\r\n   [ERROR] and\r\n   [ERROR] +-org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.5.0-SNAPSHOT\r\n   [ERROR]   +-org.glassfish.jaxb:jaxb-runtime:jar:2.3.9:compile\r\n   ``` \n\n\n", "stoty commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2321030300\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml:\n##########\n@@ -208,6 +208,12 @@\n       <artifactId>jersey-media-json-jettison</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.glassfish.jaxb</groupId>\n+      <artifactId>jaxb-runtime</artifactId>\n+      <version>${jaxb.version}</version>\n+      <scope>test</scope>\n\nReview Comment:\n   > I was not sure this needed in every case for the Jetty.\r\n   \r\n   The stack strace seems to be coming from Jetty initialization.\r\n   \r\n   > But in that case if i see well we will need this every where where we are using the HttpServer2. So if i see well this will be needed in HDFS NN also.\r\n   \r\n   Yes, that's likely.\r\n   \r\n   > \r\n   > In that case shall we add this to the `hadoop-common` project only and let others use it transitively?\r\n   \r\n   I think it's better to add it separately where needed.\r\n   Most modules don't use Jetty.\n\n\n\n", "stoty commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2321033382\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml:\n##########\n@@ -208,6 +208,12 @@\n       <artifactId>jersey-media-json-jettison</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.glassfish.jaxb</groupId>\n+      <artifactId>jaxb-runtime</artifactId>\n+      <version>${jaxb.version}</version>\n+      <scope>test</scope>\n\nReview Comment:\n   One other issue is the binary assembly\r\n   \r\n   We also need to make sure that jaxb-impl is included there.\n\n\n\n", "K0K0V0K commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3252296131\n\n   > Build issue observed on JDK 8 The Maven enforcer plugin failed due to a dependency convergence conflict for jaxb-runtime:\r\n   > \r\n   > ```java\r\n   > [ERROR] Rule 0: org.apache.maven.enforcer.rules.dependency.DependencyConvergence failed with message:\r\n   > [ERROR] Failed while enforcing releasability.\r\n   > [ERROR] \r\n   > [ERROR] Dependency convergence error for org.glassfish.jaxb:jaxb-runtime:jar:2.3.1 paths to dependency are:\r\n   > [ERROR] +-org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.5.0-SNAPSHOT\r\n   > [ERROR]   +-org.apache.hadoop:hadoop-yarn-server-web-proxy:jar:3.5.0-SNAPSHOT:compile\r\n   > [ERROR]     +-org.apache.hadoop:hadoop-yarn-server-common:jar:3.5.0-SNAPSHOT:compile\r\n   > [ERROR]       +-org.ehcache:ehcache:jar:3.8.2:compile\r\n   > [ERROR]         +-org.glassfish.jaxb:jaxb-runtime:jar:2.3.1:compile\r\n   > [ERROR] and\r\n   > [ERROR] +-org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.5.0-SNAPSHOT\r\n   > [ERROR]   +-org.glassfish.jaxb:jaxb-runtime:jar:2.3.9:compile\r\n   > ```\r\n   \r\n   Thanks @susheelgupta7 for the finding!\n\n\n", "K0K0V0K commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2322802024\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml:\n##########\n@@ -208,6 +208,12 @@\n       <artifactId>jersey-media-json-jettison</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.glassfish.jaxb</groupId>\n+      <artifactId>jaxb-runtime</artifactId>\n+      <version>${jaxb.version}</version>\n+      <scope>test</scope>\n\nReview Comment:\n   I tried to upload a corrected version.\r\n   With my understanding every Hadoop based Jetty server is instance of the `HttpServer2`, what is in the `hadoop-common`, so i added the dependency there.\r\n   \r\n   Some other modules what use` hadoop-common`, but not the Jetty server are already excludes the `jetty-server`.\r\n   I extended these cases with `jaxb-runtime` exculsion. This means: not 100% these modules dont need `jaxb-runtime`, but pretty sure not because the Jetty.\r\n   \r\n   In the httpfs modul i dont know why but seems like the `jetty-server` transitive dependency is excluded from `hadoop-common`, but there is an explicit dependency for it also in the pom.\r\n   \r\n   I run a clean install with -Pdist and i can see the jaxb-runtime jar in the `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/jaxb-runtime-2.3.9.jar` path\n\n\n\n", "stoty commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2322987219\n\n\n##########\nhadoop-client-modules/hadoop-client/pom.xml:\n##########\n@@ -53,6 +53,10 @@\n           <groupId>org.eclipse.jetty</groupId>\n           <artifactId>jetty-server</artifactId>\n         </exclusion>\n+        <exclusion>\n\nReview Comment:\n   These are for java 8, right ?\n\n\n\n##########\nhadoop-client-modules/hadoop-client/pom.xml:\n##########\n@@ -53,6 +53,10 @@\n           <groupId>org.eclipse.jetty</groupId>\n           <artifactId>jetty-server</artifactId>\n         </exclusion>\n+        <exclusion>\n\nReview Comment:\n   These are for java 8, right ?\n\n\n\n", "stoty commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2323018571\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -2152,6 +2155,11 @@\n         <artifactId>jersey-media-json-jettison</artifactId>\n         <version>${jersey2.version}</version>\n       </dependency>\n+      <dependency>\n\nReview Comment:\n   Since we're globally dependency managing jaxb-runtime, I think that we don't need all the excludes in this patch.\r\n   They should all be dependency managed to 2.3.9 by maven, and the dependency convergence check should pass.\n\n\n\n", "stoty commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2323027545\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -2152,6 +2155,11 @@\n         <artifactId>jersey-media-json-jettison</artifactId>\n         <version>${jersey2.version}</version>\n       </dependency>\n+      <dependency>\n\nReview Comment:\n   I think that the reported error was for the earlier patch without the global dependency management.\n\n\n\n", "hadoop-yetus commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3256455260\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 20s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m 54s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 43s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   8m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   7m 13s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   6m 15s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 150m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 40s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   4m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  18m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  16m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   7m  7s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   6m 11s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  65m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 31s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  24m  7s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 52s |  |  hadoop-kms in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   6m  0s |  |  hadoop-hdfs-httpfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   6m 16s |  |  hadoop-yarn-server-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 35s |  |  hadoop-yarn-server-web-proxy in the patch passed.  |\r\n   | -1 :x: |  unit  | 145m 56s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/2/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 38s |  |  hadoop-yarn-services-api in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 53s |  |  hadoop-client in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 451m 50s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapreduce.v2.TestUberAM |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7928 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 4eb1be1e663a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6086bd5ba19b4e4de56b0eba69be70962117a85a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/2/testReport/ |\r\n   | Max. process+thread count | 1273 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3257284211\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m  4s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  42m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m 38s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  24m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  11m  2s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   9m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  62m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 37s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  42m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  19m 10s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  19m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 22s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  16m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  21m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |  11m  2s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 13s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  62m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 488m 32s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/3/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 15s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 831m 35s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.TestRollingUpgrade |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7928 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux a9562a3960c9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 450d122aa0923580d4ddfe2b8ad9a734265fe7d9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/3/testReport/ |\r\n   | Max. process+thread count | 2197 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "K0K0V0K commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2324319353\n\n\n##########\nhadoop-client-modules/hadoop-client/pom.xml:\n##########\n@@ -53,6 +53,10 @@\n           <groupId>org.eclipse.jetty</groupId>\n           <artifactId>jetty-server</artifactId>\n         </exclusion>\n+        <exclusion>\n+          <groupId>org.glassfish.jaxb</groupId>\n+          <artifactId>jaxb-runtime</artifactId>\n+        </exclusion>\n\nReview Comment:\n   I think this one is necessary otherwise at the end on install i am facing this error:\r\n   \r\n   ```\r\n   [INFO] Total time:  05:20 min (Wall Clock)\r\n   [INFO] Finished at: 2025-09-05T09:02:10+02:00\r\n   [INFO] ------------------------------------------------------------------------\r\n   [ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.5.0:enforce (enforce-banned-dependencies) on project hadoop-client-check-test-invariants: \r\n   [ERROR] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message:\r\n   [ERROR] Duplicate classes found:\r\n   [ERROR] \r\n   [ERROR]   Found in:\r\n   [ERROR]     org.apache.hadoop:hadoop-client-minicluster:jar:3.5.0-SNAPSHOT:compile\r\n   [ERROR]     org.apache.hadoop:hadoop-client-runtime:jar:3.5.0-SNAPSHOT:compile\r\n   [ERROR]   Duplicate classes:\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/registries/MimeTypeEntry.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/localization/LocalizableMessageFactory$ResourceBundleSupplier.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/registries/MailcapTokenizer.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/FinalArrayList.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/Pool$Impl.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/registries/MailcapParseException.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/registries/LogSupport.class\r\n   [ERROR]     META-INF/versions/9/com/sun/istack/logging/StackHelper.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/NotNull.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/XMLStreamException2.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/viewers/TextEditor.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/XMLStreamReaderToContentHandler$1.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/localization/Localizable.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/viewers/ImageViewerCanvas.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/viewers/TextViewer.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/Interned.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/localization/Localizer.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/localization/LocalizableMessageFactory.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/registries/MailcapFile.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/logging/Logger.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/ByteArrayDataSource.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/Nullable.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/localization/NullLocalizable.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/SAXException2.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/registries/MimeTypeFile.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/logging/StackHelper.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/registries/LineTokenizer.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/Builder.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/FragmentContentHandler.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/Pool.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/localization/LocalizableMessage.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/activation/viewers/ImageViewer.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/SAXParseException2.class\r\n   [ERROR]     org/apache/hadoop/shaded/com/sun/istack/XMLStreamReaderToContentHandler.class\r\n   ```\n\n\n\n", "stoty commented on code in PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#discussion_r2324475248\n\n\n##########\nhadoop-client-modules/hadoop-client/pom.xml:\n##########\n@@ -53,6 +53,10 @@\n           <groupId>org.eclipse.jetty</groupId>\n           <artifactId>jetty-server</artifactId>\n         </exclusion>\n+        <exclusion>\n+          <groupId>org.glassfish.jaxb</groupId>\n+          <artifactId>jaxb-runtime</artifactId>\n+        </exclusion>\n\nReview Comment:\n   I have checked 3.4.2, and these classes are not present there.\r\n   Based on that, it seems that hadoop-client-runtime indeed does not include jaxb-impl traditionally.\r\n   \r\n   \n\n\n\n", "hadoop-yetus commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3259566698\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 17s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |   5m 15s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |  17m 32s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m 30s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m  7s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 13s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  56m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 37s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  36m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  1s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 40s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  14m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |  10m 38s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 20s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  61m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 446m 39s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 10s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 744m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7928 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux a22a22156e0a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c5a26a7263c9a5c356216b6426d5fe903213cd15 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/testReport/ |\r\n   | Max. process+thread count | 3607 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3259596835\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 56s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 56s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  40m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  19m 53s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 46s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 51s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  58m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 37s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  37m 21s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |  17m 10s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 11s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  21m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   9m 54s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   7m 32s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |  61m 30s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 480m  5s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 14s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 810m  1s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |\r\n   |   | hadoop.hdfs.server.namenode.TestNameNodeMXBean |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7928 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux bd7276601e6c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 835934188aaa69fac727bbe178075cadfc87e0a9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/testReport/ |\r\n   | Max. process+thread count | 2322 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3259909698\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 51s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  37m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  19m 47s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  17m 16s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  24m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  11m 10s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  59m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 35s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  35m 24s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |  18m  7s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  18m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  17m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  1s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |  11m 51s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   8m 21s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |  63m  0s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 611m 20s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 944m 54s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7928 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux f7e642f299bc 5.15.0-151-generic #161-Ubuntu SMP Tue Jul 22 14:25:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 835934188aaa69fac727bbe178075cadfc87e0a9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/testReport/ |\r\n   | Max. process+thread count | 3138 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3263251404\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 46s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  37m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 21s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 43s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  58m 26s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 37s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  41m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 44s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 13s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |  10m  6s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 45s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  59m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 481m 54s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 15s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 824m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7928 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 117987a87daa 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7d1831a9a0ae7b1f73edfce600e8a818b5ec5de6 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/testReport/ |\r\n   | Max. process+thread count | 3135 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3263561534\n\n   @K0K0V0K Thank you for your contribution! LGTM. However, we need to trigger the compilation again as the current process seems to have been interrupted.\r\n   \r\n   I also noticed some warning messages. Could you please double-check?\r\n   \r\n   \r\n   ```\r\n   [WARNING] \r\n   \r\n   Some dependencies of Maven Plugins are expected to be in provided scope.\r\n   Please make sure that dependencies listed below declared in POM\r\n   have set '<scope>provided</scope>' as well.\r\n   \r\n   The following dependencies are in wrong scope:\r\n    * org.apache.maven:maven-plugin-api:jar:3.9.5:compile\r\n    * org.apache.maven:maven-model:jar:3.9.5:compile\r\n    * org.apache.maven:maven-artifact:jar:3.9.5:compile\r\n    * org.apache.maven:maven-core:jar:3.9.5:compile\r\n    * org.apache.maven:maven-settings:jar:3.9.5:compile\r\n    * org.apache.maven:maven-settings-builder:jar:3.9.5:compile\r\n    * org.apache.maven:maven-builder-support:jar:3.9.5:compile\r\n    * org.apache.maven:maven-repository-metadata:jar:3.9.5:compile\r\n    * org.apache.maven:maven-model-builder:jar:3.9.5:compile\r\n    * org.apache.maven:maven-resolver-provider:jar:3.9.5:compile\r\n   ```\r\n   \n\n\n", "K0K0V0K commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3265388602\n\n   Hi @slfan1989!\r\n   \r\n   Thanks for the review. I checked the latest console, output and seems like the waring is not present anymore:\r\n   https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/consoleFull\n\n\n", "hadoop-yetus commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3268664561\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 51s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 56s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 29s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  23m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 16s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 46s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  58m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 39s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  41m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m  0s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  18m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 40s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |  10m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 44s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  59m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 656m 49s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/9/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 17s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 980m 24s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.TestRollingUpgrade |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7928 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux ca1384aa87d7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 56b01d2c3fd0ade4f966d40068b9fdc9c53dc7ff |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/9/testReport/ |\r\n   | Max. process+thread count | 3135 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/9/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "brumi1024 merged PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928\n\n\n", "K0K0V0K commented on PR #7928:\nURL: https://github.com/apache/hadoop/pull/7928#issuecomment-3270292832\n\n   Thanks! @stoty @slfan1989 @brumi1024 @susheelgupta7 for the review and for the help here!\n\n\n"], "labels": ["pull-request-available"], "summary": "When i try to run the *org", "qna": [{"question": "What is the issue title?", "answer": "[JDK 17] Implementation of JAXB-API has not been found on module path or classpath"}, {"question": "Who reported this issue?", "answer": "Bence Kosztolnik"}]}
{"key": "HADOOP-19673", "project": "HADOOP", "title": "BloomMapFile: invalid io.mapfile.bloom.error.rate (\u22640 or \u22651) causes NaN/zero vector size and writer construction failure", "status": "Patch Available", "reporter": "AMC-team", "created": "2025-09-02T14:20:00.000+0000", "description": "{{BloomMapFile.Writer#initBloomFilter(Configuration)}} computes the Bloom filter vector size as:\r\n{code:java}\r\nint numKeys = conf.getInt(IO_MAPFILE_BLOOM_SIZE_KEY, IO_MAPFILE_BLOOM_SIZE_DEFAULT);\r\nfloat errorRate = conf.getFloat(IO_MAPFILE_BLOOM_ERROR_RATE_KEY, IO_MAPFILE_BLOOM_ERROR_RATE_DEFAULT);\r\nint vectorSize = (int) Math.ceil(\r\n\u00a0 (double)(-HASH_COUNT * numKeys) /\r\n\u00a0 Math.log(1.0 - Math.pow(errorRate, 1.0 / HASH_COUNT))\r\n); {code}\r\nWhen {{io.mapfile.bloom.error.rate}} is *\u2264 0*\u00a0or {*}\u2265 1{*}:\r\n * {{Math.pow(errorRate, 1/k)}} produces *NaN* (negative base with non-integer exponent) or an invalid value;\r\n * {{Math.log(1 - NaN)}} becomes {*}NaN{*};\r\n * {{Math.ceil(NaN)}} cast to {{int}} yields {*}0{*}, so {{{}vectorSize == 0{}}};\r\n * constructing {{DynamicBloomFilter}} subsequently fails, and {{BloomMapFile.Writer}} construction fails (observed as assertion failure in tests).\r\n\r\nThe code misses input validation for {{io.mapfile.bloom.error.rate}} which should be strictly within {{{}(0, 1){}}}. With invalid values, the math silently degrades to NaN/0 and fails at runtime.\r\n\r\n*Reproduction*\r\n\r\nInjected values: {{io.mapfile.bloom.error.rate = 0,-1}}\r\n\r\nTest: {{org.apache.hadoop.io.TestBloomMapFile#testBloomMapFileConstructors}}\r\n{code:java}\r\n[INFO] Running org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.358 s <<< FAILURE! - in org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors \u00a0Time elapsed: 0.272 s \u00a0<<< FAILURE!\r\njava.lang.AssertionError: testBloomMapFileConstructors error !!!\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors(TestBloomMapFile.java:287{code}", "comments": [], "labels": [], "summary": "{{BloomMapFile", "qna": [{"question": "What is the issue title?", "answer": "BloomMapFile: invalid io.mapfile.bloom.error.rate (\u22640 or \u22651) causes NaN/zero vector size and writer construction failure"}, {"question": "Who reported this issue?", "answer": "AMC-team"}]}
{"key": "HADOOP-19672", "project": "HADOOP", "title": "ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))", "status": "Open", "reporter": "Manish Bhatt", "created": "2025-09-01T10:54:52.000+0000", "description": null, "comments": ["bhattmanish98 opened a new pull request, #7967:\nURL: https://github.com/apache/hadoop/pull/7967\n\n   JIRA \u2013 https://issues.apache.org/jira/browse/HADOOP-19672\r\n   \r\n   In case of a network error while using the Apache client, we allow the client to switch over from Apache to JDK. This network fallback occurs in two scenarios:\r\n   \r\n   1. During file system initialization \u2013 When warming up the cache, if no connection is created (indicating an issue with the Apache client), the system will fall back to the JDK.\r\n   2. During a network call \u2013 If an I/O or Unknown Host exception occurs for three consecutive retries, the system will fall back to the JDK.\r\n   \r\n   This fallback is applied at the JVM level, so all file system calls will use the JDK client once the switch occurs.\r\n   \r\n   There is also a possibility of recovery. During cache warmup, if connections are successfully created using the Apache client, the system will automatically switch back to the Apache client.\n\n\n", "hadoop-yetus commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3293191749\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 36s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 50s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7967 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b7ef8b2a88ce 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1dad672faf42fbfb0ada2a95456b922a50b2becf |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/testReport/ |\r\n   | Max. process+thread count | 693 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3293931499\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  50m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  6s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 156m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7967 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f1f4dba85ce3 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6faa4c9c8ad8083e0a636b6439146a24c66c0ce3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/2/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anmolanmol1234 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2381728892\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java:\n##########\n@@ -73,18 +80,21 @@ static boolean usable() {\n   }\n \n   AbfsApacheHttpClient(DelegatingSSLSocketFactory delegatingSSLSocketFactory,\n-      final AbfsConfiguration abfsConfiguration, final KeepAliveCache keepAliveCache,\n-      URL baseUrl) {\n+      final AbfsConfiguration abfsConfiguration,\n+      final KeepAliveCache keepAliveCache,\n+      URL baseUrl,\n+      final boolean isCacheWarmupNeeded) {\n     final AbfsConnectionManager connMgr = new AbfsConnectionManager(\n         createSocketFactoryRegistry(\n             new SSLConnectionSocketFactory(delegatingSSLSocketFactory,\n                 getDefaultHostnameVerifier())),\n         new AbfsHttpClientConnectionFactory(), keepAliveCache,\n-        abfsConfiguration, baseUrl);\n+        abfsConfiguration, baseUrl, isCacheWarmupNeeded);\n     final HttpClientBuilder builder = HttpClients.custom();\n     builder.setConnectionManager(connMgr)\n         .setRequestExecutor(\n-            new AbfsManagedHttpRequestExecutor(abfsConfiguration.getHttpReadTimeout()))\n+            new AbfsManagedHttpRequestExecutor(\n\nReview Comment:\n   As we were discussing last time, if we keep it to read timeout the 100 continue timeout would become 30 seconds, this should be another config for 100 continue timeout\n\n\n\n", "anmolanmol1234 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2381745959\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl,\n \n       abfsApacheHttpClient = new AbfsApacheHttpClient(\n           DelegatingSSLSocketFactory.getDefaultFactory(),\n-          abfsConfiguration, keepAliveCache, baseUrl);\n+          abfsConfiguration, keepAliveCache, baseUrl,\n+          abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType);\n\nReview Comment:\n   Can you add some comments around this change as to how it would affect the need for cache warmup\n\n\n\n", "anmolanmol1234 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2381793492\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsConnectionManager.java:\n##########\n@@ -91,26 +92,34 @@ class AbfsConnectionManager implements HttpClientConnectionManager {\n   /**\n    * The base host for which connections are managed.\n    */\n-  private HttpHost baseHost;\n+  private final HttpHost baseHost;\n \n   AbfsConnectionManager(Registry<ConnectionSocketFactory> socketFactoryRegistry,\n-      AbfsHttpClientConnectionFactory connectionFactory, KeepAliveCache kac,\n-      final AbfsConfiguration abfsConfiguration, final URL baseUrl) {\n+      AbfsHttpClientConnectionFactory connectionFactory,\n+      KeepAliveCache kac,\n+      final AbfsConfiguration abfsConfiguration,\n+      final URL baseUrl,\n+      final boolean isCacheWarmupNeeded) {\n     this.httpConnectionFactory = connectionFactory;\n     this.kac = kac;\n     this.connectionOperator = new DefaultHttpClientConnectionOperator(\n         socketFactoryRegistry, null, null);\n     this.abfsConfiguration = abfsConfiguration;\n-    if (abfsConfiguration.getApacheCacheWarmupCount() > 0\n+    this.baseHost = new HttpHost(baseUrl.getHost(),\n+        baseUrl.getDefaultPort(), baseUrl.getProtocol());\n+    if (isCacheWarmupNeeded && abfsConfiguration.getApacheCacheWarmupCount() > 0\n         && kac.getFixedThreadPool() != null) {\n       // Warm up the cache with connections.\n       LOG.debug(\"Warming up the KeepAliveCache with {} connections\",\n           abfsConfiguration.getApacheCacheWarmupCount());\n-      this.baseHost = new HttpHost(baseUrl.getHost(),\n-          baseUrl.getDefaultPort(), baseUrl.getProtocol());\n       HttpRoute route = new HttpRoute(baseHost, null, true);\n-      cacheExtraConnection(route,\n+      int totalConnectionsCreated = cacheExtraConnection(route,\n           abfsConfiguration.getApacheCacheWarmupCount());\n+      if (totalConnectionsCreated == 0) {\n\nReview Comment:\n   even if we fail or catch rejected exception for any one of the tasks we want to register fallback as the successfully submitted tasks might have increased the count of totalConnectionsCreated\r\n   \n\n\n\n", "anmolanmol1234 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2381815558\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestApacheClientConnectionPool.java:\n##########\n@@ -118,6 +121,38 @@ public void testConnectedConnectionLogging() throws Exception {\n         .isEqualTo(4);\n   }\n \n+  /**\n+   * Test to verify that the ApacheHttpClient falls back to JDK client\n+   * when connection warmup fails.\n+   * This test is applicable only for ApacheHttpClient.\n+   */\n+  @Test\n+  public void testApacheClientFallbackDuringConnectionWarmup()\n+      throws Exception {\n+    try (KeepAliveCache keepAliveCache = new KeepAliveCache(\n+        new AbfsConfiguration(new Configuration(), EMPTY_STRING))) {\n+      // Create a connection manager with invalid URL to force fallback to JDK client\n+      // during connection warmup.\n+      // This is to simulate failure during connection warmup in the connection manager.\n+      // The invalid URL will cause the connection manager to fail to create connections\n+      // during warmup, forcing it to fall back to JDK client.\n+      final AbfsConnectionManager connMgr = new AbfsConnectionManager(\n+          RegistryBuilder.<ConnectionSocketFactory>create()\n+              .register(HTTPS_SCHEME, new SSLConnectionSocketFactory(\n+                  DelegatingSSLSocketFactory.getDefaultFactory(),\n+                  getDefaultHostnameVerifier()))\n+              .build(),\n+          new AbfsHttpClientConnectionFactory(), keepAliveCache,\n+          new AbfsConfiguration(new Configuration(), EMPTY_STRING),\n+          new URL(\"https://test.com\"), true);\n+\n+      Assertions.assertThat(AbfsApacheHttpClient.usable())\n+          .describedAs(\"Apache HttpClient should be not usable\")\n\nReview Comment:\n   Can you make one http call and validate now jdk is being used, user agent can be used for validation\n\n\n\n", "anujmodi2021 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2382005373\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java:\n##########\n@@ -68,13 +68,13 @@ public AbfsClientHandler(final URL baseUrl,\n       final SASTokenProvider sasTokenProvider,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n+    initServiceType(abfsConfiguration);\n\nReview Comment:\n   Why this change?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -188,7 +189,7 @@ public AbfsBlobClient(final URL baseUrl,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n     super(baseUrl, sharedKeyCredentials, abfsConfiguration, tokenProvider,\n-        encryptionContextProvider, abfsClientContext);\n+        encryptionContextProvider, abfsClientContext, AbfsServiceType.BLOB);\n\nReview Comment:\n   Why this change?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java:\n##########\n@@ -73,18 +80,21 @@ static boolean usable() {\n   }\n \n   AbfsApacheHttpClient(DelegatingSSLSocketFactory delegatingSSLSocketFactory,\n-      final AbfsConfiguration abfsConfiguration, final KeepAliveCache keepAliveCache,\n-      URL baseUrl) {\n+      final AbfsConfiguration abfsConfiguration,\n+      final KeepAliveCache keepAliveCache,\n+      URL baseUrl,\n+      final boolean isCacheWarmupNeeded) {\n     final AbfsConnectionManager connMgr = new AbfsConnectionManager(\n         createSocketFactoryRegistry(\n             new SSLConnectionSocketFactory(delegatingSSLSocketFactory,\n                 getDefaultHostnameVerifier())),\n         new AbfsHttpClientConnectionFactory(), keepAliveCache,\n-        abfsConfiguration, baseUrl);\n+        abfsConfiguration, baseUrl, isCacheWarmupNeeded);\n     final HttpClientBuilder builder = HttpClients.custom();\n     builder.setConnectionManager(connMgr)\n         .setRequestExecutor(\n-            new AbfsManagedHttpRequestExecutor(abfsConfiguration.getHttpReadTimeout()))\n+            new AbfsManagedHttpRequestExecutor(\n\nReview Comment:\n   +1\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl,\n \n       abfsApacheHttpClient = new AbfsApacheHttpClient(\n           DelegatingSSLSocketFactory.getDefaultFactory(),\n-          abfsConfiguration, keepAliveCache, baseUrl);\n+          abfsConfiguration, keepAliveCache, baseUrl,\n+          abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType);\n\nReview Comment:\n   +1\r\n   What are we trying to achieve here?\n\n\n\n", "anmolanmol1234 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2381793492\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsConnectionManager.java:\n##########\n@@ -91,26 +92,34 @@ class AbfsConnectionManager implements HttpClientConnectionManager {\n   /**\n    * The base host for which connections are managed.\n    */\n-  private HttpHost baseHost;\n+  private final HttpHost baseHost;\n \n   AbfsConnectionManager(Registry<ConnectionSocketFactory> socketFactoryRegistry,\n-      AbfsHttpClientConnectionFactory connectionFactory, KeepAliveCache kac,\n-      final AbfsConfiguration abfsConfiguration, final URL baseUrl) {\n+      AbfsHttpClientConnectionFactory connectionFactory,\n+      KeepAliveCache kac,\n+      final AbfsConfiguration abfsConfiguration,\n+      final URL baseUrl,\n+      final boolean isCacheWarmupNeeded) {\n     this.httpConnectionFactory = connectionFactory;\n     this.kac = kac;\n     this.connectionOperator = new DefaultHttpClientConnectionOperator(\n         socketFactoryRegistry, null, null);\n     this.abfsConfiguration = abfsConfiguration;\n-    if (abfsConfiguration.getApacheCacheWarmupCount() > 0\n+    this.baseHost = new HttpHost(baseUrl.getHost(),\n+        baseUrl.getDefaultPort(), baseUrl.getProtocol());\n+    if (isCacheWarmupNeeded && abfsConfiguration.getApacheCacheWarmupCount() > 0\n         && kac.getFixedThreadPool() != null) {\n       // Warm up the cache with connections.\n       LOG.debug(\"Warming up the KeepAliveCache with {} connections\",\n           abfsConfiguration.getApacheCacheWarmupCount());\n-      this.baseHost = new HttpHost(baseUrl.getHost(),\n-          baseUrl.getDefaultPort(), baseUrl.getProtocol());\n       HttpRoute route = new HttpRoute(baseHost, null, true);\n-      cacheExtraConnection(route,\n+      int totalConnectionsCreated = cacheExtraConnection(route,\n           abfsConfiguration.getApacheCacheWarmupCount());\n+      if (totalConnectionsCreated == 0) {\n\nReview Comment:\n   even if we fail or catch rejected exception for any one of the tasks we want to register fallback ? as the successfully submitted tasks might have increased the count of totalConnectionsCreated\r\n   \n\n\n\n", "manika137 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2387212747\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -214,7 +214,7 @@ public final class FileSystemConfigurations {\n   public static final long THOUSAND = 1000L;\n \n   public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY\n-      = HttpOperationType.JDK_HTTP_URL_CONNECTION;\n+      = HttpOperationType.APACHE_HTTP_CLIENT;\n\nReview Comment:\n   Nit: We should change this in our md file as well\n\n\n\n", "manika137 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2390726345\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -214,7 +214,7 @@ public final class FileSystemConfigurations {\n   public static final long THOUSAND = 1000L;\n \n   public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY\n-      = HttpOperationType.JDK_HTTP_URL_CONNECTION;\n+      = HttpOperationType.APACHE_HTTP_CLIENT;\n \n   public static final int DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES = 3;\n\nReview Comment:\n   where are we checking after 3 retries we'll fallback to JDK?\n\n\n\n", "manika137 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2390726345\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -214,7 +214,7 @@ public final class FileSystemConfigurations {\n   public static final long THOUSAND = 1000L;\n \n   public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY\n-      = HttpOperationType.JDK_HTTP_URL_CONNECTION;\n+      = HttpOperationType.APACHE_HTTP_CLIENT;\n \n   public static final int DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES = 3;\n\nReview Comment:\n   where are we checking after 3 retries we'll fallback to JDK?\n\n\n\n", "bhattmanish98 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2393513847\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java:\n##########\n@@ -73,18 +80,21 @@ static boolean usable() {\n   }\n \n   AbfsApacheHttpClient(DelegatingSSLSocketFactory delegatingSSLSocketFactory,\n-      final AbfsConfiguration abfsConfiguration, final KeepAliveCache keepAliveCache,\n-      URL baseUrl) {\n+      final AbfsConfiguration abfsConfiguration,\n+      final KeepAliveCache keepAliveCache,\n+      URL baseUrl,\n+      final boolean isCacheWarmupNeeded) {\n     final AbfsConnectionManager connMgr = new AbfsConnectionManager(\n         createSocketFactoryRegistry(\n             new SSLConnectionSocketFactory(delegatingSSLSocketFactory,\n                 getDefaultHostnameVerifier())),\n         new AbfsHttpClientConnectionFactory(), keepAliveCache,\n-        abfsConfiguration, baseUrl);\n+        abfsConfiguration, baseUrl, isCacheWarmupNeeded);\n     final HttpClientBuilder builder = HttpClients.custom();\n     builder.setConnectionManager(connMgr)\n         .setRequestExecutor(\n-            new AbfsManagedHttpRequestExecutor(abfsConfiguration.getHttpReadTimeout()))\n+            new AbfsManagedHttpRequestExecutor(\n\nReview Comment:\n   Sure, will create a new config for read timeout and use that when 100 continue is enabled.\n\n\n\n", "bhattmanish98 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2393517614\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl,\n \n       abfsApacheHttpClient = new AbfsApacheHttpClient(\n           DelegatingSSLSocketFactory.getDefaultFactory(),\n-          abfsConfiguration, keepAliveCache, baseUrl);\n+          abfsConfiguration, keepAliveCache, baseUrl,\n+          abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType);\n\nReview Comment:\n   The reason for this change: Since the keep alive cache is on client level and we were doing cache warmup for both the client separately. Now with this change, we will do cache warmup only for the default client, not for both the clients.\n\n\n\n", "bhattmanish98 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2393517614\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl,\n \n       abfsApacheHttpClient = new AbfsApacheHttpClient(\n           DelegatingSSLSocketFactory.getDefaultFactory(),\n-          abfsConfiguration, keepAliveCache, baseUrl);\n+          abfsConfiguration, keepAliveCache, baseUrl,\n+          abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType);\n\nReview Comment:\n   The reason for this change: Since the keep alive cache is on client level and we were doing cache warmup for both the client separately. Now with this change, we will do cache warmup only for the default client, not for both the clients.\r\n   Will add the comment in the code as well\n\n\n\n", "bhattmanish98 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2393520715\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestApacheClientConnectionPool.java:\n##########\n@@ -118,6 +121,38 @@ public void testConnectedConnectionLogging() throws Exception {\n         .isEqualTo(4);\n   }\n \n+  /**\n+   * Test to verify that the ApacheHttpClient falls back to JDK client\n+   * when connection warmup fails.\n+   * This test is applicable only for ApacheHttpClient.\n+   */\n+  @Test\n+  public void testApacheClientFallbackDuringConnectionWarmup()\n+      throws Exception {\n+    try (KeepAliveCache keepAliveCache = new KeepAliveCache(\n+        new AbfsConfiguration(new Configuration(), EMPTY_STRING))) {\n+      // Create a connection manager with invalid URL to force fallback to JDK client\n+      // during connection warmup.\n+      // This is to simulate failure during connection warmup in the connection manager.\n+      // The invalid URL will cause the connection manager to fail to create connections\n+      // during warmup, forcing it to fall back to JDK client.\n+      final AbfsConnectionManager connMgr = new AbfsConnectionManager(\n+          RegistryBuilder.<ConnectionSocketFactory>create()\n+              .register(HTTPS_SCHEME, new SSLConnectionSocketFactory(\n+                  DelegatingSSLSocketFactory.getDefaultFactory(),\n+                  getDefaultHostnameVerifier()))\n+              .build(),\n+          new AbfsHttpClientConnectionFactory(), keepAliveCache,\n+          new AbfsConfiguration(new Configuration(), EMPTY_STRING),\n+          new URL(\"https://test.com\"), true);\n+\n+      Assertions.assertThat(AbfsApacheHttpClient.usable())\n+          .describedAs(\"Apache HttpClient should be not usable\")\n\nReview Comment:\n   Sure, will do that.\n\n\n\n", "bhattmanish98 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2393522594\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -188,7 +189,7 @@ public AbfsBlobClient(final URL baseUrl,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n     super(baseUrl, sharedKeyCredentials, abfsConfiguration, tokenProvider,\n-        encryptionContextProvider, abfsClientContext);\n+        encryptionContextProvider, abfsClientContext, AbfsServiceType.BLOB);\n\nReview Comment:\n   As described in the previous comment, we need to find out default client and for that we are comparing these values with the default value configured.\n\n\n\n", "bhattmanish98 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2393523578\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -214,7 +214,7 @@ public final class FileSystemConfigurations {\n   public static final long THOUSAND = 1000L;\n \n   public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY\n-      = HttpOperationType.JDK_HTTP_URL_CONNECTION;\n+      = HttpOperationType.APACHE_HTTP_CLIENT;\n\nReview Comment:\n   Sure, will make the change in .md file as well\n\n\n\n", "bhattmanish98 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2393620859\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsConnectionManager.java:\n##########\n@@ -91,26 +92,34 @@ class AbfsConnectionManager implements HttpClientConnectionManager {\n   /**\n    * The base host for which connections are managed.\n    */\n-  private HttpHost baseHost;\n+  private final HttpHost baseHost;\n \n   AbfsConnectionManager(Registry<ConnectionSocketFactory> socketFactoryRegistry,\n-      AbfsHttpClientConnectionFactory connectionFactory, KeepAliveCache kac,\n-      final AbfsConfiguration abfsConfiguration, final URL baseUrl) {\n+      AbfsHttpClientConnectionFactory connectionFactory,\n+      KeepAliveCache kac,\n+      final AbfsConfiguration abfsConfiguration,\n+      final URL baseUrl,\n+      final boolean isCacheWarmupNeeded) {\n     this.httpConnectionFactory = connectionFactory;\n     this.kac = kac;\n     this.connectionOperator = new DefaultHttpClientConnectionOperator(\n         socketFactoryRegistry, null, null);\n     this.abfsConfiguration = abfsConfiguration;\n-    if (abfsConfiguration.getApacheCacheWarmupCount() > 0\n+    this.baseHost = new HttpHost(baseUrl.getHost(),\n+        baseUrl.getDefaultPort(), baseUrl.getProtocol());\n+    if (isCacheWarmupNeeded && abfsConfiguration.getApacheCacheWarmupCount() > 0\n         && kac.getFixedThreadPool() != null) {\n       // Warm up the cache with connections.\n       LOG.debug(\"Warming up the KeepAliveCache with {} connections\",\n           abfsConfiguration.getApacheCacheWarmupCount());\n-      this.baseHost = new HttpHost(baseUrl.getHost(),\n-          baseUrl.getDefaultPort(), baseUrl.getProtocol());\n       HttpRoute route = new HttpRoute(baseHost, null, true);\n-      cacheExtraConnection(route,\n+      int totalConnectionsCreated = cacheExtraConnection(route,\n           abfsConfiguration.getApacheCacheWarmupCount());\n+      if (totalConnectionsCreated == 0) {\n\nReview Comment:\n   Yes, make sense. I have updated the returned value in case of rejected exception; other thing will remain the same.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java:\n##########\n@@ -68,13 +68,13 @@ public AbfsClientHandler(final URL baseUrl,\n       final SASTokenProvider sasTokenProvider,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n+    initServiceType(abfsConfiguration);\n\nReview Comment:\n   This will initialize the default and ingress service types. This is needed before crating the clients so that we can do cache warmup only for default client.\n\n\n\n", "hadoop-yetus commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3378551948\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 48s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  1s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  54m 51s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/3/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 150m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7967 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux 6778220a7832 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 44765c0ae41d29a78198cb113bc06780be7092af |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/3/testReport/ |\r\n   | Max. process+thread count | 561 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anmolanmol1234 commented on code in PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#discussion_r2415677758\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java:\n##########\n@@ -68,6 +68,9 @@ public AbfsClientHandler(final URL baseUrl,\n       final SASTokenProvider sasTokenProvider,\n       final EncryptionContextProvider encryptionContextProvider,\n       final AbfsClientContext abfsClientContext) throws IOException {\n+    // This will initialize the default and ingress service types.\n+    // This is needed before crating the clients so that we can do cache warmup\n\nReview Comment:\n   nit: typo creating\n\n\n\n", "hadoop-yetus commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3386392364\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  53m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 32s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  7s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 150m 27s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7967 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux a733619d79ed 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7ff70868cc0483c55d875f66cf2379f04e75075e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/4/testReport/ |\r\n   | Max. process+thread count | 526 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/4/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "bhattmanish98 commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3388294934\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 217\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 875, Failures: 0, Errors: 0, Skipped: 169\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 282\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 228\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 721, Failures: 0, Errors: 0, Skipped: 140\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 284\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 718, Failures: 0, Errors: 0, Skipped: 152\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 713, Failures: 0, Errors: 0, Skipped: 198\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 746, Failures: 0, Errors: 0, Skipped: 226\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 281\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\n\n\n", "anujmodi2021 merged PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967\n\n\n", "steveloughran commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3401193052\n\n   I understand why the startup mechanism makes sense, but I'm curious about doing it on a live connection. What problems were occurring with hostname lookup that changing client would fix?\r\n   \r\n   Do you want this in 3.4.3? if so do a backport PR ASAP\n\n\n", "bhattmanish98 commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3432065456\n\n   @steveloughran We\u2019re performing cache refresh when the number of available connections in the cache drops below a certain threshold. The warmup normally happens during file system initialization, but if the file system is already active and making multiple parallel network calls, creating new connections on demand with the Apache client tends to take longer.\r\n   \r\n   To avoid delays in such cases, we asynchronously pre-create and cache a few additional connections whenever the pool runs low. This ensures that subsequent network calls can use already-established connections and spend time on actual data processing rather than connection setup.\r\n   \r\n   \n\n\n", "bhattmanish98 commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3435042669\n\n   > Do you want this in 3.4.3? if so do a backport PR ASAP\r\n   \r\n   The previous PR that includes the Apache client changes was also not included in release 3.4.3. We\u2019re planning to include this change in the next major release.\n\n\n"], "labels": ["pull-request-available"], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))"}, {"question": "Who reported this issue?", "answer": "Manish Bhatt"}]}
{"key": "HADOOP-19671", "project": "HADOOP", "title": "Migrate to AssertJ for Assertion Verification", "status": "In Progress", "reporter": "Shilun Fan", "created": "2025-08-30T06:33:03.000+0000", "description": "Currently, the unit tests in the project do not fully leverage modern assertion capabilities, resulting in lower readability and maintainability of the test code. To improve test clarity and extensibility, we plan to migrate the existing assertion library to {*}AssertJ{*}.\r\n\r\n\u00a0\r\n\r\n*Objective:*\r\n * Migrate all assertions in existing tests from JUnit or other assertion libraries to AssertJ.\r\n\r\n * Utilize AssertJ\u2019s rich assertion methods (e.g., fluent API, more readable assertions) to enhance the expressiveness of unit tests.\r\n\r\n * Ensure that all existing unit tests continue to run correctly after migration.\r\n\r\n\u00a0\r\n\r\n*Implementation Steps:*\r\n # Analyze existing unit test code to identify assertions that need to be replaced.\r\n\r\n # Replace existing assertions with AssertJ assertion syntax.\r\n\r\n # Run unit tests to ensure the tests pass and function correctly after migration.\r\n\r\n # Update relevant documentation to ensure the team is aware of how to use AssertJ for assertions.", "comments": ["[~stevel@apache.org]\u00a0The JUnit 5 upgrade for Hadoop is nearing completion, and I will soon initiate a new upgrade plan to migrate the unit tests to AssertJ in order to improve test readability and maintainability.\r\n\u00a0\r\n\u00a0We need to establish a new set of unit testing standards. If you have any relevant suggestions or rules, feel free to share.\r\n\u00a0\r\n\u00a0"], "labels": [], "summary": "Currently, the unit tests in the project do not fully leverage modern assertion ", "qna": [{"question": "What is the issue title?", "answer": "Migrate to AssertJ for Assertion Verification"}, {"question": "Who reported this issue?", "answer": "Shilun Fan"}]}
{"key": "HADOOP-19670", "project": "HADOOP", "title": "[JDK22] Replace Thread with SubjectPreservingThread", "status": "Resolved", "reporter": "Istvan Toth", "created": "2025-08-29T08:22:44.000+0000", "description": null, "comments": ["stoty opened a new pull request, #8062:\nURL: https://github.com/apache/hadoop/pull/8062\n\n   ### Description of PR\r\n   \r\n   The second part of HADOOP-19574.\r\n   \r\n   Replace Thread wit SubjectPreservingThread\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Tested with   HADOOP-19668 which it depends on.\r\n   Ran the full test suite with JDK25, and confirmed that it does not cause regressions\r\n   (i.e. only tests which also fail with Java 17 fail with Java 25)\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3500945954\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 20s |  |  https://github.com/apache/hadoop/pull/8062 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8062/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3501083445\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 23s |  |  https://github.com/apache/hadoop/pull/8062 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8062/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "stoty commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3523505922\n\n   Rebased on trunk @steveloughran .\n\n\n", "hadoop-yetus commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3523513216\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 22s |  |  https://github.com/apache/hadoop/pull/8062 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8062/3/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3525860070\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 21s |  |  https://github.com/apache/hadoop/pull/8062 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8062/4/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on code in PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#discussion_r2523612315\n\n\n##########\nhadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java:\n##########\n@@ -61,6 +61,9 @@ public final class SubjectUtil {\n   private static final int JAVA_SPEC_VER = Math.max(8,\n       Integer.parseInt(System.getProperty(\"java.specification.version\").split(\"\\\\.\")[0]));\n \n+  /**\n+   * True if the current JVM copies the current JAAS subject into new threads automatically\n\nReview Comment:\n   nit: add a . to keep javadoc happy\n\n\n\n##########\nhadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java:\n##########\n@@ -42,7 +42,7 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.hdfs.protocol.DatanodeAdminProperties;\n-\n+import org.apache.hadoop.util.concurrent.SubjectInheritingThread;\n\nReview Comment:\n   nit, leave the blank line between this and the other package.\r\n   \r\n   I'm trying to keep that import diff down as its where we so often get needless merge conflict\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AConcurrentOps.java:\n##########\n@@ -37,7 +37,7 @@\n import org.apache.hadoop.fs.contract.ContractTestUtils.NanoTimer;\n import org.apache.hadoop.fs.s3a.S3AFileSystem;\n import org.apache.hadoop.test.tags.ScaleTest;\n-\n+import org.apache.hadoop.util.concurrent.SubjectInheritingThread;\n\nReview Comment:\n   nit: restore the blank line\n\n\n\n", "hadoop-yetus commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3528351187\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 22s |  |  https://github.com/apache/hadoop/pull/8062 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8062/5/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3547027695\n\n   what about a separate PR for that yetus update?\r\n   \r\n   Anyway, this is is all building locally is it? what modules have you tested?\n\n\n", "stoty commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3547635496\n\n   Opened #8090 which includes the fixed yetus, @steveloughran .\r\n   \r\n   Yes, it builds locally, but I have not run the tests locally on this one (I built it with -DskipTests)\n\n\n", "stoty commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3547656592\n\n   I have ran the full test suite a few weeks back on an earlier PR, and confirmed that it does not cause regressions.\n\n\n", "steveloughran merged PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062\n\n\n", "pan3793 commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3562403708\n\n   Big thanks to @stoty @steveloughran, and I think this is the last patch of the UGI compatibility story, so it's time to start backporting to branch-3.4?\n\n\n", "stoty commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3562620029\n\n   Yes, this was the last big one.\r\n   I still have an outstandig minor PR: #7961\r\n   IMO the next priority would be setting up the CI for JDK21 and 25 to avoid future regressions.\n\n\n", "h-vetinari commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3565302620\n\n   > so it's time to start backporting to branch-3.4?\r\n   \r\n   is it realistic to backport all the pieces necessary for newer Java support to the 3.4 maintenance branch?\n\n\n", "stoty commented on PR #8062:\nURL: https://github.com/apache/hadoop/pull/8062#issuecomment-3565873583\n\n   I don't see any issues.\r\n   trunk still compiles and runs the test suite  with Java 8 , so even the Java 8 requirement in 3.4 is not a problem.\r\n   \r\n   Some of the fixes are already backported as they were also needed for JDK 17 (or even for JDK8), but it is still a huge job.\r\n   Also many will not not backport cleanly due to the JUnit 4->5 migration.\r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "[JDK22] Replace Thread with SubjectPreservingThread"}, {"question": "Who reported this issue?", "answer": "Istvan Toth"}]}
{"key": "HADOOP-19669", "project": "HADOOP", "title": "Update Daemon to restore pre JDK22 Subject behaviour in Threads", "status": "Resolved", "reporter": "Istvan Toth", "created": "2025-08-29T07:57:27.000+0000", "description": null, "comments": ["Merged into HADOOP-196668"], "labels": [], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "Update Daemon to restore pre JDK22 Subject behaviour in Threads"}, {"question": "Who reported this issue?", "answer": "Istvan Toth"}]}
{"key": "HADOOP-19668", "project": "HADOOP", "title": "Add SubjectInheritingThread to restore pre JDK22 Subject behaviour in Threads", "status": "Resolved", "reporter": "Istvan Toth", "created": "2025-08-29T07:23:38.000+0000", "description": "This is the first part of HADOOP-19574 which adds the compatibility code, and fixes Daemon, but does not replace native Thread instances.\r\n", "comments": ["stoty opened a new pull request, #8061:\nURL: https://github.com/apache/hadoop/pull/8061\n\n   ### Description of PR\r\n   \r\n   First part of HADOOP-19574. \r\n   Add SubjectInheritingThread and update Daemon, but only update the classes using Daemon\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Tested together with HADOOP-19670. \r\n   Ran the full test suite with JDK25, and confirmed that it does not cause regressions\r\n   (i.e. only tests which also fail with Java 17 fail with Java 25)\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3503157466\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 30s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  7s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 44s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  8s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m  5s | [/branch-spotbugs-hadoop-common-project_hadoop-auth-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-auth-warnings.html) |  hadoop-common-project/hadoop-auth in trunk has 8 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |   3m  1s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) |  hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |   3m 38s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-client-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-client-warnings.html) |  hadoop-hdfs-project/hadoop-hdfs-client in trunk has 2801 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |   4m  0s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html) |  hadoop-hdfs-project/hadoop-hdfs in trunk has 291 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |   1m  4s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-nfs-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-nfs-warnings.html) |  hadoop-hdfs-project/hadoop-hdfs-nfs in trunk has 16 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 30s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   4m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 36s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 55s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   3m 18s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 8 new + 189 unchanged - 0 fixed = 197 total (was 189)  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 39s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 47s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-common-project_hadoop-auth-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 1 new + 71 unchanged - 0 fixed = 72 total (was 71)  |\r\n   | -1 :x: |  javadoc  |   0m 45s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-common-project_hadoop-auth-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 1 new + 58 unchanged - 0 fixed = 59 total (was 58)  |\r\n   | -1 :x: |  spotbugs  |   3m 59s | [/new-spotbugs-hadoop-hdfs-project_hadoop-hdfs-client.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/new-spotbugs-hadoop-hdfs-project_hadoop-hdfs-client.html) |  hadoop-hdfs-project/hadoop-hdfs-client generated 2 new + 2799 unchanged - 2 fixed = 2801 total (was 2801)  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 40s |  |  hadoop-auth in the patch passed.  |\r\n   | -1 :x: |  unit  |  21m 59s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 54s |  |  hadoop-hdfs-client in the patch passed.  |\r\n   | -1 :x: |  unit  | 213m 51s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt) |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 31s |  |  hadoop-hdfs-nfs in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 14s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 481m 33s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-hdfs-project/hadoop-hdfs-client |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.hdfs.DataStreamer.work()  At DataStreamer.java:org.apache.hadoop.hdfs.DataStreamer.work()  At DataStreamer.java:[line 816] |\r\n   |  |  Naked notify in org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.work()  At DataStreamer.java:At DataStreamer.java:[line 1300] |\r\n   | Failed junit tests | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   |   | hadoop.hdfs.tools.TestDFSAdmin |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8061 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 697ad653f49d 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 064e97b380f4657b58a240d3bcbf4c8e5d733af0 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/testReport/ |\r\n   | Max. process+thread count | 3468 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-nfs U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8061/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3515069861\n\n   @steveloughran @slfan1989\r\n   \r\n   I think this has already been reviewed in https://github.com/apache/hadoop/pull/7892, can we move this forward quickly?\r\n   \r\n   @stoty I think the only thing is to fix a few checkstyle issues reported by Yetus\n\n\n", "slfan1989 commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3515142992\n\n   > @steveloughran @slfan1989\r\n   > \r\n   > I think this has already been reviewed in #7892, can we move this forward quickly?\r\n   > \r\n   > @stoty I think the only thing is to fix a few checkstyle issues reported by Yetus\r\n   \r\n   I\u2019ve shared my feedback in #7892. I\u2019m giving this PR a soft +1, but I think we should remain patient for now. Thanks again for everyone\u2019s efforts.\r\n   \n\n\n", "slfan1989 commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3515210432\n\n   @stoty Could we also include the Javadoc updates and the new SpotBugs fixes? Thanks a lot!\n\n\n", "stoty commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3515267393\n\n   I've checked the reported Javadoc errors, both are for files that I haven't touched.\n\n\n", "stoty commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3515298425\n\n   I've also checked the two new SpotBugs warnings.\r\n   Those are pre-existing, they are only reported as new because the method name has changes run() -> work().\r\n   If those are fixed on trunk, then these will also go away. \n\n\n", "steveloughran commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3522338505\n\n   ok, going with the approval.\n\n\n", "steveloughran merged PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061\n\n\n", "steveloughran commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3522355610\n\n   @stoty ok, merged. please do the next stage\n\n\n", "stoty commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3523507006\n\n   Thanks a lot @steveloughran .\n\n\n", "pan3793 commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3523585270\n\n   awesome, thanks @stoty and all!\n\n\n", "steveloughran commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3523881948\n\n   ok, having merged it, I can see the javadocs are unhappy\r\n   ```\r\n    * subject restoration logic. SubjectInheritingThread provides a {@link work()}\r\n    * method instead, which is wrapped and invoked by its own final {@link run()}\r\n   ```\r\n   \r\n   you need a # in front of the work() and run() calls.\r\n   can you just add this as a followup with the same JIRA ID? no need for a new one. thanks\r\n   \n\n\n", "stoty commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3525859549\n\n   I have added the hashmarks, and added another missing commit to SubjectUtil in #8062 , @steveloughran .\r\n   \r\n   However, I wasn't able to find error you mentioned.\r\n   I ran javadoc:javadoc, and searched for Subject, but the only related error could find in the output was the SubjectUtil missing comment one.\r\n   \r\n   Where/How did you find the javadoc errors/warnings you mentioned ?\n\n\n", "steveloughran commented on PR #8061:\nURL: https://github.com/apache/hadoop/pull/8061#issuecomment-3527996853\n\n   my IDE is flagging it in SubjectInheritingThread; only thing I'd noticed\n\n\n", "FWIW I\"m now seeing new error messages in logs as the first time SubjectUtil is invoked, THREAD_INHERITS_SUBJECT is coinfigured and the JRE tells me off. \r\n\r\n{code}\r\nWARNING: A terminally deprecated method in java.lang.System has been called\r\nWARNING: System::setSecurityManager has been called by org.apache.hadoop.security.authentication.util.SubjectUtil (file:/Users/stevel/.m2/repository/org/apache/hadoop/hadoop-auth/3.5.0-SNAPSHOT/hadoop-auth-3.5.0-SNAPSHOT.jar)\r\nWARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.SubjectUtil\r\nWARNING: System::setSecurityManager will be removed in a future release\r\n{code}\r\n\r\nWe can't have this being looked all the time, as it'll be the first thing anyone sees on all cli commands, app launches etc. \r\n\r\nIs there are better way to determine this? It's only setSecurityManager() which reports this, not getSecurityManager. And on java < 22 we know the security manager is there. ", "HADOOP-19744 fixes that warning, [~stevel@apache.org]."], "labels": ["pull-request-available"], "summary": "This is the first part of HADOOP-19574 which adds the compatibility code, and fi", "qna": [{"question": "What is the issue title?", "answer": "Add SubjectInheritingThread to restore pre JDK22 Subject behaviour in Threads"}, {"question": "Who reported this issue?", "answer": "Istvan Toth"}]}
{"key": "HADOOP-19667", "project": "HADOOP", "title": "Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)", "status": "Open", "reporter": "AMC-team", "created": "2025-08-27T10:08:02.000+0000", "description": "In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as \u201cThe buffer size used by CryptoInputStream and CryptoOutputStream.\u201d It does not specify the legal value constraints.\r\n{code:java}\r\n<property>\u00a0 \r\n<name>hadoop.security.crypto.buffer.size</name>\u00a0 \r\n<value>8192</value>\u00a0 \r\n<description>The buffer size used by CryptoInputStream and CryptoOutputStream.\u00a0 </description>\r\n</property> {code}\r\nThe runtime enforces two hidden constraints that are not documented:\r\n1. Minimum value is 512 bytes. Values below 512 cause IllegalArgumentException at stream construction time.\r\n2. Block-size flooring: The effective buffer size is floored to a multiple of the cipher algorithm\u2019s block size (e.g., 16 bytes for AES/CTR/NoPadding).\r\n\r\nAs a result, users may be surprised that:\r\n1. Setting a value like 4100 results in an actual capacity of 4096.\r\n2. Setting values <512 fails fast with IllegalArgumentException.\r\n\r\n*Expected*\r\ncore-default.xml (and user-facing docs) should explicitly document:\r\n1. Minimum legal value: 512 bytes.\r\n\r\n2. The effective value is floored to the nearest multiple of the cipher algorithm block size (e.g., 16 for AES).", "comments": ["please provide a pull request in trunk which updates the file with the relevant information.\r\nhadoop-hdfs-project/hadoop-hdfs/src/site/markdown/TransparentEncryption.md", "zzz0706 opened a new pull request, #7907:\nURL: https://github.com/apache/hadoop/pull/7907\n\n   ### Description of PR\r\n   \r\n   Docs-only change to **TransparentEncryption.md** clarifying the hidden constraints for\r\n   `hadoop.security.crypto.buffer.size`:\r\n   \r\n   - **Minimum value is 512 bytes** (values <512 throw `IllegalArgumentException` at stream construction).\r\n   - **Block alignment:** the effective value is **floored** to the nearest multiple of the cipher algorithm\r\n     **block size** (e.g., 16 for AES/CTR/NoPadding). Examples: `4100 -> 4096`, `8195 -> 8192`.\r\n   \r\n   This is a minimal wording update (single-sentence addition) to avoid operator surprise, with no behavior change.\r\n   \r\n   **Target branch:** `trunk` (per contributor guide)\r\n   \r\n   **JIRA:** HADOOP-19667\r\n   \r\n   **Files changed:**\r\n   - `hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/TransparentEncryption.md`\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - Docs-only; built site/module locally to validate formatting:\r\n   \n\n\n", "Thanks for the guidance. I\u2019ve opened a PR against trunk updating the requested file.", "hadoop-yetus commented on PR #7907:\nURL: https://github.com/apache/hadoop/pull/7907#issuecomment-3228575609\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  77m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m  4s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m 51s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7907/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7907 |\r\n   | JIRA Issue | HADOOP-19667 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint |\r\n   | uname | Linux 04003553fdc2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ce5422f5a2696efe7d65ae74ebcb3413fe33abde |\r\n   | Max. process+thread count | 667 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7907/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "In core-default", "qna": [{"question": "What is the issue title?", "answer": "Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)"}, {"question": "Who reported this issue?", "answer": "AMC-team"}]}
{"key": "HADOOP-19666", "project": "HADOOP", "title": "Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension", "status": "Open", "reporter": "Lei Wen", "created": "2025-08-27T03:08:41.000+0000", "description": "This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\ninstruction sets, with full functional verification and performance testing completed.\r\n\r\nThe implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction.\r\n\r\nKey Features: \r\n1. Runtime Hardware Detection\r\nThe PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime.\r\n\r\n2. Performance Improvement\r\nHardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.", "comments": ["leiwen2025 opened a new pull request, #7912:\nURL: https://github.com/apache/hadoop/pull/7912\n\n   This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\n   instruction sets, with full functional verification and performance testing completed.\r\n   \r\n   The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction.\r\n   \r\n   Key Features:\r\n   1. Runtime Hardware Detection\r\n   The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime.\r\n   \r\n   2. Performance Improvement\r\n   Hardware-accelerated CRC32 achieves a performance boost of over **3X** compared to the software implementation.\r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3232576752\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m  5s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   7m 30s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 29s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  56m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |   7m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |   7m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   7m  3s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-eol.txt) |  The patch has 20 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-tabs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-tabs.txt) |  The patch 2 line(s) with tabs.  |\r\n   | -1 :x: |  mvnsite  |   1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  19m 44s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7912 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 3c797fab6900 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 67c832e1dc930b52b9a68c261f372b14e6cf1639 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/testReport/ |\r\n   | Max. process+thread count | 1262 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3233497529\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   7m 37s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 30s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  57m 17s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   6m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |   6m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |   6m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   6m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  19m 38s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 111m 26s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7912 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 9af82b84cde2 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / be2c9c5dfefc7f4ad7591c58f7857b177bef5b0e |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/testReport/ |\r\n   | Max. process+thread count | 3150 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "leiwen2025 commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3409341544\n\n   @steveloughran @ayushtkn Hi, this PR has been open for a while. Could you please take a look when you have time? Thanks!\n\n\n", "steveloughran commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3416334594\n\n   It's been a long time since I did C code, I'll have to stare at this a while.\r\n   \r\n   In #7903 that creation of the file bulk_crc32_riscv.c should be what the new code goes into; work with @PeterPtroc to get something you are both happy with *and tested*.\r\n   \r\n   Having two people who are set up to build and test this on riscv hardware is exactly what we need to get this done\n\n\n", "how does this differ from HADOOP-19655? I think that PR got there just before this one, so takes precedence number-wise; I'm not in a position to evaluate the code.\r\n\r\nCan you and [~peterptroc] collaborate on this? I'll give you both credit in the patches", "leiwen2025 commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3448014826\n\n   @steveloughran Thanks for the review! I'll be happy to work with @PeterPtroc on this. \r\n   \r\n   If someone in the community could help connect me with Peter, that'd be much appreciated. I'd like to coordinate testing and integration on RISC-V.\n\n\n", "Thanks! This patch implements CRC32 on RISC-V using Zvbc vector instructions, while HADOOP-19655 uses Zbc scalar ones.\r\n\r\nHappy to work together with [~peterptroc] to coordinate and test both implementations.", "leiwen2025 commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3514754045\n\n   Hi @pan3793 , could you please review this PR when you have time? The key difference from #8031  is the set of RISC-V extensions used, and I've implemented it with vector instructions. I'd really appreciate your feedback. Thanks!\n\n\n", "steveloughran commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3522330303\n\n   @leiwen2025 can you take up reviewing @PeterPtroc's work and see if you two can come up with the best design between you for performance. A joint submission is more likely to work from testing alone.\n\n\n", "leiwen2025 commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3551600528\n\n   > @leiwen2025 can you take up reviewing @PeterPtroc's work and see if you two can come up with the best design between you for performance. A joint submission is more likely to work from testing alone.\r\n   \r\n   Hi @steveloughran, I have commented on @PeterPtroc 's PR to express my willingness to collaborate with him on refining the design for optimal performance.\r\n   \r\n   If someone could ping him there to make sure he sees it, that would be great. Thanks!\r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\nin", "qna": [{"question": "What is the issue title?", "answer": "Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension"}, {"question": "Who reported this issue?", "answer": "Lei Wen"}]}
{"key": "HADOOP-19665", "project": "HADOOP", "title": "[kms] Negative hadoop.security.kms.client.encrypted.key.cache.expiry causes KMSClientProvider init failure with generic IllegalArgumentException; ", "status": "Patch Available", "reporter": "AMC-team", "created": "2025-08-26T14:11:12.000+0000", "description": "When the client-side config hadoop.security.kms.client.encrypted.key.cache.expiry is set to a negative value in core-site.xml, any tool that initializes KMSClientProvider (e.g., KeyShell) fails immediately with:\r\n\r\n{code:java}\r\njava.lang.IllegalArgumentException: expiry must be > 0\r\n    at org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:xxx)\r\n    at org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:xxx)\r\n    ...\r\n\r\n{code}\r\n\r\nThis is a controlled failure (JVM doesn\u2019t crash), but the error message does not mention which property and what value triggered it. Users typically see a stack trace without a clear remediation hint.\r\n\r\n*Expected behavior*\r\n\r\nFail fast with a clear configuration error that names the property and value, e.g.:\r\nInvalid configuration: hadoop.security.kms.client.encrypted.key.cache.expiry = -1 (must be > 0 ms)\r\n\r\n*Steps to Reproduce*\r\n1. In the client core-site.xml, set:\r\n\r\n{code:xml}\r\n<property>\r\n  <name>hadoop.security.kms.client.encrypted.key.cache.expiry</name>\r\n  <value>-1</value>\r\n</property>\r\n{code}\r\n2. Ensure the conf is active (echo $HADOOP_CONF_DIR points to this dir).\r\n3. Run:\r\n\r\n{code:java}\r\n./bin/hadoop key list -provider kms://http@localhost:9600/kms -metadata\r\n{code}\r\n", "comments": [], "labels": [], "summary": "When the client-side config hadoop", "qna": [{"question": "What is the issue title?", "answer": "[kms] Negative hadoop.security.kms.client.encrypted.key.cache.expiry causes KMSClientProvider init failure with generic IllegalArgumentException; "}, {"question": "Who reported this issue?", "answer": "AMC-team"}]}
{"key": "HADOOP-19664", "project": "HADOOP", "title": "S3A Analytics-Accelerator: Move AAL to use Java sync client", "status": "Resolved", "reporter": "Ahmar Suhail", "created": "2025-08-26T10:26:15.000+0000", "description": "Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.\u00a0", "comments": ["sync or async?", "Sync.\u00a0\r\n\r\nIt's using async currently. But the readVectored + AAL use case is not ideal for the async client. As we already have our own thread pool, and each thread is responsible for making a single S3 request, and start reading data from that input stream immediately to fill the internal buffers..\u00a0\r\n\r\nWith the async client, this means you need to join() immediately, and when at a higher concurrency things get stuck in the Netty thread pool and the AsyncResponseTransformer.toBlockingInputStream() of\r\n\r\ns3AsyncClient\r\n.getObject(builder.build(), AsyncResponseTransformer.toBlockingInputStream()).\r\n\u00a0\r\nS3Async client works well (I think) when you have high concurrency but don't need to join on the data immediately, so the netty io pool is sufficient to satisfy those requests.\u00a0", "internal benchmarking has been showing a 4-5% improvement with the Sync client consistently.\u00a0", "ahmarsuhail opened a new pull request, #7909:\nURL: https://github.com/apache/hadoop/pull/7909\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Moves AAL to use the Java sync client as this is giving us the best performance in internal benchmarks. \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Tested in \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "ahmarsuhail commented on PR #7909:\nURL: https://github.com/apache/hadoop/pull/7909#issuecomment-3228451753\n\n   @steveloughran @mukund-thakur @shameersss1  \r\n   \r\n   small PR to move to AAL to use the Java sync client\n\n\n", "hadoop-yetus commented on PR #7909:\nURL: https://github.com/apache/hadoop/pull/7909#issuecomment-3228959531\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 23s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 15s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  22m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 31s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  0s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 58s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 30s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 25s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  9s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   8m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   2m 25s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/1/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 1 new + 1 unchanged - 0 fixed = 2 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 56s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 18s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 25s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 44s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 42s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 130m 37s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7909 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 9825a7b540bf 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 77adf896d34aa6259543a9fc93161a029a208438 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/1/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "mukund-thakur commented on PR #7909:\nURL: https://github.com/apache/hadoop/pull/7909#issuecomment-3246075386\n\n   checkstyle failure. \r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/streams/TestStreamFactories.java:26:import software.amazon.awssdk.services.s3.S3AsyncClient;:8: Unused import - software.amazon.awssdk.services.s3.S3AsyncClient. [UnusedImports]\n\n\n", "steveloughran commented on PR #7909:\nURL: https://github.com/apache/hadoop/pull/7909#issuecomment-3291747586\n\n   (oh, and include S3A: in the commit message. THX)\n\n\n", "ahmarsuhail commented on PR #7909:\nURL: https://github.com/apache/hadoop/pull/7909#issuecomment-3297498659\n\n   Test results are all good, except the known `ITestS3AContractOpen>AbstractContractOpenTest.testInputStreamReadNegativePosition` failure. Will merge. \r\n   \r\n   I don't see any failures in ITestS3AContractAnalyticsStreamVectoredRead that Steve saw in the SDK upgrade PR: https://github.com/apache/hadoop/pull/7882, will take a look at that separately. \n\n\n", "ahmarsuhail merged PR #7909:\nURL: https://github.com/apache/hadoop/pull/7909\n\n\n", "hadoop-yetus commented on PR #7909:\nURL: https://github.com/apache/hadoop/pull/7909#issuecomment-3298248022\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 28s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  26m 11s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/2/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   9m  1s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 18s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 51s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 50s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 25s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m  8s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   8m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 49s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 18s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 19s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 136m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7909 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux f17818206b60 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 112594c60b788e6c2f51021c0bcd017b3e0467ed |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/2/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7909:\nURL: https://github.com/apache/hadoop/pull/7909#issuecomment-3298290502\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m  2s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  26m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   9m 31s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 14s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 50s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 25s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m  5s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  2s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   8m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 47s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 45s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 20s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 46s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 26s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 45s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 135m 27s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7909 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 304e885133c5 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 65d16df11981860e9a6ef04a84e13672d7bb1ee3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/3/testReport/ |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #7909:\nURL: https://github.com/apache/hadoop/pull/7909#issuecomment-3304093797\n\n   @ahmarsuhail can you do a followup to add the library to LICENSE-binary? thanks\n\n\n"], "labels": ["pull-request-available"], "summary": "Java sync client is giving the best performance for our use case, especially for", "qna": [{"question": "What is the issue title?", "answer": "S3A Analytics-Accelerator: Move AAL to use Java sync client"}, {"question": "Who reported this issue?", "answer": "Ahmar Suhail"}]}
{"key": "HADOOP-19663", "project": "HADOOP", "title": "Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration", "status": "Resolved", "reporter": "Ptroc", "created": "2025-08-26T09:30:19.000+0000", "description": "Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations.", "comments": ["PeterPtroc opened a new pull request, #7903:\nURL: https://github.com/apache/hadoop/pull/7903\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Add a RISC-V-specific compilation unit: org/apache/hadoop/util/bulk_crc32_riscv.c.\r\n   \r\n   - Contains a no-op constructor reserved for future HW capability detection and dispatch.\r\n   - Keeps runtime behavior unchanged (falls back to the generic software path in bulk_crc32.c).\r\n   - Wire CMake to select bulk_crc32_riscv.c on riscv32/riscv64, mirroring other platforms.\r\n   \r\n   This PR establishes the foundational build infrastructure for future RISC-V Zbc (CLMUL) CRC32/CRC32C acceleration without changing current behavior. Follow-ups (HADOOP-19655) will introduce HW-accelerated implementations and runtime dispatch.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - Ensured native build for hadoop-common compiles cleanly with RISC-V selection.\r\n   - Verified by test_bulk_crc32.\r\n   - No new tests added, as this patch is scaffolding-only without any behavior change.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\n\n\n", "hadoop-yetus commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3225752648\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  14m 21s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 56s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  94m 44s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 54s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 54s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 198m 14s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7903 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux af19edaaec5b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1b159b6f8fadc7e4229a3aa0eeaa184b6d2f25cc |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/testReport/ |\r\n   | Max. process+thread count | 1279 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3226747056\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  14m  4s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 55s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  94m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  13m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  13m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  13m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 57s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 39s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 53s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 176m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7903 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux c0759ac9ab3e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3d607d41a2a311260c28800135009dfca09fb4f3 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/testReport/ |\r\n   | Max. process+thread count | 2145 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "PeterPtroc commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3228926064\n\n   Due to some CI infrastructure issues, I will paste the result of validating this patch on a RISC-V machine. Below are the command and the results.\r\n   \r\n   Command\uff1a \r\n   \r\n   ```\r\n   mvn -Pnative \\\r\n     -Dtest=org.apache.hadoop.util.TestNativeCrc32 \\\r\n     -Djava.library.path=\"$HADOOP_COMMON_LIB_NATIVE_DIR\" \\\r\n     test\r\n   ```\r\n   \r\n   Results\r\n   \r\n   ```\r\n   [INFO] -------------------------------------------------------\r\n   [INFO]  T E S T S\r\n   [INFO] -------------------------------------------------------\r\n   [INFO] Running org.apache.hadoop.util.TestNativeCrc32\r\n   [INFO] Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.72 s ", "PeterPtroc commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3228966982\n\n   Hi @pan3793 @slfan1989 , could you please take a look when you have a moment? This PR adds RISC-V CRC32 scaffolding and keeps behavior unchanged. Happy to address any feedback. Thanks!\n\n\n", "pan3793 commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3232286543\n\n   @PeterPtroc I suppose most developers here do not have RISC-V env, is it possible to have a docs about how to verify it by leveraging QEMU or some common tools?\n\n\n", "PeterPtroc commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3235836592\n\n   \r\n   @pan3793 Thanks for the suggestion! Below is a concise doc to verify the correctness of the crc32riscv implementation:\r\n   \r\n   I mainly verify on RISC\u2011V by using QEMU together with the openEuler RISC\u2011V image.\r\n   \r\n   Download the image\r\n   - https://dl-cdn.openeuler.openatom.cn/openEuler-25.03/virtual_machine_img/riscv64/\r\n   \r\n   For me, from the above link, download these four files: RISCV_VIRT_CODE.fd, RISCV_VIRT_VARS.fd, openEuler-25.03-riscv64.qcow2.xz, and start_vm.sh; then log in as root with the password: openEuler12#$.\r\n   \r\n   Install required packages\r\n   ````bash\r\n   yum install -y gcc gcc-c++ gcc-gfortran libgcc cmake\r\n   yum install -y wget openssl openssl-devel zlib zlib-devel automake libtool make libstdc++-static glibc-static git snappy snappy-devel fuse fuse-devel doxygen clang cyrus-sasl cyrus-sasl-devel libtirpc libtirpc-devel\r\n   yum install -y java-17-openjdk.riscv64 java-17-openjdk-devel.riscv64 java-17-openjdk-headless.riscv64\r\n   ````\r\n   \r\n   Install Protobuf 2.5.0 (with RISC\u2011V patches)\r\n   ````bash\r\n   mkdir protobuf && cd protobuf\r\n   \r\n   # Fetch sources\r\n   git clone https://gitee.com/src-openeuler/protobuf2.git\r\n   cd protobuf2\r\n   tar -xjf protobuf-2.5.0.tar.bz2\r\n   cp *.patch protobuf-2.5.0 && cd protobuf-2.5.0\r\n   \r\n   # Apply patches (adds riscv64 support and build fixes)\r\n   patch -p1 < 0001-Add-generic-GCC-support-for-atomic-operations.patch\r\n   patch -p1 < protobuf-2.5.0-gtest.patch\r\n   patch -p1 < protobuf-2.5.0-java-fixes.patch\r\n   patch -p1 < protobuf-2.5.0-makefile.patch\r\n   patch -p1 < add-riscv64-support.patch\r\n   \r\n   # Autotools setup\r\n   libtoolize\r\n   yum install -y automake\r\n   automake-1.17 -a\r\n   chmod +x configure\r\n   \r\n   # Configure, build, install\r\n   ./configure --build=riscv64-unknown-linux --prefix=/usr/local/protobuf-2.5.0\r\n   make\r\n   make check\r\n   make install\r\n   ldconfig\r\n   \r\n   # Publish protoc 2.5.0 into local Maven repo (riscv64 classifier)\r\n   mvn install:install-file \\\r\n     -DgroupId=com.google.protobuf \\\r\n     -DartifactId=protoc \\\r\n     -Dversion=2.5.0 \\\r\n     -Dclassifier=linux-riscv64 \\\r\n     -Dpackaging=exe \\\r\n     -Dfile=/usr/local/protobuf-2.5.0/bin/protoc\r\n   \r\n   cd ..\r\n   ````\r\n   \r\n   Install Protobuf 3.25.5\r\n   ````bash\r\n   # Download and unpack\r\n   wget -c https://github.com/protocolbuffers/protobuf/releases/download/v25.5/protobuf-25.5.tar.gz\r\n   tar -xzf protobuf-25.5.tar.gz\r\n   cd protobuf-25.5\r\n   \r\n   # Abseil dependency\r\n   git clone https://github.com/abseil/abseil-cpp third_party/abseil-cpp\r\n   \r\n   # Configure and build\r\n   cmake ./ \\\r\n     -DCMAKE_BUILD_TYPE=RELEASE \\\r\n     -Dprotobuf_BUILD_TESTS=off \\\r\n     -DCMAKE_CXX_STANDARD=20 \\\r\n     -DCMAKE_INSTALL_PREFIX=/usr/local/protobuf-3.25.5\r\n   \r\n   make install -j \"$(nproc)\"\r\n   \r\n   # Publish protoc 3.25.5 into local Maven repo (riscv64 classifier)\r\n   mvn install:install-file \\\r\n     -DgroupId=com.google.protobuf \\\r\n     -DartifactId=protoc \\\r\n     -Dversion=3.25.5 \\\r\n     -Dclassifier=linux-riscv64 \\\r\n     -Dpackaging=exe \\\r\n     -Dfile=/usr/local/protobuf-3.25.5/bin/protoc\r\n   \r\n   # Make protoc available on PATH and verify\r\n   sudo ln -sfn /usr/local/protobuf-3.25.5/bin/protoc /usr/local/bin/protoc\r\n   protoc --version\r\n   ````\r\n   \r\n   Verify CRC32 using Hadoop native\r\n   ````bash\r\n   # Clone Hadoop\r\n   git clone https://github.com/apache/hadoop.git\r\n   cd hadoop\r\n   \r\n   # Increase Maven memory\r\n   export MAVEN_OPTS=\"-Xmx8g -Xms6g\"\r\n   \r\n   # Build Hadoop Common (native enabled)\r\n   nohup mvn -pl hadoop-common-project/hadoop-common -am -Pnative -DskipTests clean install > build.log 2>&1 &\r\n   \r\n   # Point to built native library directory\r\n   export HADOOP_COMMON_LIB_NATIVE_DIR=\"$PWD/hadoop-common-project/hadoop-common/target/native/target/usr/local/lib\"\r\n   export LD_LIBRARY_PATH=\"$HADOOP_COMMON_LIB_NATIVE_DIR:$LD_LIBRARY_PATH\"\r\n   \r\n   # Run the CRC32 native test\r\n   nohup mvn -Pnative -Dtest=org.apache.hadoop.util.TestNativeCrc32 \\\r\n     -Djava.library.path=\"$HADOOP_COMMON_LIB_NATIVE_DIR\" test > test.log 2>&1 &\n\n\n", "PeterPtroc commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3240157752\n\n   Hi @cnauroth , could you please have a look? This PR adds RISC-V CRC32 scaffolding and keeps behavior unchanged. Thanks!\n\n\n", "PeterPtroc commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3269017471\n\n   Hi @brumi1024 , this PR has been open for a while. Could you please take a look when you have time? Thanks!\n\n\n", "PeterPtroc commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3279207867\n\n   @steveloughran\u00a0\r\n   Hi, this PR has been open for a while. Could you please take a look when you have time? Thanks!\r\n   \r\n   \r\n   \n\n\n", "steveloughran commented on code in PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#discussion_r2414452670\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32_riscv.c:\n##########\n@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+/**\n+ * RISC-V CRC32 hardware acceleration (placeholder)\n+ *\n+ * Phase 1: provide a RISC-V-specific compilation unit that currently makes\n+ * no runtime changes and falls back to the generic software path in\n+ * bulk_crc32.c. Future work will add Zbc-based acceleration and runtime\n+ * dispatch.\n+ */\n+\n+#include <assert.h>\n+#include <stddef.h> // for size_t\n+\n+#include \"bulk_crc32.h\"\n+#include \"gcc_optimizations.h\"\n+\n+/* Constructor hook reserved for future HW capability detection and\n+ * function-pointer dispatch. Intentionally a no-op for the initial phase. */\n+void __attribute__((constructor)) init_riscv_crc_support(void)\n+{\n+  /* No-op: keep using the default software implementations. */\n+}\n\nReview Comment:\n   nit: add a newline\n\n\n\n", "PeterPtroc commented on code in PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#discussion_r2415703394\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32_riscv.c:\n##########\n@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+/**\n+ * RISC-V CRC32 hardware acceleration (placeholder)\n+ *\n+ * Phase 1: provide a RISC-V-specific compilation unit that currently makes\n+ * no runtime changes and falls back to the generic software path in\n+ * bulk_crc32.c. Future work will add Zbc-based acceleration and runtime\n+ * dispatch.\n+ */\n+\n+#include <assert.h>\n+#include <stddef.h> // for size_t\n+\n+#include \"bulk_crc32.h\"\n+#include \"gcc_optimizations.h\"\n+\n+/* Constructor hook reserved for future HW capability detection and\n+ * function-pointer dispatch. Intentionally a no-op for the initial phase. */\n+void __attribute__((constructor)) init_riscv_crc_support(void)\n+{\n+  /* No-op: keep using the default software implementations. */\n+}\n\nReview Comment:\n   thanks, a newline has been added\n\n\n\n", "hadoop-yetus commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3384419165\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  31m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 35s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 34s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/branch-compile-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 35s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   3m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 34s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 34s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  cc  |   0m 34s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  golang  |   0m 34s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javac  |   0m 34s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   0m 35s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   1m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 34s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 34s |  |  ASF License check generated no output?  |\r\n   |  |   |  41m  2s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7903 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 9c4b8be6f414 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d4a02dc4d4cdadab4fab43b36f756826e4ace9ba |\r\n   | Default Java | Red Hat, Inc.-1.8.0_462-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/testReport/ |\r\n   | Max. process+thread count | 29 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3384529552\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 44s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   1m  8s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   1m  9s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/branch-compile-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 37s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   4m 24s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 26s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 28s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  cc  |   0m 28s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  golang  |   0m 28s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javac  |   0m 28s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 31s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  shadedclient  |   2m 54s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 37s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 37s |  |  ASF License check generated no output?  |\r\n   |  |   |  12m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7903 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux c62a15e0c2ed 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8fee308fe2f2e2eaf4cc8631fc41b9bcd471609d |\r\n   | Default Java | Red Hat, Inc.-1.8.0_462-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/testReport/ |\r\n   | Max. process+thread count | 51 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "PeterPtroc commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3384544541\n\n   It seems the CI failure (unable to create new native thread) is due to a resource issue on the build agent, not related to the code changes.\n\n\n", "steveloughran merged PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903\n\n\n", "steveloughran commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3385847214\n\n   Merged\r\n   * Ignored the CI failure; it does that sometimes, and as your code is #ifdef'd out, I'm not worried. Anything bigger and we'd have to retry the CI run\r\n   * added a [RISC-V] category for this change -if future work does the same then it'll be consistent.\r\n   \n\n\n", "PeterPtroc commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3386121381\n\n   Thanks @pan3793 @steveloughran for the review and the merge!\n\n\n"], "labels": ["pull-request-available"], "summary": "Establish the foundational build infrastructure for RISC-V CRC32 hardware accele", "qna": [{"question": "What is the issue title?", "answer": "Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration"}, {"question": "Who reported this issue?", "answer": "Ptroc"}]}
{"key": "HADOOP-19662", "project": "HADOOP", "title": " Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c", "status": "Open", "reporter": "Ptroc", "created": "2025-08-26T09:00:41.000+0000", "description": "## Description\r\n\u00a0\r\nThere is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30.\r\n\r\n## Current Code\r\n```c\r\n#include \"gcc_optimizations.h\"\r\n#include \"gcc_optimizations.h\"\r\n```", "comments": ["PeterPtroc opened a new pull request, #7899:\nURL: https://github.com/apache/hadoop/pull/7899\n\n   \u2026_crc32_x86.c\r\n   \r\n   Removes duplicate #include \"gcc_optimizations.h\" statement.\r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Removes duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c`.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - [x] Code compiles without issues\r\n   - [x] No functional changes expected\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7899:\nURL: https://github.com/apache/hadoop/pull/7899#issuecomment-3224065363\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  28m 36s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 57s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 50s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 107m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 54s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 43s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 14s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 223m 13s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7899 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux e8837fccd16c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c1d9fb6a820785db99554892fe312f5135201bf3 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/testReport/ |\r\n   | Max. process+thread count | 1253 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "PeterPtroc commented on PR #7899:\nURL: https://github.com/apache/hadoop/pull/7899#issuecomment-3224106808\n\n   I'll push an empty commit to re-run the tests.\n\n\n", "hadoop-yetus commented on PR #7899:\nURL: https://github.com/apache/hadoop/pull/7899#issuecomment-3224890091\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 34s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 57s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 103m 45s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  14m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  14m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  14m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 53s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 24s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m  9s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 190m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7899 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 4cb5f960ad43 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7baa586bb254bf11e9f074aaae0aebb1ddca3f87 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/testReport/ |\r\n   | Max. process+thread count | 1863 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "## Description\r\n\u00a0\r\nThere is a duplicate include statement for `gcc_optimizations", "qna": [{"question": "What is the issue title?", "answer": " Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c"}, {"question": "Who reported this issue?", "answer": "Ptroc"}]}
{"key": "HADOOP-19661", "project": "HADOOP", "title": "Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile", "status": "Resolved", "reporter": "Cheng Pan", "created": "2025-08-26T08:04:43.000+0000", "description": null, "comments": ["pan3793 opened a new pull request, #7900:\nURL: https://github.com/apache/hadoop/pull/7900\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Rocky Linux is supposed to be a drop-in replacement for the discontinued CentOS. See more details at https://rockylinux.org/about\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   ```\r\n   $ ./start-build-env.sh rockylinux_8\r\n   ...\r\n   \r\n    _   _           _                    ______\r\n   | | | |         | |                   |  _  \\\r\n   | |_| | __ _  __| | ___   ___  _ __   | | | |_____   __\r\n   |  _  |/ _` |/ _` |/ _ \\ / _ \\| '_ \\  | | | / _ \\ \\ / /\r\n   | | | | (_| | (_| | (_) | (_) | |_) | | |/ /  __/\\ V /\r\n   \\_| |_/\\__,_|\\__,_|\\___/ \\___/| .__/  |___/ \\___| \\_(_)\r\n                                 | |\r\n                                 |_|\r\n   \r\n   This is the standard Hadoop Developer build environment.\r\n   This has all the right tools installed required to build\r\n   Hadoop from source.\r\n   \r\n   [chengpan@4af99dc981b9 hadoop]$\r\n   ```\r\n   \r\n   ```\r\n   $ mvn clean install -DskipTests -Pnative -Pyarn-ui -DskipShade\r\n   ...\r\n   [INFO] Reactor Summary for Apache Hadoop Main 3.5.0-SNAPSHOT:\r\n   [INFO]\r\n   [INFO] Apache Hadoop Main ................................. SUCCESS [  0.675 s]\r\n   [INFO] Apache Hadoop Build Tools .......................... SUCCESS [  1.518 s]\r\n   [INFO] Apache Hadoop Project POM .......................... SUCCESS [  0.651 s]\r\n   [INFO] Apache Hadoop Annotations .......................... SUCCESS [  0.686 s]\r\n   [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  0.072 s]\r\n   [INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.077 s]\r\n   [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  1.476 s]\r\n   [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  0.316 s]\r\n   [INFO] Apache Hadoop Auth ................................. SUCCESS [  2.002 s]\r\n   [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  0.535 s]\r\n   [INFO] Apache Hadoop Common ............................... SUCCESS [ 18.943 s]\r\n   [INFO] Apache Hadoop NFS .................................. SUCCESS [  1.071 s]\r\n   [INFO] Apache Hadoop KMS .................................. SUCCESS [  1.085 s]\r\n   [INFO] Apache Hadoop Registry ............................. SUCCESS [  1.282 s]\r\n   [INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.046 s]\r\n   [INFO] Apache Hadoop HDFS Client .......................... SUCCESS [ 10.259 s]\r\n   [INFO] Apache Hadoop HDFS ................................. SUCCESS [ 17.829 s]\r\n   [INFO] Apache Hadoop HDFS Native Client ................... SUCCESS [02:26 min]\r\n   [INFO] Apache Hadoop HttpFS ............................... SUCCESS [  1.772 s]\r\n   [INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  1.020 s]\r\n   [INFO] Apache Hadoop YARN ................................. SUCCESS [  0.041 s]\r\n   [INFO] Apache Hadoop YARN API ............................. SUCCESS [  5.855 s]\r\n   [INFO] Apache Hadoop YARN Common .......................... SUCCESS [  4.910 s]\r\n   [INFO] Apache Hadoop YARN Server .......................... SUCCESS [  0.038 s]\r\n   [INFO] Apache Hadoop YARN Server Common ................... SUCCESS [  3.994 s]\r\n   [INFO] Apache Hadoop YARN ApplicationHistoryService ....... SUCCESS [  1.619 s]\r\n   [INFO] Apache Hadoop YARN Timeline Service ................ SUCCESS [  1.207 s]\r\n   [INFO] Apache Hadoop YARN Web Proxy ....................... SUCCESS [  1.058 s]\r\n   [INFO] Apache Hadoop YARN ResourceManager ................. SUCCESS [ 10.150 s]\r\n   [INFO] Apache Hadoop YARN NodeManager ..................... SUCCESS [ 28.435 s]\r\n   [INFO] Apache Hadoop YARN Server Tests .................... SUCCESS [  1.134 s]\r\n   [INFO] Apache Hadoop YARN Client .......................... SUCCESS [  1.959 s]\r\n   [INFO] Apache Hadoop MapReduce Client ..................... SUCCESS [  0.616 s]\r\n   [INFO] Apache Hadoop MapReduce Core ....................... SUCCESS [  4.440 s]\r\n   [INFO] Apache Hadoop MapReduce Common ..................... SUCCESS [  2.062 s]\r\n   [INFO] Apache Hadoop MapReduce Shuffle .................... SUCCESS [  1.414 s]\r\n   [INFO] Apache Hadoop MapReduce App ........................ SUCCESS [  2.901 s]\r\n   [INFO] Apache Hadoop MapReduce HistoryServer .............. SUCCESS [  1.887 s]\r\n   [INFO] Apache Hadoop MapReduce JobClient .................. SUCCESS [  3.319 s]\r\n   [INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [  1.954 s]\r\n   [INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  1.113 s]\r\n   [INFO] Apache Hadoop Federation Balance ................... SUCCESS [  1.234 s]\r\n   [INFO] Apache Hadoop HDFS-RBF ............................. SUCCESS [  5.733 s]\r\n   [INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN SharedCacheManager .............. SUCCESS [  0.886 s]\r\n   [INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SUCCESS [  0.866 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Backend ... SUCCESS [  0.035 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Common .... SUCCESS [  1.291 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Client .... SUCCESS [  1.709 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Servers ... SUCCESS [  0.036 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Server 2.5  SUCCESS [  1.677 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase tests ..... SUCCESS [  1.493 s]\r\n   [INFO] Apache Hadoop YARN Router .......................... SUCCESS [  1.912 s]\r\n   [INFO] Apache Hadoop YARN TimelineService DocumentStore ... SUCCESS [  1.018 s]\r\n   [INFO] Apache Hadoop YARN GlobalPolicyGenerator ........... SUCCESS [  1.175 s]\r\n   [INFO] Apache Hadoop YARN Applications .................... SUCCESS [  0.036 s]\r\n   [INFO] Apache Hadoop YARN DistributedShell ................ SUCCESS [  1.166 s]\r\n   [INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SUCCESS [  0.689 s]\r\n   [INFO] Apache Hadoop YARN Services ........................ SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN Services Core ................... SUCCESS [  2.393 s]\r\n   [INFO] Apache Hadoop YARN Services API .................... SUCCESS [  1.552 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog ............. SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog Webapp ...... SUCCESS [ 10.674 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog Docker Image  SUCCESS [  0.046 s]\r\n   [INFO] Apache Hadoop YARN Application MaWo ................ SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN Application MaWo Core ........... SUCCESS [  0.781 s]\r\n   [INFO] Apache Hadoop YARN Site ............................ SUCCESS [  0.036 s]\r\n   [INFO] Apache Hadoop YARN Registry ........................ SUCCESS [  0.439 s]\r\n   [INFO] Apache Hadoop YARN UI .............................. SUCCESS [ 51.592 s]\r\n   [INFO] Apache Hadoop YARN CSI ............................. SUCCESS [  2.776 s]\r\n   [INFO] Apache Hadoop YARN Project ......................... SUCCESS [  1.155 s]\r\n   [INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SUCCESS [  0.854 s]\r\n   [INFO] Apache Hadoop MapReduce NativeTask ................. SUCCESS [ 19.795 s]\r\n   [INFO] Apache Hadoop MapReduce Uploader ................... SUCCESS [  0.777 s]\r\n   [INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  1.427 s]\r\n   [INFO] Apache Hadoop MapReduce ............................ SUCCESS [  1.079 s]\r\n   [INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [  1.533 s]\r\n   [INFO] Apache Hadoop Client Aggregator .................... SUCCESS [  0.642 s]\r\n   [INFO] Apache Hadoop Dynamometer Workload Simulator ....... SUCCESS [  1.110 s]\r\n   [INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SUCCESS [  2.098 s]\r\n   [INFO] Apache Hadoop Dynamometer Block Listing Generator .. SUCCESS [  0.994 s]\r\n   [INFO] Apache Hadoop Dynamometer Dist ..................... SUCCESS [  1.063 s]\r\n   [INFO] Apache Hadoop Dynamometer .......................... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop Archives ............................. SUCCESS [  0.908 s]\r\n   [INFO] Apache Hadoop Archive Logs ......................... SUCCESS [  1.125 s]\r\n   [INFO] Apache Hadoop Rumen ................................ SUCCESS [  1.466 s]\r\n   [INFO] Apache Hadoop Gridmix .............................. SUCCESS [  1.450 s]\r\n   [INFO] Apache Hadoop Data Join ............................ SUCCESS [  1.002 s]\r\n   [INFO] Apache Hadoop Extras ............................... SUCCESS [  0.995 s]\r\n   [INFO] Apache Hadoop Pipes ................................ SUCCESS [  3.333 s]\r\n   [INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [  4.599 s]\r\n   [INFO] Apache Hadoop Kafka Library support ................ SUCCESS [  0.636 s]\r\n   [INFO] Apache Hadoop Azure support ........................ SUCCESS [  4.082 s]\r\n   [INFO] Apache Hadoop Aliyun OSS support ................... SUCCESS [  0.878 s]\r\n   [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  1.312 s]\r\n   [INFO] Apache Hadoop Resource Estimator Service ........... SUCCESS [  0.868 s]\r\n   [INFO] Apache Hadoop Azure Data Lake support .............. SUCCESS [  0.844 s]\r\n   [INFO] Apache Hadoop Image Generation Tool ................ SUCCESS [  0.893 s]\r\n   [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [  0.624 s]\r\n   [INFO] Apache Hadoop OpenStack support .................... SUCCESS [  0.042 s]\r\n   [INFO] Apache Hadoop Common Benchmark ..................... SUCCESS [  9.870 s]\r\n   [INFO] Apache Hadoop Compatibility Benchmark .............. SUCCESS [  0.598 s]\r\n   [INFO] Apache Hadoop Tools ................................ SUCCESS [  0.032 s]\r\n   [INFO] Apache Hadoop Client API ........................... SUCCESS [  0.851 s]\r\n   [INFO] Apache Hadoop Client Runtime ....................... SUCCESS [  0.639 s]\r\n   [INFO] Apache Hadoop Client Packaging Invariants .......... SUCCESS [  0.373 s]\r\n   [INFO] Apache Hadoop Client Test Minicluster .............. SUCCESS [  0.690 s]\r\n   [INFO] Apache Hadoop Client Packaging Invariants for Test . SUCCESS [  0.171 s]\r\n   [INFO] Apache Hadoop Client Packaging Integration Tests ... SUCCESS [  0.639 s]\r\n   [INFO] Apache Hadoop Distribution ......................... SUCCESS [  0.447 s]\r\n   [INFO] Apache Hadoop Client Modules ....................... SUCCESS [  0.032 s]\r\n   [INFO] Apache Hadoop Tencent COS Support .................. SUCCESS [  0.640 s]\r\n   [INFO] Apache Hadoop OBS support .......................... SUCCESS [  1.068 s]\r\n   [INFO] Apache Hadoop Volcano Engine Services support ...... SUCCESS [  1.601 s]\r\n   [INFO] Apache Hadoop Cloud Storage ........................ SUCCESS [  0.401 s]\r\n   [INFO] Apache Hadoop Cloud Storage Project ................ SUCCESS [  0.033 s]\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] BUILD SUCCESS\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] Total time:  07:35 min\r\n   [INFO] Finished at: 2025-08-26T10:05:06Z\r\n   [INFO] ------------------------------------------------------------------------\r\n   [chengpan@4af99dc981b9 hadoop]$\r\n   ```\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223525111\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   0m 21s |  |  Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7900 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223527763\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   0m 21s |  |  Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7900 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223534265\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   0m 21s |  |  Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900@2/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7900 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/3/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223579339\n\n   Looks like Yetus finds Dockerfile from the trunk branch instead of the PR branch, I'm not very familiar with this part. If the behavior is not easy to change, I'm afraid this PR must be committed first, then we will know what is going to happen.\r\n   ```\r\n   [2025-08-26T10:13:14.863Z] Already on 'trunk'\r\n   [2025-08-26T10:13:14.863Z] Your branch is up to date with 'origin/trunk'.\r\n   [2025-08-26T10:13:14.863Z] HEAD is now at 9d2a83d18ba HADOOP-19636. [JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. (#7822)\r\n   [2025-08-26T10:13:15.444Z] \r\n   [2025-08-26T10:13:15.444Z] Testing https://github.com/apache/hadoop/pull/7900 patch on trunk.\r\n   [2025-08-26T10:13:15.444Z] ERROR: Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900@2/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.\r\n   ```\n\n\n", "slfan1989 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226472958\n\n   > Looks like Yetus finds Dockerfile from the trunk branch instead of the PR branch, I'm not very familiar with this part. If the behavior is not easy to change, I'm afraid this PR must be committed first, then we will know what is going to happen.\r\n   > \r\n   > ```\r\n   > [2025-08-26T10:13:14.863Z] Already on 'trunk'\r\n   > [2025-08-26T10:13:14.863Z] Your branch is up to date with 'origin/trunk'.\r\n   > [2025-08-26T10:13:14.863Z] HEAD is now at 9d2a83d18ba HADOOP-19636. [JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. (#7822)\r\n   > [2025-08-26T10:13:15.444Z] \r\n   > [2025-08-26T10:13:15.444Z] Testing https://github.com/apache/hadoop/pull/7900 patch on trunk.\r\n   > [2025-08-26T10:13:15.444Z] ERROR: Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900@2/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.\r\n   > ```\r\n   \r\n   @pan3793 Currently, Hadoop's Yetus performs the build twice: first with the trunk code, and then again after applying the patch. At this point, I\u2019m not certain whether what you pointed out is actually an issue.\r\n   \r\n   @aajisaka @ayushtkn @GauthamBanasandra Could you please take a look at this issue? Thank you very much!\n\n\n", "aajisaka commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226645951\n\n   Would you push the commit to a new branch and then create a test PR based on the branch to check Yetus is working fine?\n\n\n", "hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226672783\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   0m 20s |  |  Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7900 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/4/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226674179\n\n   > Would you push the commit to a new branch and then create a test PR based on the branch to check Yetus is working fine?\r\n   \r\n   @aajisaka I don't have the permission to commit hadoop repo ...\n\n\n", "slfan1989 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226783929\n\n   > > Would you push the commit to a new branch and then create a test PR based on the branch to check Yetus is working fine?\r\n   > \r\n   > @aajisaka I don't have the permission to commit hadoop repo ...\r\n   \r\n   @pan3793 let\u2019s keep the file name unchanged as `Dockerfile_centos_8`. If needed, I can help create a branch.\n\n\n", "pan3793 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3227608632\n\n   with the help of @slfan1989, branch `HADOOP-19661` was created at `apache/hadoop` repo, I opened https://github.com/apache/hadoop/pull/7898 targeting branch `HADOOP-19661`, the `Dockerfile_rockylinux_8 not found` issue is gone, but it seems platform change is not detected thus the Docker building test was skipped ...\r\n   \r\n   <img width=\"1201\" height=\"408\" alt=\"image\" src=\"https://github.com/user-attachments/assets/cb8f314b-3c7a-41b0-81ee-cea21d3f04e3\" />\r\n   \r\n   @aajisaka do you have other suggestions?\n\n\n", "slfan1989 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3231574620\n\n   > with the help of @slfan1989, branch `HADOOP-19661` was created at `apache/hadoop` repo, I opened #7898 targeting branch `HADOOP-19661`, the `Dockerfile_rockylinux_8 not found` issue is gone, but it seems platform change is not detected thus the Docker building test was skipped ...\r\n   > \r\n   > <img alt=\"image\" width=\"1201\" height=\"408\" src=\"https://private-user-images.githubusercontent.com/26535726/482609828-cb8f314b-3c7a-41b0-81ee-cea21d3f04e3.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTYzNDk4ODgsIm5iZiI6MTc1NjM0OTU4OCwicGF0aCI6Ii8yNjUzNTcyNi80ODI2MDk4MjgtY2I4ZjMxNGItM2M3YS00MWIwLTgxZWUtY2VhMjFkM2YwNGUzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODI4VDAyNTMwOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJiNTY3MTUzMmY0ZWJlYWIzMTMwODhhODMyNzhlMjcxZGQxNzhiYjcwYzdkNTI1NDU1Zjk0M2U4ZTAwODAyZDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.8bGH6guvb6ZgikJ021xLnCWaef1WTDj3ge5b1cbxxcA\">\r\n   > @aajisaka do you have other suggestions?\r\n   \r\n   @pan3793 Although we can compile locally, there are some issues with Yetus, so I cannot confirm whether this PR will affect other team members' code submissions. I still need @aajisaka @GauthamBanasandra  to help verify it.\r\n   \r\n   cc: @steveloughran @ayushtkn \n\n\n", "pan3793 opened a new pull request, #7913:\nURL: https://github.com/apache/hadoop/pull/7913\n\n   (no comment)\n\n\n", "hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3233914038\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/5/console in case of problems.\r\n   \n\n\n", "pan3793 closed pull request #7913: DO-NOT-MERGE. HADOOP-19661. Test auxiliary patch\nURL: https://github.com/apache/hadoop/pull/7913\n\n\n", "hadoop-yetus commented on PR #7913:\nURL: https://github.com/apache/hadoop/pull/7913#issuecomment-3234393664\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 45s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ HADOOP-19661 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 44s |  |  HADOOP-19661 passed  |\r\n   | +1 :green_heart: |  compile  |   3m 36s |  |  HADOOP-19661 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   3m 36s |  |  HADOOP-19661 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  HADOOP-19661 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  85m  0s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   3m 46s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  cc  |   3m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |   3m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   3m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   3m 40s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  cc  |   3m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |   3m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   3m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  10m  8s |  |  hadoop-hdfs-native-client in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 156m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7913/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7913 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux c6281ba91ece 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | HADOOP-19661 / df44bca1341e8d0b97bc0d7d75bc90fb50ec87b8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7913/1/testReport/ |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client U: hadoop-hdfs-project/hadoop-hdfs-native-client |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7913/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3234661592\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  | 106m 45s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  | 107m 19s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  20m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  60m 45s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  16m 17s |  |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 45s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 245m 14s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7900 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint mvnsite unit jsonlint |\r\n   | uname | Linux 5842e2ca7744 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b2e037215b456012892b30f6e4adcca349e53794 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/5/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/5/console |\r\n   | versions | git=2.43.7 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235763986\n\n   I have to rename it back to workaround the `Dockerfile_rockylinux_8 not found` issue in https://github.com/apache/hadoop/pull/7900/commits/b2e037215b456012892b30f6e4adcca349e53794.\r\n   \r\n   @slfan1989 @aajisaka, please take a look, and ping me to revert the renaming change if you think it's ready to go.\n\n\n", "slfan1989 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235795315\n\n   @pan3793 LGTM. I don't see any issues. I checked the compilation results and didn't find any problems.\n\n\n", "slfan1989 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235798705\n\n   @cnauroth @stoty Could you please help review this PR? Thank you very much!\n\n\n", "cnauroth closed pull request #7900: HADOOP-19661. Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile\nURL: https://github.com/apache/hadoop/pull/7900\n\n\n", "hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235821612\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 20s |  |  https://github.com/apache/hadoop/pull/7900 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7900 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/6/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235821982\n\n   @cnauroth I need to revert the renaming change before committing to trunk ... let me send a follow up to address it\n\n\n", "cnauroth commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235825803\n\n   > @cnauroth I need to revert the renaming change before committing to trunk ... let me send a follow up to address it\r\n   \r\n   @pan3793 , oops, I committed. :-D LMK, and I'll watch for more patches required.\n\n\n", "pan3793 opened a new pull request, #7917:\nURL: https://github.com/apache/hadoop/pull/7917\n\n   I have to keep the Dockerfile name to bypass Yetus issue, see discussion in https://github.com/apache/hadoop/pull/7900\r\n   \r\n   But should rename it back before committing to trunk, since the thing has already happened, send a followup to fix it.\n\n\n", "pan3793 commented on PR #7917:\nURL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235833407\n\n   @cnauroth \n\n\n", "hadoop-yetus commented on PR #7917:\nURL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235839287\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   0m 21s |  |  Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7917/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7917 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7917/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #7917:\nURL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235842248\n\n   no need to wait for Yetus's report since Yetus will fail with `Dockerfile_rockylinux_8 not found` immediately\n\n\n", "slfan1989 commented on PR #7917:\nURL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235854584\n\n   @cnauroth I suggest waiting for a day to see if tomorrow's auto-build email triggers RockyLinux_8 completely. If it doesn't trigger properly, we'll look for a solution.\r\n   \r\n   cc: @pan3793\n\n\n", "slfan1989 commented on PR #7917:\nURL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235873037\n\n   > @cnauroth I suggest waiting for a day to see if tomorrow's auto-build email triggers RockyLinux_8 completely. If it doesn't trigger properly, we'll look for a solution.\r\n   > \r\n   > cc: @pan3793\r\n   \r\n   @cnauroth @pan3793 \r\n   \r\n   After offline discussion with Pan, we've decided to merge this PR for now. If any issues arise, we'll make updates accordingly.\n\n\n", "slfan1989 merged PR #7917:\nURL: https://github.com/apache/hadoop/pull/7917\n\n\n", "pan3793 opened a new pull request, #7931:\nURL: https://github.com/apache/hadoop/pull/7931\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   The issue is identified by https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7760/\r\n   \r\n   ```\r\n   00:08:13                Reason | Tests\r\n   00:08:13   Failed junit tests  |  hadoop.fs.compat.common.TestHdfsCompatShellCommand \r\n   00:08:13                       |  hadoop.fs.compat.common.TestHdfsCompatDefaultSuites \r\n   00:08:13                       |  hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints \r\n   ```\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Manually checked the failed tests after installing the missing deps inside the dev container.\r\n   \r\n   ```\r\n   $ ./start-build-env.sh rockylinux_8\r\n   ```\r\n   \r\n   ```\r\n   $ mvn clean install -DskipTests -Pnative\r\n   ```\r\n   \r\n   ```\r\n   $ mvn test -pl :hadoop-compat-bench -Dtest=hadoop.fs.compat.common.TestHdfsCompatShellCommand\r\n   ...\r\n   [INFO]  T E S T S\r\n   [INFO] -------------------------------------------------------\r\n   [INFO] Running org.apache.hadoop.fs.compat.common.TestHdfsCompatShellCommand\r\n   [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.64 s ", "hadoop-yetus commented on PR #7931:\nURL: https://github.com/apache/hadoop/pull/7931#issuecomment-3253960302\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  24m  9s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 52s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  44m 17s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 46s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 58s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 106m 24s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7931 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux e8aea56e1b0c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2ba58898a54a722abb39526a9e95468802f00470 |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/console |\r\n   | versions | git=2.43.7 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7931:\nURL: https://github.com/apache/hadoop/pull/7931#issuecomment-3254275709\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 24s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 23s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 22s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  25m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  79m  7s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7931 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux 912e624ed05f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2ba58898a54a722abb39526a9e95468802f00470 |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/console |\r\n   | versions | git=2.30.2 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7931:\nURL: https://github.com/apache/hadoop/pull/7931#issuecomment-3254585161\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 45s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  5s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  72m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7931 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux 53ab742d41d0 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2ba58898a54a722abb39526a9e95468802f00470 |\r\n   | Max. process+thread count | 561 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7931:\nURL: https://github.com/apache/hadoop/pull/7931\n\n\n", "slfan1989 commented on PR #7931:\nURL: https://github.com/apache/hadoop/pull/7931#issuecomment-3257273745\n\n   @pan3793 Thanks for the contribution! Merged into trunk.\n\n\n", "pan3793 opened a new pull request, #7938:\nURL: https://github.com/apache/hadoop/pull/7938\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   I see some CI starts to fail due to\r\n   \r\n   https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7760/27/console\r\n   \r\n   ```\r\n   23:59:25  58.82 Last metadata expiration check: 0:00:40 ago on Sat Sep  6 15:58:37 2025.\r\n   23:59:25  59.80 No match for argument: bats\r\n   ```\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   ```\r\n   $ ./start-build-env.sh rockylinux_8\r\n   ...\r\n    _   _           _                    ______\r\n   | | | |         | |                   |  _  \\\r\n   | |_| | __ _  __| | ___   ___  _ __   | | | |_____   __\r\n   |  _  |/ _` |/ _` |/ _ \\ / _ \\| '_ \\  | | | / _ \\ \\ / /\r\n   | | | | (_| | (_| | (_) | (_) | |_) | | |/ /  __/\\ V /\r\n   \\_| |_/\\__,_|\\__,_|\\___/ \\___/| .__/  |___/ \\___| \\_(_)\r\n                                 | |\r\n                                 |_|\r\n   \r\n   This is the standard Hadoop Developer build environment.\r\n   This has all the right tools installed required to build\r\n   Hadoop from source.\r\n   \r\n   [chengpan@c79114c5d3d4 hadoop]$\r\n   ```\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7938:\nURL: https://github.com/apache/hadoop/pull/7938#issuecomment-3263376365\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/console in case of problems.\r\n   \n\n\n", "hadoop-yetus commented on PR #7938:\nURL: https://github.com/apache/hadoop/pull/7938#issuecomment-3263454078\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  31m 16s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 21s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  52m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 46s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 40s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 52s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 128m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7938 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs |\r\n   | uname | Linux 11b4c3233f9f 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 117e7c2d608809997f7eeb57f6f6a5e924595dc7 |\r\n   | Max. process+thread count | 538 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/console |\r\n   | versions | git=2.43.7 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7938:\nURL: https://github.com/apache/hadoop/pull/7938#issuecomment-3263456222\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/console in case of problems.\r\n   \n\n\n", "hadoop-yetus commented on PR #7938:\nURL: https://github.com/apache/hadoop/pull/7938#issuecomment-3263508181\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 55s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  hadolint  |   0m  1s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  85m 13s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7938 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs |\r\n   | uname | Linux 1632828a4b30 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 117e7c2d608809997f7eeb57f6f6a5e924595dc7 |\r\n   | Max. process+thread count | 525 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7938:\nURL: https://github.com/apache/hadoop/pull/7938#issuecomment-3264759129\n\n   @pan3793 Thank you for your contribution, but I think we still need other members to take another look.\r\n   \r\n   @GauthamBanasandra Could you please help review this PR? Thank you very much!\n\n\n", "slfan1989 commented on PR #7938:\nURL: https://github.com/apache/hadoop/pull/7938#issuecomment-3276952959\n\n   @GauthamBanasandra Thanks for helping with the review \u2014 LGTM.\n\n\n", "slfan1989 commented on PR #7938:\nURL: https://github.com/apache/hadoop/pull/7938#issuecomment-3276955337\n\n   @pan3793 Thank you for your contribution! If there are no further comments today, this PR will be merged.\n\n\n", "slfan1989 merged PR #7938:\nURL: https://github.com/apache/hadoop/pull/7938\n\n\n", "slfan1989 commented on PR #7938:\nURL: https://github.com/apache/hadoop/pull/7938#issuecomment-3282884265\n\n   @pan3793 Thanks for the contribution! @GauthamBanasandra Thanks for the review!\n\n\n"], "labels": ["pull-request-available"], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile"}, {"question": "Who reported this issue?", "answer": "Cheng Pan"}]}
{"key": "HADOOP-19660", "project": "HADOOP", "title": "ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider", "status": "Open", "reporter": "Anuj Modi", "created": "2025-08-26T06:53:52.000+0000", "description": "Externally Reported Enhancement:\r\n\r\n*Current Limitation*\r\nThe current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources\r\n\r\n{*}Use Case{*}:\u00a0*Kubernetes TokenRequest API*\u00a0\r\nIn modern Kubernetes environments, the recommended approach is to use the TokenRequest API to generate short-lived, on-demand service account tokens rather than relying on projected volume mounts.\r\n\r\n*Proposed Enhancement*\u00a0\r\nI propose modifying WorkloadIdentityTokenProvider to accept a Supplier for token retrieval instead of being hardcoded to file operations:", "comments": ["[~anujmodi] can you assign this to me?", "Draft PR without test to see if the structure looks okay - [https://github.com/apache/hadoop/pull/7901]\r\n\r\ncc: [~anujmodi]\u00a0", "Its not showing your name somehow", "hadoop-yetus commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3224527888\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 38s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  1s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/artifact/out/blanks-eol.txt) |  The patch has 17 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 54s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 166m 34s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7901 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 91770e23ae15 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / dee5a7d480a91f1099224b2565ed93ab8587e580 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/testReport/ |\r\n   | Max. process+thread count | 607 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3256894074\n\n   Thanks for the patch @kunalmnnit \r\n   We will review this PR and add comments if any.\n\n\n", "anmolanmol1234 commented on code in PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#discussion_r2344037166\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java:\n##########\n@@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider {\n   private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \";\n   private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \";\n \n+  /**\n+   * Internal implementation of ClientAssertionProvider for file-based token reading.\n+   * This provides backward compatibility for the file-based constructor.\n+   */\n+  private static class FileBasedClientAssertionProvider implements ClientAssertionProvider {\n+    private final String tokenFile;\n+\n+    public FileBasedClientAssertionProvider(String tokenFile) {\n+      this.tokenFile = tokenFile;\n+    }\n+\n+    @Override\n+    public void initialize(Configuration configuration, String accountName) throws IOException {\n+      // No initialization needed for file-based provider\n+    }\n+\n+    @Override\n+    public String getClientAssertion() throws IOException {\n+      String clientAssertion = \"\";\n+      try {\n+        File file = new File(tokenFile);\n+        clientAssertion = FileUtils.readFileToString(file, \"UTF-8\");\n\nReview Comment:\n   encoding should come from constants, should not be hardcoded\n\n\n\n", "anmolanmol1234 commented on code in PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#discussion_r2344048057\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java:\n##########\n@@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider {\n   private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \";\n   private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \";\n \n+  /**\n+   * Internal implementation of ClientAssertionProvider for file-based token reading.\n+   * This provides backward compatibility for the file-based constructor.\n+   */\n+  private static class FileBasedClientAssertionProvider implements ClientAssertionProvider {\n+    private final String tokenFile;\n+\n+    public FileBasedClientAssertionProvider(String tokenFile) {\n+      this.tokenFile = tokenFile;\n+    }\n+\n+    @Override\n+    public void initialize(Configuration configuration, String accountName) throws IOException {\n+      // No initialization needed for file-based provider\n+    }\n+\n+    @Override\n+    public String getClientAssertion() throws IOException {\n+      String clientAssertion = \"\";\n+      try {\n+        File file = new File(tokenFile);\n+        clientAssertion = FileUtils.readFileToString(file, \"UTF-8\");\n+      } catch (Exception e) {\n+        throw new IOException(TOKEN_FILE_READ_ERROR + tokenFile, e);\n+      }\n+      if (Strings.isNullOrEmpty(clientAssertion)) {\n\nReview Comment:\n   If the token only contains whitespaces the empty check will pass, token can be trimmed before checking for empty.\n\n\n\n", "anmolanmol1234 commented on code in PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#discussion_r2344049708\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java:\n##########\n@@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider {\n   private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \";\n   private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \";\n \n+  /**\n+   * Internal implementation of ClientAssertionProvider for file-based token reading.\n+   * This provides backward compatibility for the file-based constructor.\n+   */\n+  private static class FileBasedClientAssertionProvider implements ClientAssertionProvider {\n+    private final String tokenFile;\n+\n+    public FileBasedClientAssertionProvider(String tokenFile) {\n+      this.tokenFile = tokenFile;\n+    }\n+\n+    @Override\n+    public void initialize(Configuration configuration, String accountName) throws IOException {\n+      // No initialization needed for file-based provider\n+    }\n+\n+    @Override\n+    public String getClientAssertion() throws IOException {\n+      String clientAssertion = \"\";\n\nReview Comment:\n   use constant for EMPTY_STRING\n\n\n\n", "anmolanmol1234 commented on code in PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#discussion_r2344054829\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java:\n##########\n@@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider {\n   private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \";\n   private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \";\n \n+  /**\n+   * Internal implementation of ClientAssertionProvider for file-based token reading.\n+   * This provides backward compatibility for the file-based constructor.\n+   */\n+  private static class FileBasedClientAssertionProvider implements ClientAssertionProvider {\n+    private final String tokenFile;\n+\n+    public FileBasedClientAssertionProvider(String tokenFile) {\n+      this.tokenFile = tokenFile;\n+    }\n+\n+    @Override\n+    public void initialize(Configuration configuration, String accountName) throws IOException {\n+      // No initialization needed for file-based provider\n+    }\n+\n+    @Override\n+    public String getClientAssertion() throws IOException {\n+      String clientAssertion = \"\";\n+      try {\n+        File file = new File(tokenFile);\n+        clientAssertion = FileUtils.readFileToString(file, \"UTF-8\");\n\nReview Comment:\n   Here we are reading the whole token file as a string every time getClientAssertion() is called. If file is large or accessed frequently, it could be inefficient. Can we cache the value until the token provider explicitly refreshes ?\n\n\n\n", "anmolanmol1234 commented on code in PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#discussion_r2344089164\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java:\n##########\n@@ -20,13 +20,13 @@\n \n import java.io.File;\n import java.io.IOException;\n-\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n import org.apache.commons.io.FileUtils;\n import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.thirdparty.com.google.common.base.Strings;\n import org.apache.hadoop.util.Preconditions;\n+import org.slf4j.Logger;\n\nReview Comment:\n   Import ordering is incorrect\r\n   \r\n   import java.io.File;\r\n   import java.io.IOException;\r\n   \r\n   import org.slf4j.Logger;\r\n   import org.slf4j.LoggerFactory;\r\n   \r\n   import org.apache.commons.io.FileUtils;\r\n   import org.apache.hadoop.classification.VisibleForTesting;\r\n   import org.apache.hadoop.conf.Configuration;\r\n   import org.apache.hadoop.thirdparty.com.google.common.base.Strings;\r\n   import org.apache.hadoop.util.Preconditions;\n\n\n\n", "kunalmnnit commented on code in PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#discussion_r2347334850\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java:\n##########\n@@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider {\n   private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \";\n   private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \";\n \n+  /**\n+   * Internal implementation of ClientAssertionProvider for file-based token reading.\n+   * This provides backward compatibility for the file-based constructor.\n+   */\n+  private static class FileBasedClientAssertionProvider implements ClientAssertionProvider {\n+    private final String tokenFile;\n+\n+    public FileBasedClientAssertionProvider(String tokenFile) {\n+      this.tokenFile = tokenFile;\n+    }\n+\n+    @Override\n+    public void initialize(Configuration configuration, String accountName) throws IOException {\n+      // No initialization needed for file-based provider\n+    }\n+\n+    @Override\n+    public String getClientAssertion() throws IOException {\n+      String clientAssertion = \"\";\n+      try {\n+        File file = new File(tokenFile);\n+        clientAssertion = FileUtils.readFileToString(file, \"UTF-8\");\n\nReview Comment:\n   Can we take this optimization in subsequent PR since this was existing piece of code?\r\n   https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java#L103-L115\n\n\n\n", "kunalmnnit commented on code in PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#discussion_r2347335431\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java:\n##########\n@@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider {\n   private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \";\n   private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \";\n \n+  /**\n+   * Internal implementation of ClientAssertionProvider for file-based token reading.\n+   * This provides backward compatibility for the file-based constructor.\n+   */\n+  private static class FileBasedClientAssertionProvider implements ClientAssertionProvider {\n+    private final String tokenFile;\n+\n+    public FileBasedClientAssertionProvider(String tokenFile) {\n+      this.tokenFile = tokenFile;\n+    }\n+\n+    @Override\n+    public void initialize(Configuration configuration, String accountName) throws IOException {\n+      // No initialization needed for file-based provider\n+    }\n+\n+    @Override\n+    public String getClientAssertion() throws IOException {\n+      String clientAssertion = \"\";\n+      try {\n+        File file = new File(tokenFile);\n+        clientAssertion = FileUtils.readFileToString(file, \"UTF-8\");\n\nReview Comment:\n   Additionally, this will only be invoked when the actual AAD token is expired which is roughly every hour and directly coincides with expiry of KSA token so don't think this will be unnecessarily invoked. wdyt?\n\n\n\n", "kunalmnnit commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289649227\n\n   Thanks @anmolanmol1234 for the review. PTAL again for second pass.\n\n\n", "hadoop-yetus commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289669768\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  56m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/2/artifact/out/blanks-eol.txt) |  The patch has 17 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 53s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 174m 53s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7901 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux e0210d80824e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e368af43811f9fdd335c0e43ac1d5e0c2d1f33f0 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/2/testReport/ |\r\n   | Max. process+thread count | 575 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289765623\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  58m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/3/artifact/out/blanks-eol.txt) |  The patch has 17 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 56s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 156m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7901 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3f98b80109aa 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 33ec902359aea0f83b7a9dfe6cea176f740f3878 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/3/testReport/ |\r\n   | Max. process+thread count | 584 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289771396\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   9m 30s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  33m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/5/artifact/out/blanks-eol.txt) |  The patch has 45 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 48s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  94m 55s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7901 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 9639e6676688 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d5030434aa067cdc77378bb13bac5579ecaea881 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/5/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289810843\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  58m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 48s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 10s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/4/artifact/out/blanks-eol.txt) |  The patch has 16 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 58s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 53s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 157m 10s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7901 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 42faac865c3a 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 992ac17ec35adde9c4f27c307c0362a5d6d79cb0 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/4/testReport/ |\r\n   | Max. process+thread count | 587 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3291004596\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  3s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/6/artifact/out/blanks-eol.txt) |  The patch has 33 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 46s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 51s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 154m 30s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7901 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 06eef491e2ea 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9495f55f855656096b3d593248a2e8b74f51de26 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/6/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3291863724\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  55m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 47s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  44m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/7/artifact/out/blanks-eol.txt) |  The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 56s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 159m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7901 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7f151f599e70 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 68a5b0b9735b0bfd79a577d7ce292ad2399be467 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/7/testReport/ |\r\n   | Max. process+thread count | 530 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3293541604\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  56m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  45m  0s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  44m 22s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  7s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 163m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7901 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 587337ec2872 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 31906ea3983a99845ff3205fdb889fafdb7fe89b |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/8/testReport/ |\r\n   | Max. process+thread count | 525 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "kunalmnnit commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3319670981\n\n   @anujmodi2021 @anmolanmol1234 Appreciate your review here again. Thanks!\n\n\n", "kunalmnnit commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3336856485\n\n   @anmolanmol1234 Gentle bump if you could please have a pass. This change would be really helpful for us to use k8s TokenRequest API\n\n\n", "kunalmnnit commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3337137345\n\n   @anmolanmol1234 Thank you for the approval. Would you know who would be able to help with merge? I do not seem to have access\n\n\n", "anmolanmol1234 commented on PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901#issuecomment-3337219417\n\n   @anujmodi2021 should be able to help with the merge\n\n\n", "anujmodi2021 merged PR #7901:\nURL: https://github.com/apache/hadoop/pull/7901\n\n\n"], "labels": ["pull-request-available"], "summary": "Externally Reported Enhancement:\r\n\r\n*Current Limitation*\r\nThe current WorkloadId", "qna": [{"question": "What is the issue title?", "answer": "ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider"}, {"question": "Who reported this issue?", "answer": "Anuj Modi"}]}
{"key": "HADOOP-19659", "project": "HADOOP", "title": "Upgrade Debian 10 to 11 in build env Dockerfile", "status": "Resolved", "reporter": "Cheng Pan", "created": "2025-08-26T03:33:48.000+0000", "description": "Debian 10 EOL, and the apt repo is unavailable\r\n{code:bash}\r\ndocker run --rm -it debian:10 bash\r\nroot@bc2a4c509cb3:/# apt update\r\nIgn:1 http://deb.debian.org/debian buster InRelease\r\nIgn:2 http://deb.debian.org/debian-security buster/updates InRelease\r\nIgn:3 http://deb.debian.org/debian buster-updates InRelease\r\nErr:4 http://deb.debian.org/debian buster Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:5 http://deb.debian.org/debian-security buster/updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:6 http://deb.debian.org/debian buster-updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nReading package lists... Done\r\nE: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian-security buster/updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian buster-updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nroot@bc2a4c509cb3:/#\r\n{code}", "comments": ["pan3793 opened a new pull request, #7898:\nURL: https://github.com/apache/hadoop/pull/7898\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Debian 10 EOL, and the apt repo is unavailable, this PR upgrades it to Debian 11\r\n   \r\n   ```\r\n   docker run --rm -it debian:10 bash\r\n   root@bc2a4c509cb3:/# apt update\r\n   Ign:1 http://deb.debian.org/debian buster InRelease\r\n   Ign:2 http://deb.debian.org/debian-security buster/updates InRelease\r\n   Ign:3 http://deb.debian.org/debian buster-updates InRelease\r\n   Err:4 http://deb.debian.org/debian buster Release\r\n     404  Not Found [IP: 151.101.90.132 80]\r\n   Err:5 http://deb.debian.org/debian-security buster/updates Release\r\n     404  Not Found [IP: 151.101.90.132 80]\r\n   Err:6 http://deb.debian.org/debian buster-updates Release\r\n     404  Not Found [IP: 151.101.90.132 80]\r\n   Reading package lists... Done\r\n   E: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file.\r\n   N: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\n   N: See apt-secure(8) manpage for repository creation and user configuration details.\r\n   E: The repository 'http://deb.debian.org/debian-security buster/updates Release' does not have a Release file.\r\n   N: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\n   N: See apt-secure(8) manpage for repository creation and user configuration details.\r\n   E: The repository 'http://deb.debian.org/debian buster-updates Release' does not have a Release file.\r\n   N: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\n   N: See apt-secure(8) manpage for repository creation and user configuration details.\r\n   root@bc2a4c509cb3:/#\r\n   ```\r\n   \r\n   [Debian 11 will be EOL after 31 Aug 2026](https://endoflife.date/debian). I didn't switch it to Debian 12 or 13 because the new version does not have `openjdk-11-jdk` in the apt repo.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   ```\r\n   $ ./start-build-env.sh debian_11\r\n   ...\r\n    _   _           _                    ______\r\n   | | | |         | |                   |  _  \\\r\n   | |_| | __ _  __| | ___   ___  _ __   | | | |_____   __\r\n   |  _  |/ _` |/ _` |/ _ \\ / _ \\| '_ \\  | | | / _ \\ \\ / /\r\n   | | | | (_| | (_| | (_) | (_) | |_) | | |/ /  __/\\ V /\r\n   \\_| |_/\\__,_|\\__,_|\\___/ \\___/| .__/  |___/ \\___| \\_(_)\r\n                                 | |\r\n                                 |_|\r\n   \r\n   This is the standard Hadoop Developer build environment.\r\n   This has all the right tools installed required to build\r\n   Hadoop from source.\r\n   \r\n   chengpan@c85a4426ba52:~/hadoop$\r\n   ```\r\n   \r\n   ```\r\n   $ mvn clean install -DskipTests -Pnative -Pyarn-ui -DskipShade\r\n   ...\r\n   [INFO] Reactor Summary for Apache Hadoop Main 3.5.0-SNAPSHOT:\r\n   [INFO]\r\n   [INFO] Apache Hadoop Main ................................. SUCCESS [  0.662 s]\r\n   [INFO] Apache Hadoop Build Tools .......................... SUCCESS [  1.010 s]\r\n   [INFO] Apache Hadoop Project POM .......................... SUCCESS [  0.592 s]\r\n   [INFO] Apache Hadoop Annotations .......................... SUCCESS [  0.618 s]\r\n   [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  0.069 s]\r\n   [INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.109 s]\r\n   [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  1.754 s]\r\n   [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  0.416 s]\r\n   [INFO] Apache Hadoop Auth ................................. SUCCESS [  2.437 s]\r\n   [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  0.581 s]\r\n   [INFO] Apache Hadoop Common ............................... SUCCESS [ 22.868 s]\r\n   [INFO] Apache Hadoop NFS .................................. SUCCESS [  1.273 s]\r\n   [INFO] Apache Hadoop KMS .................................. SUCCESS [  1.393 s]\r\n   [INFO] Apache Hadoop Registry ............................. SUCCESS [  1.533 s]\r\n   [INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.040 s]\r\n   [INFO] Apache Hadoop HDFS Client .......................... SUCCESS [ 11.693 s]\r\n   [INFO] Apache Hadoop HDFS ................................. SUCCESS [ 22.103 s]\r\n   [INFO] Apache Hadoop HDFS Native Client ................... SUCCESS [02:13 min]\r\n   [INFO] Apache Hadoop HttpFS ............................... SUCCESS [  2.320 s]\r\n   [INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  1.312 s]\r\n   [INFO] Apache Hadoop YARN ................................. SUCCESS [  0.042 s]\r\n   [INFO] Apache Hadoop YARN API ............................. SUCCESS [  7.877 s]\r\n   [INFO] Apache Hadoop YARN Common .......................... SUCCESS [  6.436 s]\r\n   [INFO] Apache Hadoop YARN Server .......................... SUCCESS [  0.038 s]\r\n   [INFO] Apache Hadoop YARN Server Common ................... SUCCESS [  5.022 s]\r\n   [INFO] Apache Hadoop YARN ApplicationHistoryService ....... SUCCESS [  2.078 s]\r\n   [INFO] Apache Hadoop YARN Timeline Service ................ SUCCESS [  1.541 s]\r\n   [INFO] Apache Hadoop YARN Web Proxy ....................... SUCCESS [  1.331 s]\r\n   [INFO] Apache Hadoop YARN ResourceManager ................. SUCCESS [ 13.285 s]\r\n   [INFO] Apache Hadoop YARN NodeManager ..................... SUCCESS [ 24.058 s]\r\n   [INFO] Apache Hadoop YARN Server Tests .................... SUCCESS [  1.467 s]\r\n   [INFO] Apache Hadoop YARN Client .......................... SUCCESS [  2.682 s]\r\n   [INFO] Apache Hadoop MapReduce Client ..................... SUCCESS [  0.682 s]\r\n   [INFO] Apache Hadoop MapReduce Core ....................... SUCCESS [  5.570 s]\r\n   [INFO] Apache Hadoop MapReduce Common ..................... SUCCESS [  2.672 s]\r\n   [INFO] Apache Hadoop MapReduce Shuffle .................... SUCCESS [  1.767 s]\r\n   [INFO] Apache Hadoop MapReduce App ........................ SUCCESS [  3.697 s]\r\n   [INFO] Apache Hadoop MapReduce HistoryServer .............. SUCCESS [  2.380 s]\r\n   [INFO] Apache Hadoop MapReduce JobClient .................. SUCCESS [  4.119 s]\r\n   [INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [  2.581 s]\r\n   [INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  1.206 s]\r\n   [INFO] Apache Hadoop Federation Balance ................... SUCCESS [  1.630 s]\r\n   [INFO] Apache Hadoop HDFS-RBF ............................. SUCCESS [ 15.224 s]\r\n   [INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.035 s]\r\n   [INFO] Apache Hadoop YARN SharedCacheManager .............. SUCCESS [  1.195 s]\r\n   [INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SUCCESS [  1.104 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Backend ... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Common .... SUCCESS [  1.532 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Client .... SUCCESS [  4.403 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Servers ... SUCCESS [  0.039 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Server 2.5  SUCCESS [  1.835 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase tests ..... SUCCESS [  1.916 s]\r\n   [INFO] Apache Hadoop YARN Router .......................... SUCCESS [  2.665 s]\r\n   [INFO] Apache Hadoop YARN TimelineService DocumentStore ... SUCCESS [  1.284 s]\r\n   [INFO] Apache Hadoop YARN GlobalPolicyGenerator ........... SUCCESS [  1.474 s]\r\n   [INFO] Apache Hadoop YARN Applications .................... SUCCESS [  0.033 s]\r\n   [INFO] Apache Hadoop YARN DistributedShell ................ SUCCESS [  1.413 s]\r\n   [INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SUCCESS [  0.925 s]\r\n   [INFO] Apache Hadoop YARN Services ........................ SUCCESS [  0.033 s]\r\n   [INFO] Apache Hadoop YARN Services Core ................... SUCCESS [  3.138 s]\r\n   [INFO] Apache Hadoop YARN Services API .................... SUCCESS [  1.914 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog ............. SUCCESS [  0.038 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog Webapp ...... SUCCESS [ 11.624 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog Docker Image  SUCCESS [  0.048 s]\r\n   [INFO] Apache Hadoop YARN Application MaWo ................ SUCCESS [  0.035 s]\r\n   [INFO] Apache Hadoop YARN Application MaWo Core ........... SUCCESS [  0.991 s]\r\n   [INFO] Apache Hadoop YARN Site ............................ SUCCESS [  0.075 s]\r\n   [INFO] Apache Hadoop YARN Registry ........................ SUCCESS [  0.458 s]\r\n   [INFO] Apache Hadoop YARN UI .............................. SUCCESS [02:07 min]\r\n   [INFO] Apache Hadoop YARN CSI ............................. SUCCESS [  3.403 s]\r\n   [INFO] Apache Hadoop YARN Project ......................... SUCCESS [  1.283 s]\r\n   [INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SUCCESS [  1.046 s]\r\n   [INFO] Apache Hadoop MapReduce NativeTask ................. SUCCESS [ 18.413 s]\r\n   [INFO] Apache Hadoop MapReduce Uploader ................... SUCCESS [  1.040 s]\r\n   [INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  1.912 s]\r\n   [INFO] Apache Hadoop MapReduce ............................ SUCCESS [  1.140 s]\r\n   [INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [  2.034 s]\r\n   [INFO] Apache Hadoop Client Aggregator .................... SUCCESS [  0.728 s]\r\n   [INFO] Apache Hadoop Dynamometer Workload Simulator ....... SUCCESS [  1.363 s]\r\n   [INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SUCCESS [  1.878 s]\r\n   [INFO] Apache Hadoop Dynamometer Block Listing Generator .. SUCCESS [  1.241 s]\r\n   [INFO] Apache Hadoop Dynamometer Dist ..................... SUCCESS [  1.161 s]\r\n   [INFO] Apache Hadoop Dynamometer .......................... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop Archives ............................. SUCCESS [  1.310 s]\r\n   [INFO] Apache Hadoop Archive Logs ......................... SUCCESS [  1.536 s]\r\n   [INFO] Apache Hadoop Rumen ................................ SUCCESS [  1.914 s]\r\n   [INFO] Apache Hadoop Gridmix .............................. SUCCESS [  1.920 s]\r\n   [INFO] Apache Hadoop Data Join ............................ SUCCESS [  1.347 s]\r\n   [INFO] Apache Hadoop Extras ............................... SUCCESS [  1.377 s]\r\n   [INFO] Apache Hadoop Pipes ................................ SUCCESS [  2.679 s]\r\n   [INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [  7.690 s]\r\n   [INFO] Apache Hadoop Kafka Library support ................ SUCCESS [  0.901 s]\r\n   [INFO] Apache Hadoop Azure support ........................ SUCCESS [  5.553 s]\r\n   [INFO] Apache Hadoop Aliyun OSS support ................... SUCCESS [  1.183 s]\r\n   [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  1.679 s]\r\n   [INFO] Apache Hadoop Resource Estimator Service ........... SUCCESS [  1.213 s]\r\n   [INFO] Apache Hadoop Azure Data Lake support .............. SUCCESS [  1.156 s]\r\n   [INFO] Apache Hadoop Image Generation Tool ................ SUCCESS [  1.256 s]\r\n   [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [  0.704 s]\r\n   [INFO] Apache Hadoop OpenStack support .................... SUCCESS [  0.047 s]\r\n   [INFO] Apache Hadoop Common Benchmark ..................... SUCCESS [  9.812 s]\r\n   [INFO] Apache Hadoop Compatibility Benchmark .............. SUCCESS [  0.853 s]\r\n   [INFO] Apache Hadoop Tools ................................ SUCCESS [  0.032 s]\r\n   [INFO] Apache Hadoop Client API ........................... SUCCESS [  0.911 s]\r\n   [INFO] Apache Hadoop Client Runtime ....................... SUCCESS [  0.763 s]\r\n   [INFO] Apache Hadoop Client Packaging Invariants .......... SUCCESS [  0.367 s]\r\n   [INFO] Apache Hadoop Client Test Minicluster .............. SUCCESS [  0.756 s]\r\n   [INFO] Apache Hadoop Client Packaging Invariants for Test . SUCCESS [  0.174 s]\r\n   [INFO] Apache Hadoop Client Packaging Integration Tests ... SUCCESS [  0.778 s]\r\n   [INFO] Apache Hadoop Distribution ......................... SUCCESS [  0.493 s]\r\n   [INFO] Apache Hadoop Client Modules ....................... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop Tencent COS Support .................. SUCCESS [ 56.113 s]\r\n   [INFO] Apache Hadoop OBS support .......................... SUCCESS [  1.335 s]\r\n   [INFO] Apache Hadoop Volcano Engine Services support ...... SUCCESS [  2.005 s]\r\n   [INFO] Apache Hadoop Cloud Storage ........................ SUCCESS [  0.444 s]\r\n   [INFO] Apache Hadoop Cloud Storage Project ................ SUCCESS [  0.032 s]\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] BUILD SUCCESS\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] Total time:  10:25 min\r\n   [INFO] Finished at: 2025-08-26T05:03:57Z\r\n   [INFO] ------------------------------------------------------------------------\r\n   ```\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "pan3793 commented on code in PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#discussion_r2299802155\n\n\n##########\nstart-build-env.sh:\n##########\n@@ -93,7 +93,7 @@ RUN userdel -r \\$(getent passwd ${USER_ID} | cut -d: -f1) 2>/dev/null || :\n RUN groupadd --non-unique -g ${GROUP_ID} ${USER_NAME}\n RUN useradd -g ${GROUP_ID} -u ${USER_ID} -k /root -m ${USER_NAME} -d \"${DOCKER_HOME_DIR}\"\n RUN echo \"${USER_NAME} ALL=NOPASSWD: ALL\" > \"/etc/sudoers.d/hadoop-build-${USER_ID}\"\n-ENV HOME \"${DOCKER_HOME_DIR}\"\n+ENV HOME=\"${DOCKER_HOME_DIR}\"\n\nReview Comment:\n   update because\r\n   ```\r\n    1 warning found (use docker --debug to expand):\r\n    - LegacyKeyValueFormat: \"ENV key=value\" should be used instead of legacy \"ENV key value\" format (line 7)\r\n   ```\n\n\n\n", "slfan1989 commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3223070232\n\n   LGTM.\n\n\n", "hadoop-yetus commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3223228361\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m  6s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 22s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |  17m 26s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  94m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  32m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |  11m 20s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  47m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  14m 13s |  |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 214m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7898 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint mvnsite unit jsonlint |\r\n   | uname | Linux 71b5c8e33180 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f40defe72ea1f0f9d9778a4da76097fa2e1fa02b |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/testReport/ |\r\n   | Max. process+thread count | 707 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "pan3793 commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3223320943\n\n   > We need to solve the compilation error issue.\r\n   \r\n   The building error occurs on CentOS 8 stage, looks like I need to fix it first\n\n\n", "hadoop-yetus commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3226674242\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   0m 19s |  |  Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7898/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7898 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3226834289\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/3/console in case of problems.\r\n   \n\n\n", "hadoop-yetus commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3227579915\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 58s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  1s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ HADOOP-19661 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 24s |  |  HADOOP-19661 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 56s |  |  HADOOP-19661 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  hadolint  |   0m  2s |  |  No new issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  20m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  40m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  16m  4s |  |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 246m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7898 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs mvnsite unit jsonlint |\r\n   | uname | Linux 1b828c1875fb 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | HADOOP-19661 / 7f02ba4fa4464804763bb80838673e37361a7f6b |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/3/testReport/ |\r\n   | Max. process+thread count | 588 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "stoty commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3228059389\n\n   I think it would be useful to run at least the native (C code) tests.\r\n   \n\n\n", "hadoop-yetus commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3238250392\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  36m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  1s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  1s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  1s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 34s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  39m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  checkstyle  |   5m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  24m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   8m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  spotbugs  |  39m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  83m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  83m 51s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 43s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  39m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  16m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   6m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  24m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  spotbugs  |  42m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  83m 31s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 438m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/5/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   2m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 861m 21s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.compat.common.TestHdfsCompatShellCommand |\r\n   |   | hadoop.fs.compat.common.TestHdfsCompatDefaultSuites |\r\n   |   | hadoop.hdfs.tools.TestDFSAdmin |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7898 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint mvnsite unit jsonlint compile javac javadoc mvninstall shadedclient spotbugs checkstyle |\r\n   | uname | Linux b15de39f0892 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d7d9893cf80efe73d802c253eb12508143e8a636 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_462-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/5/testReport/ |\r\n   | Max. process+thread count | 2833 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/5/console |\r\n   | versions | git=2.43.7 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on code in PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#discussion_r2311396990\n\n\n##########\nhadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/TestCLI.java:\n##########\n@@ -43,7 +43,6 @@ public void tearDown() throws Exception {\n   @Override\n   protected CommandExecutor.Result execute(CLICommand cmd) throws Exception {\n     return cmd.getExecutor(\"\", conf).executeCommand(cmd.getCmd());\n-\n\nReview Comment:\n   The changes in Debian are as expected, and we can roll back the modifications to this unit as they are unrelated to our main changes.\n\n\n\n", "pan3793 commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3241344735\n\n   @slfan1989, unnecessary changes are reverted, should be good to go\n\n\n", "slfan1989 commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3241445604\n\n   > @slfan1989, unnecessary changes are reverted, should be good to go\r\n   \r\n   @pan3793 Thank you for your contribution, LGTM. I will merge this PR soon.\n\n\n", "hadoop-yetus commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3242127699\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  1s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  1s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  1s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  25m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  | 111m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  | 112m 30s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  21m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  62m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 40s |  |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 56s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 234m 15s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7898 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint mvnsite unit jsonlint |\r\n   | uname | Linux f4938eb59c7e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 131b7dbbd530ccac5ad0d41775484cb7bc2306e2 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/6/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/6/console |\r\n   | versions | git=2.43.7 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898\n\n\n", "slfan1989 commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3242753656\n\n   @pan3793 Thanks for the contribution! Merged into trunk.\n\n\n", "pan3793 opened a new pull request, #7932:\nURL: https://github.com/apache/hadoop/pull/7932\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Fix the issue that `kill` command is not found\r\n   \r\n   ```\r\n   [INFO] -------------------------------------------------------\r\n   [INFO]  T E S T S\r\n   [INFO] -------------------------------------------------------\r\n   [INFO] Running org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree\r\n   [ERROR] Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.950 s <<< FAILURE! ", "hadoop-yetus commented on PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932#issuecomment-3257889051\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   3m  0s |  |  Docker failed to build run-specific yetus/hadoop:tp-500}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7932 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932#issuecomment-3258044364\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   2m 35s |  |  Docker failed to build run-specific yetus/hadoop:tp-8139}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7932 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932#issuecomment-3258108386\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   2m 35s |  |  Docker failed to build run-specific yetus/hadoop:tp-26905}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7932 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/3/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932#issuecomment-3262448562\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   2m 40s |  |  Docker failed to build run-specific yetus/hadoop:tp-22814}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7932 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/4/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932#issuecomment-3283858265\n\n   @pan3793 Thanks for the contribution! LGTM. I plan to merge this PR shortly.\n\n\n", "hadoop-yetus commented on PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932#issuecomment-3284225600\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  30m  3s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 16s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  50m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 45s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  9s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 58s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 125m 49s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7932 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux 43d6fc34e5bc 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / df78520ad4a7647e76e2bafd68ab5ce287573783 |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/console |\r\n   | versions | git=2.43.7 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932#issuecomment-3284551565\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  27m  0s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 26s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  30m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 22s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  31m  5s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  91m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7932 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux 74aa9d50bdcb 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / df78520ad4a7647e76e2bafd68ab5ce287573783 |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/console |\r\n   | versions | git=2.30.2 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932#issuecomment-3284832335\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  84m 46s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7932 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets jsonlint |\r\n   | uname | Linux a725f8615e43 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / df78520ad4a7647e76e2bafd68ab5ce287573783 |\r\n   | Max. process+thread count | 530 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932\n\n\n", "slfan1989 commented on PR #7932:\nURL: https://github.com/apache/hadoop/pull/7932#issuecomment-3289300445\n\n   @pan3793 Thanks for the contribution!\n\n\n"], "labels": ["pull-request-available"], "summary": "Debian 10 EOL, and the apt repo is unavailable\r\n{code:bash}\r\ndocker run --rm -it", "qna": [{"question": "What is the issue title?", "answer": "Upgrade Debian 10 to 11 in build env Dockerfile"}, {"question": "Who reported this issue?", "answer": "Cheng Pan"}]}
{"key": "HADOOP-19658", "project": "HADOOP", "title": "ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side", "status": "Open", "reporter": "Anmol Asrani", "created": "2025-08-21T12:01:50.000+0000", "description": "\u00a0Support create and rename idempotency on FNS Blob from client side", "comments": ["hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3234744350\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 147m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux bf9d5b01e663 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3ef89e1b92e564468a24cb8d200567b30f283b10 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3234745585\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 22s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5ba8302b1e20 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3ef89e1b92e564468a24cb8d200567b30f283b10 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#discussion_r2309688341\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java:\n##########\n@@ -2236,6 +2238,98 @@ public void testFailureInGetPathStatusDuringCreateRecovery() throws Exception {\n     }\n   }\n \n+  /**\n+   * Test to simulate a successful create operation followed by a connection reset\n+   * on the response, triggering a retry.\n+   *\n+   * This test verifies that the create operation is retried in the event of a\n+   * connection reset during the response phase. The test creates a mock\n+   * AzureBlobFileSystem and its associated components to simulate the create\n+   * operation and the connection reset. It then verifies that the create\n+   * operation is retried once before succeeding.\n+   *\n+   * @throws Exception if an error occurs during the test execution.\n+   */\n+  @Test\n+  public void testCreateIdempotencyForNonHnsBlob() throws Exception {\n+    assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse();\n+    // Create a spy of AzureBlobFileSystem\n+    try (AzureBlobFileSystem fs = Mockito.spy(\n+        (AzureBlobFileSystem) FileSystem.newInstance(getRawConfiguration()))) {\n+      assumeHnsDisabled();\n+      // Create a spy of AzureBlobFileSystemStore\n+      AzureBlobFileSystemStore store = Mockito.spy(fs.getAbfsStore());\n+      assumeBlobServiceType();\n\nReview Comment:\n   We can move all assume before any other statement\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -1702,6 +1705,85 @@ public void testRenamePathRetryIdempotency() throws Exception {\n     }\n   }\n \n+  /**\n+   * Test to simulate a successful copy blob operation followed by a connection reset\n+   * on the response, triggering a retry.\n+   *\n+   * This test verifies that the copy blob operation is retried in the event of a\n+   * connection reset during the response phase. The test creates a mock\n+   * AzureBlobFileSystem and its associated components to simulate the copy blob\n+   * operation and the connection reset. It then verifies that the create\n+   * operation is retried once before succeeding.\n+   *\n+   * @throws Exception if an error occurs during the test execution.\n+   */\n+  @Test\n+  public void testRenameIdempotencyForNonHnsBlob() throws Exception {\n+    assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse();\n+    // Create a spy of AzureBlobFileSystem\n+    try (AzureBlobFileSystem fs = Mockito.spy(\n+        (AzureBlobFileSystem) FileSystem.newInstance(getRawConfiguration()))) {\n+      assumeHnsDisabled();\n\nReview Comment:\n   Same here, move all assume to first few lines\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -509,9 +509,30 @@ public AbfsRestOperation createPath(final String path,\n       final TracingContext tracingContext) throws AzureBlobFileSystemException {\n     AbfsRestOperation op;\n     if (isFileCreation) {\n-      // Create a file with the specified parameters\n-      op = createFile(path, overwrite, permissions, isAppendBlob, eTag,\n-          contextEncryptionAdapter, tracingContext);\n+      AbfsRestOperation statusOp = null;\n+      try {\n+        // Check if the file already exists by calling GetPathStatus\n+        statusOp = getPathStatus(path, false, tracingContext, null);\n\nReview Comment:\n   In case of override true, flow might come here with already a Head call done on path. \r\n   Can we avoid this head call in that case?\n\n\n\n", "hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3236588656\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 17s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 53s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 163m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 08a9e7f8f5d9 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 99bc8f51316f94c4c90e429cce43b8e1bfbbf9f7 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/3/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3237131417\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 55s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 53s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 22s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 8b01375fdcf8 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5bbeb2334a61e143c467bda06c0c3aaca8dfcfae |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/4/testReport/ |\r\n   | Max. process+thread count | 534 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#discussion_r2313015686\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -427,6 +427,8 @@ public static String containerProperty(String property, String fsName, String ac\n   public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\";\n   /**Flag to enable/disable sending client transactional ID during create/rename operations: {@value}*/\n   public static final String FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = \"fs.azure.enable.client.transaction.id\";\n+  /**Flag to enable/disable create idempotency during create operation: {@value}*/\n+  public static final String FS_AZURE_ENABLE_CREATE_IDEMPOTENCY = \"fs.azure.enable.create.idempotency\";\n\nReview Comment:\n   This is only for Blob Idempotency, may be we can keep config name accordingly\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -239,5 +239,7 @@ public final class FileSystemConfigurations {\n \n   public static final boolean DEFAULT_FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = true;\n \n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_ENABLE_CREATE_IDEMPOTENCY = true;\n\nReview Comment:\n   Typo, ENABLE added twice\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsIoUtils.java:\n##########\n@@ -54,7 +56,15 @@ public static void dumpHeadersToDebugLog(final String origin,\n         if (key == null) {\n           key = \"HTTP Response\";\n         }\n-        String values = StringUtils.join(\";\", entry.getValue());\n+        List<String> valuesList = entry.getValue();\n\nReview Comment:\n   Why this change?\n\n\n\n", "anmolanmol1234 commented on code in PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#discussion_r2313023120\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsIoUtils.java:\n##########\n@@ -54,7 +56,15 @@ public static void dumpHeadersToDebugLog(final String origin,\n         if (key == null) {\n           key = \"HTTP Response\";\n         }\n-        String values = StringUtils.join(\";\", entry.getValue());\n+        List<String> valuesList = entry.getValue();\n\nReview Comment:\n   Due to null pointer exceptions on enabling AbfsIoUtils logging if value is null.\n\n\n\n", "anmolanmol1234 commented on code in PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#discussion_r2313027848\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -427,6 +427,8 @@ public static String containerProperty(String property, String fsName, String ac\n   public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\";\n   /**Flag to enable/disable sending client transactional ID during create/rename operations: {@value}*/\n   public static final String FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = \"fs.azure.enable.client.transaction.id\";\n+  /**Flag to enable/disable create idempotency during create operation: {@value}*/\n+  public static final String FS_AZURE_ENABLE_CREATE_IDEMPOTENCY = \"fs.azure.enable.create.idempotency\";\n\nReview Comment:\n   taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -239,5 +239,7 @@ public final class FileSystemConfigurations {\n \n   public static final boolean DEFAULT_FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = true;\n \n+  public static final boolean DEFAULT_FS_AZURE_ENABLE_ENABLE_CREATE_IDEMPOTENCY = true;\n\nReview Comment:\n   taken\n\n\n\n", "hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3241561632\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  5s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 54s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 142m 37s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 820b22f42029 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 61f56c9f98a43e05563fbe7b4fb4683a5e9912ea |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/5/testReport/ |\r\n   | Max. process+thread count | 528 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "bhattmanish98 commented on code in PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#discussion_r2313575382\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1043,6 +1047,12 @@ public String getAzureAtomicRenameDirs() {\n   }\n \n   public boolean isConditionalCreateOverwriteEnabled() {\n+    // If either the configured FS service type or the ingress service type is BLOB,\n+    // conditional create-overwrite is not used.\n+    if (getFsConfiguredServiceType() == AbfsServiceType.BLOB\n\nReview Comment:\n   why is this change needed?\n\n\n\n", "hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3241959922\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 56s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 53s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m 30s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ba0b32cbb7fa 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 417c97d09cd9160e443ed9e355437d9d49037297 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/6/testReport/ |\r\n   | Max. process+thread count | 599 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3243070175\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 54s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m  5s |  |  https://github.com/apache/hadoop/pull/7914 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/7/console |\r\n   | versions | git=2.25.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3243278526\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 6 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 37s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 53s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m 58s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d4a1cb1bcff9 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4948a4f00c6d3afad2416ed34d88533c201b4c24 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/8/testReport/ |\r\n   | Max. process+thread count | 525 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anmolanmol1234 commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3244719265\n\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 819, Failures: 0, Errors: 0, Skipped: 167\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   [ERROR] org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemFileStatus.testLastModifiedTime ", "anujmodi2021 merged PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914\n\n\n"], "labels": ["pull-request-available"], "summary": "\u00a0Support create and rename idempotency on FNS Blob from client side", "qna": [{"question": "What is the issue title?", "answer": "ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side"}, {"question": "Who reported this issue?", "answer": "Anmol Asrani"}]}
{"key": "HADOOP-19657", "project": "HADOOP", "title": "Update 3.4.2 docs landing page to highlight changes shipped in the release", "status": "Resolved", "reporter": "Ahmar Suhail", "created": "2025-08-19T13:16:06.000+0000", "description": "The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes.", "comments": ["ahmarsuhail opened a new pull request, #7887:\nURL: https://github.com/apache/hadoop/pull/7887\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Updates landing page with 3.4.2 changes.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "ahmarsuhail commented on PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887#issuecomment-3200748949\n\n   @anujmodi2021 could you please review the ABFS changes, and let me know if there's anything else you want to highlight. \r\n   \r\n    @steveloughran anything else in S3A we want to highlight?\r\n   \r\n   I'll merge this in tomorrow and kick off the new build.\n\n\n", "hadoop-yetus commented on PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887#issuecomment-3201009350\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   7m  4s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ branch-3.4.2 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 22s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  18m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  71m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7887/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7887 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets |\r\n   | uname | Linux 85cc1ac66933 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4.2 / 62e2fc17c26865e8b191c7b34617af0ea7a5fa95 |\r\n   | Max. process+thread count | 717 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7887/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "iwasakims commented on code in PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887#discussion_r2286735439\n\n\n##########\nhadoop-project/src/site/markdown/index.md.vm:\n##########\n@@ -23,75 +23,33 @@ Overview of Changes\n Users are encouraged to read the full set of release notes.\n This page provides an overview of the major changes.\n \n-Bulk Delete API\n-----------------------------------------\n-\n-[HADOOP-18679](https://issues.apache.org/jira/browse/HADOOP-18679) Bulk Delete API.\n-\n-This release provides an API to perform bulk delete of files/objects\n-in an object store or filesystem.\n-\n New binary distribution\n -----------------------\n \n-[HADOOP-19083](https://issues.apache.org/jira/browse/HADOOP-19083) provide hadoop binary tarball without aws v2 sdk\n-\n-Hadoop has added a new variant of the binary distribution tarball, labeled with \"lean\" in the file\n-name. This tarball excludes the full AWS SDK v2 bundle, resulting in approximately 50% reduction in\n-file size.\n+As of v3.4.2, Hadoop will only be distributed with a lean tarball, which excludes the full AWS SDK v2 bundle to reduce\n+overall file size. This release has been tested with AWS SDK v2 2.29.52, which can be downloaded from Maven\n+[here](https://mvnrepository.com/artifact/software.amazon.awssdk/aws-sdk-java/2.29.52).\n \n S3A improvements\n ----------------\n \n **Improvement**\n \n-[HADOOP-18886](https://issues.apache.org/jira/browse/HADOOP-18886) S3A: AWS SDK V2 Migration: stabilization and S3Express\n-\n-This release completes stabilization efforts on the AWS SDK v2 migration and support of Amazon S3\n-Express One Zone storage. S3 Select is no longer supported.\n-\n-[HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) S3A: Add option fs.s3a.classloader.isolation (#6301)\n-\n-This introduces configuration property `fs.s3a.classloader.isolation`, which defaults to `true`.\n-Set to `false` to disable S3A classloader isolation, which can be useful for installing custom\n-credential providers in user-provided jars.\n-\n-[HADOOP-19047](https://issues.apache.org/jira/browse/HADOOP-19047) Support InMemory Tracking Of S3A Magic Commits\n-\n-The S3A magic committer now supports configuration property\n-`fs.s3a.committer.magic.track.commits.in.memory.enabled`. Set this to `true` to track commits in\n-memory instead of on the file system, which reduces the number of remote calls.\n+[HADOOP-19363](https://issues.apache.org/jira/browse/HADOOP-19363) S3A: Support analytics-accelerator-s3 input streams\n+for parquet read performance.\n \n-[HADOOP-19161](https://issues.apache.org/jira/browse/HADOOP-19161) S3A: option \u201cfs.s3a.performance.flags\u201d to take list of performance flags\n-\n-S3A now supports configuration property `fs.s3a.performance.flag` for controlling activation of\n-multiple performance optimizations. Refer to the S3A performance documentation for details.\n+HADOOP-19256](https://issues.apache.org/jira/browse/HADOOP-19256) S3A: Adds support for S3 Conditional Writes.\n\nReview Comment:\n   formatting error due to missing leading `[`. @ahmarsuhail \n\n\n\n", "anujmodi2021 commented on code in PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887#discussion_r2286959916\n\n\n##########\nhadoop-project/src/site/markdown/index.md.vm:\n##########\n@@ -23,75 +23,33 @@ Overview of Changes\n Users are encouraged to read the full set of release notes.\n This page provides an overview of the major changes.\n \n-Bulk Delete API\n-----------------------------------------\n-\n-[HADOOP-18679](https://issues.apache.org/jira/browse/HADOOP-18679) Bulk Delete API.\n-\n-This release provides an API to perform bulk delete of files/objects\n-in an object store or filesystem.\n-\n New binary distribution\n -----------------------\n \n-[HADOOP-19083](https://issues.apache.org/jira/browse/HADOOP-19083) provide hadoop binary tarball without aws v2 sdk\n-\n-Hadoop has added a new variant of the binary distribution tarball, labeled with \"lean\" in the file\n-name. This tarball excludes the full AWS SDK v2 bundle, resulting in approximately 50% reduction in\n-file size.\n+As of v3.4.2, Hadoop will only be distributed with a lean tarball, which excludes the full AWS SDK v2 bundle to reduce\n+overall file size. This release has been tested with AWS SDK v2 2.29.52, which can be downloaded from Maven\n+[here](https://mvnrepository.com/artifact/software.amazon.awssdk/aws-sdk-java/2.29.52).\n \n S3A improvements\n ----------------\n \n **Improvement**\n \n-[HADOOP-18886](https://issues.apache.org/jira/browse/HADOOP-18886) S3A: AWS SDK V2 Migration: stabilization and S3Express\n-\n-This release completes stabilization efforts on the AWS SDK v2 migration and support of Amazon S3\n-Express One Zone storage. S3 Select is no longer supported.\n-\n-[HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) S3A: Add option fs.s3a.classloader.isolation (#6301)\n-\n-This introduces configuration property `fs.s3a.classloader.isolation`, which defaults to `true`.\n-Set to `false` to disable S3A classloader isolation, which can be useful for installing custom\n-credential providers in user-provided jars.\n-\n-[HADOOP-19047](https://issues.apache.org/jira/browse/HADOOP-19047) Support InMemory Tracking Of S3A Magic Commits\n-\n-The S3A magic committer now supports configuration property\n-`fs.s3a.committer.magic.track.commits.in.memory.enabled`. Set this to `true` to track commits in\n-memory instead of on the file system, which reduces the number of remote calls.\n+[HADOOP-19363](https://issues.apache.org/jira/browse/HADOOP-19363) S3A: Support analytics-accelerator-s3 input streams\n+for parquet read performance.\n \n-[HADOOP-19161](https://issues.apache.org/jira/browse/HADOOP-19161) S3A: option \u201cfs.s3a.performance.flags\u201d to take list of performance flags\n-\n-S3A now supports configuration property `fs.s3a.performance.flag` for controlling activation of\n-multiple performance optimizations. Refer to the S3A performance documentation for details.\n+HADOOP-19256](https://issues.apache.org/jira/browse/HADOOP-19256) S3A: Adds support for S3 Conditional Writes.\n \n ABFS improvements\n -----------------\n \n **Improvement**\n \n-[HADOOP-18516](https://issues.apache.org/jira/browse/HADOOP-18516) [ABFS]: Support fixed SAS token config in addition to Custom SASTokenProvider Implementation\n-\n-ABFS now supports authentication via a fixed Shared Access Signature token. Refer to ABFS\n-documentation of configuration property `fs.azure.sas.fixed.token` for details.\n-\n-[HADOOP-19089](https://issues.apache.org/jira/browse/HADOOP-19089) [ABFS] Reverting Back Support of setXAttr() and getXAttr() on root path\n-\n-[HADOOP-18869](https://issues.apache.org/jira/browse/HADOOP-18869) previously implemented support for xattrs on the root path in the 3.4.0 release. Support for this has been removed in 3.4.1 to prevent the need for calling container APIs.\n-\n-[HADOOP-19178](https://issues.apache.org/jira/browse/HADOOP-19178) WASB Driver Deprecation and eventual removal\n-\n-This release announces deprecation of the WASB file system in favor of ABFS. Refer to ABFS\n-documentation for additional guidance.\n-\n-**Bug**\n-\n-[HADOOP-18542](https://issues.apache.org/jira/browse/HADOOP-18542) Azure Token provider requires tenant and client IDs despite being optional\n+[HADOOP-19226](https://issues.apache.org/jira/browse/HADOOP-19226) ABFS: [FnsOverBlob] Implementing Azure Rest APIs on\n+Blob Endpoint for AbfsBlobClient.\n \n-It is no longer necessary to specify a tenant and client ID in configuration for MSI authentication\n-when running in an Azure instance.\n+[HADOOP-19474](https://issues.apache.org/jira/browse/HADOOP-19474)  ABFS: [FnsOverBlob] Listing Optimizations to avoid\n\nReview Comment:\n   https://issues.apache.org/jira/browse/HADOOP-19543 also can be added.\n\n\n\n##########\nhadoop-project/src/site/markdown/index.md.vm:\n##########\n@@ -23,75 +23,33 @@ Overview of Changes\n Users are encouraged to read the full set of release notes.\n This page provides an overview of the major changes.\n \n-Bulk Delete API\n-----------------------------------------\n-\n-[HADOOP-18679](https://issues.apache.org/jira/browse/HADOOP-18679) Bulk Delete API.\n-\n-This release provides an API to perform bulk delete of files/objects\n-in an object store or filesystem.\n-\n New binary distribution\n -----------------------\n \n-[HADOOP-19083](https://issues.apache.org/jira/browse/HADOOP-19083) provide hadoop binary tarball without aws v2 sdk\n-\n-Hadoop has added a new variant of the binary distribution tarball, labeled with \"lean\" in the file\n-name. This tarball excludes the full AWS SDK v2 bundle, resulting in approximately 50% reduction in\n-file size.\n+As of v3.4.2, Hadoop will only be distributed with a lean tarball, which excludes the full AWS SDK v2 bundle to reduce\n+overall file size. This release has been tested with AWS SDK v2 2.29.52, which can be downloaded from Maven\n+[here](https://mvnrepository.com/artifact/software.amazon.awssdk/aws-sdk-java/2.29.52).\n \n S3A improvements\n ----------------\n \n **Improvement**\n \n-[HADOOP-18886](https://issues.apache.org/jira/browse/HADOOP-18886) S3A: AWS SDK V2 Migration: stabilization and S3Express\n-\n-This release completes stabilization efforts on the AWS SDK v2 migration and support of Amazon S3\n-Express One Zone storage. S3 Select is no longer supported.\n-\n-[HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) S3A: Add option fs.s3a.classloader.isolation (#6301)\n-\n-This introduces configuration property `fs.s3a.classloader.isolation`, which defaults to `true`.\n-Set to `false` to disable S3A classloader isolation, which can be useful for installing custom\n-credential providers in user-provided jars.\n-\n-[HADOOP-19047](https://issues.apache.org/jira/browse/HADOOP-19047) Support InMemory Tracking Of S3A Magic Commits\n-\n-The S3A magic committer now supports configuration property\n-`fs.s3a.committer.magic.track.commits.in.memory.enabled`. Set this to `true` to track commits in\n-memory instead of on the file system, which reduces the number of remote calls.\n+[HADOOP-19363](https://issues.apache.org/jira/browse/HADOOP-19363) S3A: Support analytics-accelerator-s3 input streams\n+for parquet read performance.\n \n-[HADOOP-19161](https://issues.apache.org/jira/browse/HADOOP-19161) S3A: option \u201cfs.s3a.performance.flags\u201d to take list of performance flags\n-\n-S3A now supports configuration property `fs.s3a.performance.flag` for controlling activation of\n-multiple performance optimizations. Refer to the S3A performance documentation for details.\n+HADOOP-19256](https://issues.apache.org/jira/browse/HADOOP-19256) S3A: Adds support for S3 Conditional Writes.\n \n ABFS improvements\n -----------------\n \n **Improvement**\n \n-[HADOOP-18516](https://issues.apache.org/jira/browse/HADOOP-18516) [ABFS]: Support fixed SAS token config in addition to Custom SASTokenProvider Implementation\n-\n-ABFS now supports authentication via a fixed Shared Access Signature token. Refer to ABFS\n-documentation of configuration property `fs.azure.sas.fixed.token` for details.\n-\n-[HADOOP-19089](https://issues.apache.org/jira/browse/HADOOP-19089) [ABFS] Reverting Back Support of setXAttr() and getXAttr() on root path\n-\n-[HADOOP-18869](https://issues.apache.org/jira/browse/HADOOP-18869) previously implemented support for xattrs on the root path in the 3.4.0 release. Support for this has been removed in 3.4.1 to prevent the need for calling container APIs.\n-\n-[HADOOP-19178](https://issues.apache.org/jira/browse/HADOOP-19178) WASB Driver Deprecation and eventual removal\n-\n-This release announces deprecation of the WASB file system in favor of ABFS. Refer to ABFS\n-documentation for additional guidance.\n-\n-**Bug**\n-\n-[HADOOP-18542](https://issues.apache.org/jira/browse/HADOOP-18542) Azure Token provider requires tenant and client IDs despite being optional\n+[HADOOP-19226](https://issues.apache.org/jira/browse/HADOOP-19226) ABFS: [FnsOverBlob] Implementing Azure Rest APIs on\n\nReview Comment:\n   For FNSOverBlob support,may be we can use parent JIRA here as it will track all the related work items.\n\n\n\n", "John6665 commented on PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887#issuecomment-3204892829\n\n   Good job \n\n\n", "ahmarsuhail merged PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887\n\n\n", "hadoop-yetus commented on PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887#issuecomment-3205356855\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 38s |  |  https://github.com/apache/hadoop/pull/7887 does not apply to branch-3.4.2. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7887 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7887/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "The landing page for the 3", "qna": [{"question": "What is the issue title?", "answer": "Update 3.4.2 docs landing page to highlight changes shipped in the release"}, {"question": "Who reported this issue?", "answer": "Ahmar Suhail"}]}
{"key": "HADOOP-19656", "project": "HADOOP", "title": "Fix hadoop-client-minicluster", "status": "Resolved", "reporter": "Cheng Pan", "created": "2025-08-19T12:53:49.000+0000", "description": null, "comments": ["can you please fill the description of the ticket, it isn't conclusive from the ticket, like what is broken which needs to be fixed", "I should close this ticket, the issue was fixed by HADOOP-19652"], "labels": [], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "Fix hadoop-client-minicluster"}, {"question": "Who reported this issue?", "answer": "Cheng Pan"}]}
{"key": "HADOOP-19655", "project": "HADOOP", "title": "Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation", "status": "Open", "reporter": "Ptroc", "created": "2025-08-19T02:52:30.000+0000", "description": "This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c.\r\nKey changes:\r\n * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials.\r\n * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-based software implementation.\r\n * Maintains compatibility with platforms lacking Zbc support.\r\n\r\nThis optimization improves CRC performance on RISC-V CPUs with Zbc extension.", "comments": ["PeterPtroc opened a new pull request, #7896:\nURL: https://github.com/apache/hadoop/pull/7896\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   - Implements a CLMUL-based 16B fold + Barrett reduction algorithm, adapted from riscv-crc32-clmul.\r\n   - True interleaved (round-robin) multi-block pipeline (1\u20133 blocks) to increase ILP.\r\n   - Small buffers and tails fall back to the existing table-based software path.\r\n   - Runtime gating:\r\n     - Double-checked detection: \u201czbc\u201d in /proc/cpuinfo AND a SIGILL-safe CLMUL probe.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - Build (native profile):\r\n     - mvn -pl hadoop-common-project/hadoop-common -am -Pnative -DskipTests clean install\r\n   - Run benchmark:\r\n     - mvn -Pnative -DskipTests -Dexec.classpathScope=test -Dexec.mainClass=org.apache.hadoop.util.Crc32PerformanceTest \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7896:\nURL: https://github.com/apache/hadoop/pull/7896#issuecomment-3222853817\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  27m 44s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  7s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 45s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 105m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 47s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  24m 53s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 47s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 223m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7896 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 4727479d09e6 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 58baa6eb96c75dd288767f650918073c5c094a6f |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/testReport/ |\r\n   | Max. process+thread count | 3133 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7896:\nURL: https://github.com/apache/hadoop/pull/7896#issuecomment-3222859121\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  29m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 41s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 45s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 107m 55s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  16m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  16m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  16m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 43s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  24m 53s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 54s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 227m 14s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7896 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 262a84b27cca 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 58baa6eb96c75dd288767f650918073c5c094a6f |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/testReport/ |\r\n   | Max. process+thread count | 3133 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-", "qna": [{"question": "What is the issue title?", "answer": "Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation"}, {"question": "Who reported this issue?", "answer": "Ptroc"}]}
{"key": "HADOOP-19654", "project": "HADOOP", "title": "Upgrade AWS SDK to 2.35.4", "status": "Resolved", "reporter": "Steve Loughran", "created": "2025-08-18T16:47:04.000+0000", "description": "Upgrade to a recent version of 2.33.x or later while off the critical path of things.\r\n\r\n\r\nHADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.", "comments": ["steveloughran opened a new pull request, #7882:\nURL: https://github.com/apache/hadoop/pull/7882\n\n   ### How was this patch tested?\r\n   \r\n   Testing in progress; still trying to get the ITests working.\r\n   \r\n   JUnit5 update complicates things here, as it highlights that minicluster tests aren't working.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "pan3793 commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201364651\n\n   > JUnit5 update complicates things here, as it highlights that minicluster tests aren't working.\r\n   \r\n   I found `hadoop-client-runtime` and `hadoop-client-minicluster` broken during integration with Spark, HADOOP-19652 plus YARN-11824 recovers that, is it the same issue?\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201641390\n\n   @pan3793 maybe. \r\n   \r\n   what is unrelated is out the box the SDK doesn't do bulk delete with third party stores which support it (Dell ECS).\r\n   ```\r\n   org.apache.hadoop.fs.s3a.AWSBadRequestException: bulkDelete on job-00-fork-0001/test/org.apache.hadoop.fs.contract.s3a.ITestS3AContractBulkDelete: software.amazon.awssdk.services.s3.model.InvalidRequestException: Missing required header for this request: Content-MD5 (Service: S3, Status Code: 400, Request ID: 0c07c87d:196d43d824a:d5329:91d, Extended Request ID: 85e1d41b57b608d4e58222b552dea52902e93b05a12f63f54730ae77769df8d1) (SDK Attempt Count: 1):InvalidRequest: Missing required header for this request: Content-MD5 (Service: S3, Status Code: 400, Request ID: 0c07c87d:196d43d824a:d5329:91d, Extended Request ID: 85e1d41b57b608d4e58222b552dea52902e93b05a12f63f54730ae77769df8d1) (SDK Attempt Count: 1)\r\n   --\r\n   \r\n   ```\r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201646178\n\n   @pan3793 no, it's lifecycle related. Test needs to set up that minicluster before the test cases. and that's somehow not happening\n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3209266234\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 49s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  34m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 32s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m 18s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 49s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  65m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   1m 15s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  9s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 51s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  18m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 48s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  66m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 368m 52s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 735m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux cb65e960fd1f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0d3f20b487ebe8cca5f4b91a3197d7e6cc639901 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/testReport/ |\r\n   | Max. process+thread count | 3658 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3221833528\n\n   regressions\r\n   \r\n   ## everywhere\r\n   \r\n   No logging. Instead we get\r\n   \r\n   ```\r\n   SLF4J: Failed to load class \"org.slf4j.impl.StaticMDCBinder\".\r\n   SLF4J: Defaulting to no-operation MDCAdapter implementation.\r\n   SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.\r\n   ```\r\n   \r\n   `ITestS3AContractAnalyticsStreamVectoredRead` failures -stream closed.\r\n   \r\n   more on this once I've looked at it. If it is an SDK issue, major regression, though it may be something needing changes in the aal libary\r\n   \r\n   ## s3 express\r\n   ```\r\n   [ERROR]   ITestTreewalkProblems.testDistCp:319->lambda$testDistCp$3:320 [Exit code of distcp -useiterator -update -delete -direct s3a://stevel--usw2-az1--x-s3/job-00-fork-0005/test/testDistCp/src s3a://stevel--usw2-az1--x-s3/job-00-fork-0005/test/testDistCp/dest]    \r\n   ```\r\n   assumption: now that the store has lifecycle rules, you don't get prefix listings when there's an in-progress upload. \r\n   \r\n   Fix: change test but also path capability warning of inconsistency. this is good.\r\n   \r\n   Operation costs/auditing count an extra HTTP request, so cost tests fail. I suspect it is always calling CreateSession, but without logging can't be sure\r\n   \r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3222650336\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 57s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 39s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 50s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  65m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   1m  3s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 22s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 52s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 52s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   4m 11s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47)  |\r\n   | +1 :green_heart: |  mvnsite  |  18m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 53s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  65m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 371m  3s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 17s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 733m 32s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.TestRollingUpgrade |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux 880f3cb624ae 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5b9a7e32525c27e876698f49e88ab520eae2d8c4 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/testReport/ |\r\n   | Max. process+thread count | 3821 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3296808329\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 31s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  24m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   9m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 50s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 11s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 40s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  23m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  3s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 24s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 24s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   1m 54s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47)  |\r\n   | +1 :green_heart: |  mvnsite  |  11m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 26s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  7s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 15s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 678m 20s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  8s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 913m 40s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux 113d355d9ed2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / cc31e5be98b54ee418f5ddad4696de2d40e099a0 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/testReport/ |\r\n   | Max. process+thread count | 4200 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3298415046\n\n   Thanks @steveloughran, PR looks good overall. \r\n   \r\n   Are then failures in `ITestS3AContractAnalyticsStreamVectoredRead` intermittent? I've not been able to reproduce, am running the test on this SDK upgrade branch. \n\n\n", "ahmarsuhail commented on code in PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#discussion_r2352378026\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:\n##########\n@@ -390,7 +416,7 @@ public void testIfNoneMatchOverwriteWithEmptyFile() throws Throwable {\n \n     // close the stream, should throw RemoteFileChangedException\n     RemoteFileChangedException exception = intercept(RemoteFileChangedException.class, stream::close);\n-    assertS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception);\n+    verifyS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception);\n\nReview Comment:\n   do you know what the difference is with the other tests here? \n   \n   As in, why with S3 express is it ok to assert that we'll get a 412, whereas the others tests will throw a 200?\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java:\n##########\n@@ -203,7 +206,7 @@ protected Configuration createValidRoleConf() throws JsonProcessingException {\n     conf.set(ASSUMED_ROLE_SESSION_DURATION, \"45m\");\n     // disable create session so there's no need to\n     // add a role policy for it.\n-    disableCreateSession(conf);\n+    //disableCreateSession(conf);\n\nReview Comment:\n   nit: can just cut this instead of commenting it out, since we're skipping these tests if S3 Express is enabled\n\n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3301096683\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 32s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  23m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 32s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 30s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  14m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 33s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  5s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 15s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 36s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  23m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 15s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   1m 58s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47)  |\r\n   | +1 :green_heart: |  mvnsite  |  12m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 27s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  4s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 15s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 678m 56s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 11s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 905m  0s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux 3b890eb50412 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3351e41830fbc9230ffe18bd88bfc0e2a60b20bd |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/testReport/ |\r\n   | Max. process+thread count | 4379 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3304013014\n\n   I've attached a log of a test run against an s3 express bucket where the test `ITestAWSStatisticCollection.testSDKMetricsCostOfGetFileStatusOnFile()` is failing because the AWS SDK stats report 2 http requests for the probe. I'd thought it was create-session related but it isn't: it looks like somehow the stream is broken. This happens reliably on every test runs.\r\n   \r\n   \r\n   The relevant stuff is at line 564 where a HEAD request fails because the stream is broken\r\n   \"end of stream\".\r\n   \r\n   ```\r\n   2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"HEAD /test/testSDKMetricsCostOfGetFileStatusOnFile HTTP/1.1[\\r][\\n]\"\r\n   2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Host: stevel--usw2-az1--x-s3.s3express-usw2-az1.us-west-2.amazonaws.com[\\r][\\n]\"\r\n   2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"amz-sdk-invocation-id: 1804bbcd-04de-cba8-8055-6a09917ca20d[\\r][\\n]\"\r\n   2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"amz-sdk-request: attempt=1; max=3[\\r][\\n]\"\r\n   2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Authorization: AWS4-HMAC-SHA256 Credential=AKIA/20250917/us-west-2/s3express/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;host;referer;x-amz-content-sha256;x-amz-date, Signature=228a46bb1d008468d38afd0da0ed7b4c354ab12631a63bf4283cb23dc02527a3[\\r][\\n]\"\r\n   2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Referer: https://audit.example.org/hadoop/1/op_get_file_status/cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008/?op=op_get_file_status&p1=test/testSDKMetricsCostOfGetFileStatusOnFile&pr=stevel&ps=282e3c5d-c1bd-4859-94b9-82e77ff225d1&id=cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008&t0=1&fs=cf739331-1f2e-42dd-a5d9-f564d6023a23&t1=1&ts=1758131029311[\\r][\\n]\"\r\n   2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"User-Agent: Hadoop 3.5.0-SNAPSHOT aws-sdk-java/2.33.8 md/io#sync md/http#Apache ua/2.1 api/S3#2.33.x os/Mac_OS_X#15.6.1 lang/java#17.0.8 md/OpenJDK_64-Bit_Server_VM#17.0.8+7-LTS md/vendor#Amazon.com_Inc. md/en_GB m/F,G hll/cross-region[\\r][\\n]\"\r\n   2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"x-amz-content-sha256: UNSIGNED-PAYLOAD[\\r][\\n]\"\r\n   2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"X-Amz-Date: 20250917T174349Z[\\r][\\n]\"\r\n   2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Connection: Keep-Alive[\\r][\\n]\"\r\n   2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"[\\r][\\n]\"\r\n   2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(87)) - http-outgoing-1 << \"end of stream\"\r\n   2025-09-17 18:43:49,314 [setup] DEBUG awssdk.request (LoggerAdapter.java:debug(125)) - Retryable error detected. Will retry in 51ms. Request attempt number 1\r\n   software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n   \tat software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:130)\r\n   \tat software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:47)\r\n   \r\n   ```\r\n   \r\n   The second request always works.\r\n   ```\r\n   2025-09-17 18:43:49,672 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"HEAD /test/testSDKMetricsCostOfGetFileStatusOnFile HTTP/1.1[\\r][\\n]\"\r\n   2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Host: stevel--usw2-az1--x-s3.s3express-usw2-az1.us-west-2.amazonaws.com[\\r][\\n]\"\r\n   2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"amz-sdk-invocation-id: 1804bbcd-04de-cba8-8055-6a09917ca20d[\\r][\\n]\"\r\n   2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"amz-sdk-request: attempt=2; max=3[\\r][\\n]\"\r\n   2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Authorization: AWS4-HMAC-SHA256 Credential=AKIA/20250917/us-west-2/s3express/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;host;referer;x-amz-content-sha256;x-amz-date, Signature=920d981fad319228c969f5df7f5c1a3c7e4d3c0e2f45ff53bba73e6cf47c5871[\\r][\\n]\"\r\n   2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Referer: https://audit.example.org/hadoop/1/op_get_file_status/cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008/?op=op_get_file_status&p1=test/testSDKMetricsCostOfGetFileStatusOnFile&pr=stevel&ps=282e3c5d-c1bd-4859-94b9-82e77ff225d1&id=cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008&t0=1&fs=cf739331-1f2e-42dd-a5d9-f564d6023a23&t1=1&ts=1758131029311[\\r][\\n]\"\r\n   2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"User-Agent: Hadoop 3.5.0-SNAPSHOT aws-sdk-java/2.33.8 md/io#sync md/http#Apache ua/2.1 api/S3#2.33.x os/Mac_OS_X#15.6.1 lang/java#17.0.8 md/OpenJDK_64-Bit_Server_VM#17.0.8+7-LTS md/vendor#Amazon.com_Inc. md/en_GB m/F,G hll/cross-region[\\r][\\n]\"\r\n   2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"x-amz-content-sha256: UNSIGNED-PAYLOAD[\\r][\\n]\"\r\n   2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"X-Amz-Date: 20250917T174349Z[\\r][\\n]\"\r\n   2025-09-17 18:43:49,674 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Connection: Keep-Alive[\\r][\\n]\"\r\n   2025-09-17 18:43:49,674 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"HTTP/1.1 200 OK[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"server: AmazonS3[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-request-id: 01869434dd00019958c6871b05090b3f875a3c90[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-id-2: 9GqfbNyMyUs6[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"etag: \"6036aaaf62444466bf0a21cc7518f738\"[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"accept-ranges: bytes[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"last-modified: Wed, 17 Sep 2025 17:43:49 GMT[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-storage-class: EXPRESS_ONEZONE[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"content-type: application/octet-stream[\\r][\\n]\"\r\n   2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-server-side-encryption: AES256[\\r][\\n]\"\r\n   2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"content-length: 0[\\r][\\n]\"\r\n   2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-expiration: NotImplemented[\\r][\\n]\"\r\n   2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"date: Wed, 17 Sep 2025 17:43:48 GMT[\\r][\\n]\"\r\n   2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"[\\r][\\n]\"\r\n   2025-09-17 18:43:49,860 [setup] DEBUG awssdk.request (LoggerAdapter.java:debug(105)) - Received successful response: 200, Request ID: \r\n   ```\r\n   \r\n   Either the request is being rejected (why?) or the connection has gone stale. But why should it happen at exactly the same place on every single test run?\r\n   \r\n   \r\n   [org.apache.hadoop.fs.s3a.statistics.ITestAWSStatisticCollection-output.txt](https://github.com/user-attachments/files/22391405/org.apache.hadoop.fs.s3a.statistics.ITestAWSStatisticCollection-output.txt)\r\n   \n\n\n", "steveloughran commented on code in PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#discussion_r2356314622\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:\n##########\n@@ -390,7 +416,7 @@ public void testIfNoneMatchOverwriteWithEmptyFile() throws Throwable {\n \n     // close the stream, should throw RemoteFileChangedException\n     RemoteFileChangedException exception = intercept(RemoteFileChangedException.class, stream::close);\n-    assertS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception);\n+    verifyS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception);\n\nReview Comment:\n   Hey, it's your server code. Go see.\n\n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3305933937\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  12m 12s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  40m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 48s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m  0s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  21m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 58s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  66m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   1m  3s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  40m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 50s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 50s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  1s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   4m 10s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 7 new + 42 unchanged - 5 fixed = 49 total (was 47)  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 38s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 50s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  66m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 450m 14s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 832m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux 40fa101aa5ab 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 661dc6e3caa66f1218db70d8e6959c2ee3cb0a87 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/testReport/ |\r\n   | Max. process+thread count | 3559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3306500643\n\n   @steveloughran discovered completely by accident, but it's something to do with the checksumming code. \r\n   \r\n   If you comment out these lines: \r\n   \r\n   ```\r\n      //  builder.addPlugin(LegacyMd5Plugin.create());\r\n   \r\n       // do not do request checksums as this causes third-party store problems.\r\n     //  builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED);\r\n   \r\n       // response checksum validation. Slow, even with CRC32 checksums.\r\n   //    if (parameters.isChecksumValidationEnabled()) {\r\n   //      builder.responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED);\r\n   //    }\r\n   \r\n   ```\r\n   \r\n   the test will pass. Could be something to do with s3Express not supporting md5, will look into it.\n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3306741287\n\n   Specifically, it's this line: `builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED);` that causes this. \r\n   \r\n   Comment that out, or change it to `builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_SUPPORTED)`, it passes. \r\n   \r\n   My guess is it's something to do with S3 express not supporting MD5, but for operations where `RequestChecksumCalculation.WHEN_REQUIRED` is true, SDK calculates the m5 and then S3 express rejects it. \r\n   \r\n   Have asked the SDK team. \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3307416441\n\n   ok, so maybe for s3express stores we don't do legacy MD5 plugin stuff all is good?\r\n   \r\n   1. Does imply the far end is breaking the connection when it is unhappy -at least our unit tests found this stuff before the cost of every HEAD doubles.\r\n   2. maybe we should make the choice of checksums an enum with md5 the default, so it is something that can be turned off/changed in future.\r\n   \r\n   While on the topic of S3 Express, is it now the case that because there's lifecycle rules for cleanup, LIST calls don't return prefixes of paths with incomplete uploads? If so I will need to change production code and the test -with a separate JIRA for that for completeness\n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3311549488\n\n   > for s3express stores we don't do legacy MD5 plugin stuff all is good\r\n   \r\n   @steveloughran confirming with the SDK team, since the MD5 plugin is supposed to restore previous behaviour, the server rejecting the first request seems wrong. let's see what they have to say. \r\n   \r\n   >  LIST calls don't return prefixes of paths with incomplete uploads\r\n   \r\n   Will check with S3 express team on this\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3312197290\n\n   thanks. I don't see it on tests against s3 with the 2.29.52 release, so something is changing with the requests made with new SDK + MD5 stuff.\r\n   \r\n   \n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3318915081\n\n   @steveloughran not able to narrow this error down just yet, it looks like it's a combination of S3A's configuration of the S3 client + these new Md5 changes. \r\n   \r\n   ```\r\n     @Test\r\n     public void testHead() throws Throwable {\r\n      // S3Client s3Client = getFileSystem().getS3AInternals().getAmazonS3Client(\"test instance\");\r\n   \r\n       S3Client s3Client = S3Client.builder().region(Region.US_EAST_1)\r\n               .addPlugin(LegacyMd5Plugin.create())\r\n               .requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)\r\n               .responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED)\r\n               .overrideConfiguration(o -> o.retryStrategy(b -> b.maxAttempts(1)))\r\n               .build();\r\n   \r\n   \r\n   \r\n       s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\")\r\n               .key(\"<>\").build());\r\n     }\r\n   ```\r\n   \r\n   I see the failure when the S3A client, and don't see it when I use a newly created client. So it's not just because of `requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)`\r\n   \r\n   Looking into it some more. \r\n   \r\n    S3 express team said there have been no changes in LIST behaviour. \n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3320014995\n\n   able to reproduce the issue outside of S3A. Basically did what would happen when you run a test in S3A:\r\n   \r\n   * a probe for the `test/` directory, and then create the `test/` directory, and then do the `headObject()` call. \r\n   \r\n   The head fails, but if you comment out `requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)` it works again. \r\n   \r\n   no idea what's going on. but have shared this local reproduction with SDK team. And rules out that it's something in the S3A code. \r\n   \r\n   \r\n   \r\n   ```\r\n   public class TestClass {\r\n   \r\n       S3Client s3Client;\r\n   \r\n       public TestClass() {\r\n           this.s3Client = S3Client.builder().region(Region.US_EAST_1)\r\n                   .addPlugin(LegacyMd5Plugin.create())\r\n                   .requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)\r\n                   .responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED)\r\n                   .overrideConfiguration(o -> o.retryStrategy(b -> b.maxAttempts(1)))\r\n                   .build();\r\n       }\r\n   \r\n   \r\n       public void testS3Express(String bucket, String key) {\r\n           s3Client.listObjectsV2(ListObjectsV2Request.builder()\r\n                   .bucket(\"<>\")\r\n                   .maxKeys(2)\r\n                   .prefix(\"test/\")\r\n                   .build());\r\n   \r\n   \r\n           try {\r\n               s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\")\r\n                       .key(\"test\")\r\n                       .build());\r\n           } catch (Exception e) {\r\n               System.out.println(\"Exception thrown: \" + e.getMessage());\r\n           }\r\n   \r\n           s3Client.putObject(PutObjectRequest\r\n                   .builder()\r\n                   .bucket(\"<>\")\r\n                   .key(\"test/\").build(), RequestBody.empty());\r\n   \r\n           s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\")\r\n                   .key(\"<>\")\r\n                   .build());\r\n       }\r\n   \r\n   ```\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3325180154\n\n   well, nice and simple code snippet for the regression testing. Shows the value in having sdk metrics tied up...this is the only case which failed because it's the one asserting at the SDK level values.\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3353451859\n\n   @ahmarsuhail is there a public sdk issue for this for me to link/track?\n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3355348352\n\n   Just created https://github.com/aws/aws-sdk-java-v2/issues/6459\n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3370618249\n\n   FYI @steveloughran , SDK team was able to root cause the issue, details here: https://github.com/aws/aws-sdk-java-v2/issues/6459#issuecomment-3362570846\r\n   \r\n   Since it's a bit of an edge case, and the SDK retry means we recover from it anyway, you think we can go ahead with the upgrade or should we wait for the fix?\n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3405709735\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  11m 32s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  27m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 43s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 48s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 13s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |  10m 26s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 10s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |  31m  2s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/branch-spotbugs-root-warnings.html) |  root in trunk has 12 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  57m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 50s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  6s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 39s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 39s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   3m 13s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 7 new + 42 unchanged - 5 fixed = 49 total (was 47)  |\r\n   | -1 :x: |  mvnsite  |   7m 15s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   9m  2s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 1 new + 43015 unchanged - 0 fixed = 43016 total (was 43015)  |\r\n   | +1 :green_heart: |  javadoc  |   8m 45s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  57m  1s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 793m 21s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 44s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1110m 44s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux 503d715744fe 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e319765ca591fc2a0968f3b2e900586bb46ce7c1 |\r\n   | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/testReport/ |\r\n   | Max. process+thread count | 3551 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3408678775\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 15 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m  9s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  16m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 41s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   6m  5s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   5m  5s |  |  trunk passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 15s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |   0m 28s | [/branch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in trunk failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 16s | [/branch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/branch-spotbugs-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 30s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  15m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   8m 25s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   1m 32s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 11 new + 47 unchanged - 6 fixed = 58 total (was 53)  |\r\n   | -1 :x: |  mvnsite  |   3m 45s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   4m 54s | [/results-javadoc-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/results-javadoc-javadoc-root.txt) |  root generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015)  |\r\n   | +0 :ok: |  spotbugs  |   0m 12s |  |  hadoop-project has no data from spotbugs  |\r\n   | -1 :x: |  spotbugs  |   0m 26s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 17s | [/patch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-spotbugs-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 44s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 227m 31s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 343m 23s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux f49b0547d834 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c2eb04aa497f8d4648a3b457a90843ce96abe7fe |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/testReport/ |\r\n   | Max. process+thread count | 5153 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/console |\r\n   | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412290514\n\n   @ahmarsuhail \r\n   \r\n   I'm handling the retries now by requiring the md5 plugin to be explicitly requested (i.e. third party stores); also making it easier to switch checksum generation from ALWAYS to WHEN_REQUESTED. So for AWS S3: stricter checksums, no md5. Other stores: configure it as needed. \r\n   \r\n   Still wondering if we should make this more automated, but not in a way which causes problems later.\r\n   \r\n   ---\r\n   \r\n   I am now seeing failings against s3 express\r\n   ```\r\n   org.opentest4j.AssertionFailedError: [Counter named audit_request_execution with expected value 4] \r\n   Expecting:\r\n    <11L>\r\n   to be equal to:\r\n    <4L>\r\n   but was not.\r\n   Expected :4\r\n   Actual   :11\r\n   <Click to see difference>\r\n   \r\n   \r\n   \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n   \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n   \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n   \tat org.apache.hadoop.fs.statistics.IOStatisticAssertions.verifyStatisticValue(IOStatisticAssertions.java:274)\r\n   \tat org.apache.hadoop.fs.statistics.IOStatisticAssertions.verifyStatisticCounterValue(IOStatisticAssertions.java:175)\r\n   \tat org.apache.hadoop.fs.s3a.ITestS3AAnalyticsAcceleratorStreamReading.testMultiRowGroupParquet(ITestS3AAnalyticsAcceleratorStreamReading.java:186)\r\n   \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n   \tat java.util.ArrayList.forEach(ArrayList.java:1259)\r\n   \tat java.util.ArrayList.forEach(ArrayList.java:1259)\r\n   ```\r\n   \r\n   I'm changing this test to measure the # of audited requests before the file opening begins and then assert on the difference between them.\r\n   \r\n   \r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412377933\n\n   Now that 3rd party is good, I'm getting S3 express happy, mainly by test tuning. But many, many errors with vectored reads\r\n   \r\n   ```\r\n   [ERROR] Errors: \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testNormalReadAfterVectoredRead\r\n   [INFO]   Run 1: PASS\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testNormalReadAfterVectoredRead \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAndReadFully\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:220 \u00bb IO test/vectored_file.txt: Stream is closed!\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:220 \u00bb IO test/vectored_file.txt: Stream is closed!\r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadMultipleRanges\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:206 \u00bb Execution java.io.IOException: Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt                                                                                                                                                               \r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:206 \u00bb Execution java.io.IOException: Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt                                                                                                                                                               \r\n   [INFO] \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile\r\n   [ERROR]   Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [ERROR]   Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt\r\n   [INFO] \r\n   [INFO] \r\n   ```\r\n   \r\n   \r\n   ```\r\n   \r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection ", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412492676\n\n   @steveloughran just ran with the old 2.29.x SDK, failures there too. will look into it and fix\n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3415388890\n\n    This is happening because those readVectored() tests create a new `vectored-read.txt` file on the setup() before each test. Since the tests are parameterized, they run twice, once for `direct-buffer` and then for `array-buffer`. \r\n    \r\n   On the first run for `direct-buffer`, a HEAD for the metadata is made and cached, and the data for `vectored-read.txt` is also cached. Then the stream is `closed()` and since the file ends in `.txt`, AAL clears the data cache. (since it's a sequential format, the chances there will be a backward seek and the same data will be accessed are low so it's better to clear the data cache). The metadata cache is not cleared here (it should be, and I will make that fix).\r\n   \r\n   On the second run for `array-buffer`, the `vectored-file` is written again. AAL will get the metadata from the metadata cache, and use that eTag when making the GETS for the block data. Since on S3 express, the eTag is no longer the md5 of the object content, even though the object content is the same, the eTag has changed. And hence the 412s on the GETS. \r\n   \r\n   On consistency with caching in general:\r\n   \r\n   * AAL provides a `metadatastore.ttl` config, set that to 0 and HEAD responses are never cached. This solves the caching issues we had when overwrite files before, as with that `ttl` 0 we will always get the latest version of the file. \r\n   \r\n   * Data blocks will be removed once memory usage is > defined memory threshold (2GB), and clean up happens every 5s by default. The edge case here is that what if data usage is always below 2GB, and data blocks never get evicted? This is why the   `metadatastore.ttl` was introduced. \r\n   \r\n   * Our `BlockKey` which is the key under which file data is stored is a combination of the S3URI + eTag. If the eTag changes, then we'll have a different BlockKey, which means we don't have any data stored for it. For example:\r\n   \r\n   ```\r\n   * Data is written to A.parquet, etag is \"1234\".\r\n   * A.parquet is read fully in to the cache, with key \"A.parquet + 1234\"\r\n   * A.parquet is overwritten, etag is \"6789\". \r\n   * A.parquet is opened for reading again:\r\n   \r\n   If metadata ttl has not yet expired, and  metadata cache has eTag as `1234`, so AAL will return data from the data cache using key \"A.parquet + 1234\". If the requested data is not in the data cache, we'll make a GET with the outdated eTag as `1234` and this will fail with a 412. \r\n   \r\n   If metadata TTL has expired, a new HEAD request is made, and we now have the eTag `6789`, this will now create a new BlockKey \"A.parquet + 6789\", and since there is no data stored here, will make GETS for the data. \r\n   ```\r\n   With this we ensure two things:\r\n   \r\n   1/ Once a stream opened it will always serve bytes from the same object version, or fail. \r\n   \r\n   2/ Data will be stale at maximum metadata.tll milliseconds, with the exception of stream's lifetime. \r\n   \r\n   Basically, if your data changes often, set the metadataTTL to 0, and AAL will always get the latest data. Otherwise we have eventually consistency. \n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3415396379\n\n   The TLDR is: \r\n   \r\n   * I will make a small fix to clear the metadata cache on stream close for sequential formats (which fixes this issue)\r\n   * Setting the `metadata.ttl` also fixes this issue. \r\n   \r\n   I've tested with both, and all `ITestS3AContractAnalyticsStreamVectoredRead` test cases pass.\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3416145759\n\n   good explanation. Though I would have expected a bit broader test coverage of your own stores; something to look for on the next library update.\r\n   \r\n   Can I also get improvements in error translation too -we need the error string including request IDs. Relying on the stack entry below to print it isn't enough, as deep exception nesting (hive, spark) can lose that.\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3416153341\n\n   one more thing here: make sure you can handle `null` as an etag in the cache.\r\n   Not all stores have it, which is why it can be turned off for classic input stream version checking.\r\n   \r\n   You won't be able to detect overwrites, but we can just document having a short TTL here.\n\n\n", "ahmarsuhail commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3422219254\n\n   @steveloughran updated exception handling: https://github.com/awslabs/analytics-accelerator-s3/pull/361, next release will have include the requestIDs in the message, eg:\r\n   \r\n   ```\r\n   java.io.IOException: Server error accessing s3://xxx, request failed with: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 412, Request ID: xxxx, Extended Request ID: xxxx)\r\n   ```\r\n   \r\n   The null as `eTag` will require more work, the only way to do that reliably is to disable the caching fully and provide a pass through stream. Do you know which stores don't support eTags?\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3428848006\n\n   latest iteration works with third party stores without MPU (so no magic or use of memory for upload buffering), or bulk delete.\r\n   \r\n   tested google gcs, only underful buffers which can be ignored.\r\n   ```\r\n   [ERROR] Failures: \r\n   [ERROR]   ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testMultipleUnbuffers:108->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <533>                                                          \r\n   [ERROR]   ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferAfterRead:61->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <533>                                                           \r\n   [ERROR]   ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferBeforeRead:71->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <539>                                                          \r\n   [ERROR]   ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferOnClosedFile:91->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <539>                                                        \r\n   [INFO] \r\n   [ERROR] Tests run: 1253, Failures: 4, Errors: 0, Skipped: 450\r\n   [INFO] \r\n   ```\r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3428880917\n\n   @ahmarsuhail I think Apache Ozone is the one.\r\n   \r\n   I just added an `etag` command to cloudstore to print this stuff out and experimented with various stores: https://github.com/steveloughran/cloudstore/blob/main/src/main/site/etag.md\r\n   \r\n   dell ECS and Google both supply etags. We don't retrieve them for directory markers anyway, which isn't an issue\r\n   \r\n   * I've updated the third-party docs to cover etags in more detail, and say \"switch to classic and disable version checking\"\r\n   * I do think the cache needs to handle null/empty string tags, somehow. Certainly by not caching metadata.\n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3432106892\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m  2s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 38 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 43s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  29m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 53s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 41s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 15s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |  10m 51s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 36s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 31s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |   1m 18s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) |  hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |  36m 27s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/branch-spotbugs-root-warnings.html) |  root in trunk has 9241 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  61m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 48s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  27m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 53s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 53s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 24s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 24s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/blanks-eol.txt) |  The patch has 24 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   3m 15s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 14 new + 79 unchanged - 6 fixed = 93 total (was 85)  |\r\n   | -1 :x: |  mvnsite  |   7m  2s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   9m 37s | [/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 4 new + 46184 unchanged - 0 fixed = 46188 total (was 46184)  |\r\n   | -1 :x: |  javadoc  |   8m 41s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015)  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  62m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 741m 49s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 44s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1085m  5s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   |   | hadoop.yarn.service.TestYarnNativeServices |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux 8fbc6faf5962 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1cd8a2820c4aadeca61f3a7449c7d98fd34bb9d8 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/testReport/ |\r\n   | Max. process+thread count | 3717 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3434634222\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 27s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 39 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 18s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  15m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 18s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 18s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 25s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   6m 10s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 31s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 43s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +0 :ok: |  spotbugs  |   0m 15s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |   0m 41s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) |  hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |  18m 42s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/branch-spotbugs-root-warnings.html) |  root in trunk has 9241 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  32m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  15m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 52s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   7m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 17s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 17s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/blanks-eol.txt) |  The patch has 24 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   1m 33s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 14 new + 79 unchanged - 6 fixed = 93 total (was 85)  |\r\n   | -1 :x: |  mvnsite  |   3m 43s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   5m 23s | [/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 4 new + 46184 unchanged - 0 fixed = 46188 total (was 46184)  |\r\n   | -1 :x: |  javadoc  |   4m 38s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015)  |\r\n   | +0 :ok: |  spotbugs  |   0m 12s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  32m 15s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 594m 47s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 50s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 779m 27s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy |\r\n   |   | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes |\r\n   |   | hadoop.hdfs.server.federation.router.async.TestRouterAsyncRpcClient |\r\n   |   | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux c90f744f2220 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 96c3f38ea5e033636e1acdb8fe2ed4b398bedb08 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/testReport/ |\r\n   | Max. process+thread count | 4624 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3437736945\n\n   fun test run today, against s3 london. Most of the multipart upload/commit tests were failing \"missing part\", from cli or IDE. Testing with S3 express was happy. (`-Dparallel-tests -DtestsThreadCount=8 -Panalytics -Dscale`)\r\n   \r\n   ```\r\n   [ERROR]   ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1)                                                                                                                                                                                   \r\n   [ERROR]   ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit                                                                                                                                                      \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)                                                                                                                     \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                                                                                                  \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src  \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src      \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src            \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src          \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1)                                                                                                           \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                   \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                             \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                 \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                       \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                     \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1)                                                                                                                                                                                       \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src                                                                                                                     \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src     \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src         \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src               \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src             \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1)                                                                                                                   \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src                                                                                                                 \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src     \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src           \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src         \r\n   [ERROR]   ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1)                                                                                        \r\n   [ERROR]   ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                                                             \r\n   [ERROR]   ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src     \r\n   [ERROR]   ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                   \r\n   [INFO] \r\n   [ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13\r\n   [INFO] \r\n   ```\r\n   \r\n   This has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h\r\n   \r\n   When these uploads fail we do leave incomplete uploads in progress:\r\n   ```\r\n   Listing uploads under path \"\"\r\n   job-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98\r\n   job-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I\r\n   job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4\r\n   job-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV\r\n   job-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh\r\n   job-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC\r\n   job-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw--\r\n   job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq\r\n   test/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA--\r\n   test/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA--\r\n   Total 10 uploads found.\r\n   ```\r\n   \r\n   Most interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. \r\n   \r\n   The attempt to complete failed.\r\n   ```\r\n   [ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads:380->createFileWithFlags:190 \u00bb AWSBadRequest Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1)          \r\n   ```\r\n   \r\n   Yet the uploads list afterwards finds it\r\n   ```\r\n   job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4\r\n   ```\r\n   \r\n   I have to conclude that the list of pending uploads was briefly offline/inconsistent.\r\n   \r\n   This is presumably so, so rare that there's almost no point retrying here. With no retries, every active write/job would have failed, even though the system had recovered within a minute.\r\n   \r\n   Maybe we should retry here? I remember a long long time ago the v1 sdk didn't retry on failures of the final POST to commit an upload, and how that sporadically caused problems. Retrying on MPU failures will allow for recovery in the presence of a transient failure here, and the cost of \"deletion of all pending uploads will take longer to fail all active uploads\". \r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3441063874\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 39 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 42s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  15m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 10s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 15s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 36s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   6m 44s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 35s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 51s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +0 :ok: |  spotbugs  |   0m 15s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |   0m 41s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) |  hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |  19m  4s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/branch-spotbugs-root-warnings.html) |  root in trunk has 9241 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 44s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  16m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 49s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m 26s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 41s |  |  root: The patch generated 0 new + 79 unchanged - 6 fixed = 79 total (was 85)  |\r\n   | -1 :x: |  mvnsite  |   4m 11s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 17s |  |  root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 0 new + 46182 unchanged - 2 fixed = 46182 total (was 46184)  |\r\n   | +1 :green_heart: |  javadoc  |   4m 44s |  |  root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 43013 unchanged - 2 fixed = 43013 total (was 43015)  |\r\n   | +0 :ok: |  spotbugs  |   0m 11s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 53s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 591m 32s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 51s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 781m 38s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs |\r\n   | uname | Linux 54d25015775c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fa906dcf97ff8829f50184906bd7433bc2a0a73a |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/testReport/ |\r\n   | Max. process+thread count | 4675 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3444243683\n\n   OK, this is all related to checksums on multipart puts.\r\n   \r\n   If you declare that checksums are always required on requests, you MUST define a checksum algorithm to use for multipart put, otherwise upload completions fail. I have no idea why, will file some SDK bug report to say \"this is wrong\" and simply change our settings to\r\n   - checksums NOT always required\r\n   - MD5 always enabled\r\n   - checksum algorithm is CRC32C (will test with third party store)\r\n   \r\n   checksums in MPUs breaks a couple of the multipart uploader tests; more worried that about a ITestS3AOpenCost test failing with checksum verification being enabled (slow, expensive). I need to make sure that this is not an SDK regression.\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3444263138\n\n   Its a change in the default value: downloads have checksums verified unless you say \"no\". we say no.\n\n\n", "AWS SDK issue #6518 shows how checksum generation on uploaded data (fs.s3a.create.checksum) must be set if request checksum calculation is enabled (fs.s3a.checksum.generation)\r\n\r\nChecksum validation has also been enabled by default; {{ITestS3AOpenCost.testStreamIsNotChecksummed()}} caught that change.\r\n\r\nIt looks like the SDK has really embraced checksums, which first broke compatibility with other stores, but which has also surfaced problems within their own code.\r\n\r\nAll checksum logic will be off by default; MD5 headers will be attached now ", "hadoop-yetus commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3446382053\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 23s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 44 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m  1s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  15m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 12s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 36s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   5m 54s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 18s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 41s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +0 :ok: |  spotbugs  |   0m 16s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |   1m 27s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) |  hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |   0m 39s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) |  hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings.  |\r\n   | -1 :x: |  spotbugs  |  18m 32s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-spotbugs-root-warnings.html) |  root in trunk has 9241 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  32m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 29s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  15m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  6s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 20s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/blanks-eol.txt) |  The patch has 6 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   1m 30s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 6 new + 83 unchanged - 6 fixed = 89 total (was 89)  |\r\n   | -1 :x: |  mvnsite  |   3m 47s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 16s |  |  root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 0 new + 46182 unchanged - 2 fixed = 46182 total (was 46184)  |\r\n   | +1 :green_heart: |  javadoc  |   4m 41s |  |  root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 43013 unchanged - 2 fixed = 43013 total (was 43015)  |\r\n   | +0 :ok: |  spotbugs  |   0m 11s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  32m 34s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 587m  3s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 50s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 770m  3s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.service.TestYarnNativeServices |\r\n   |   | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7882 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint shellcheck shelldocs |\r\n   | uname | Linux cc0336ad4f43 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 149e98291ada965d1dc2b85b4214f235bb4b5c5d |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/testReport/ |\r\n   | Max. process+thread count | 4586 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3486477634\n\n   Seeing \r\n   * test timeouts on junit5 + scale + parallel: https://issues.apache.org/jira/browse/HADOOP-19739\r\n   * some failures launching \r\n   \r\n   ```\r\n   [ERROR] Failures: \r\n   [ERROR] org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob.test_200_execute\r\n   [INFO]   Run 1: PASS\r\n   [ERROR]   Run 2: ITestS3ACommitterMRJob.test_200_execute:342 [Files found in s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned] \r\n   Expecting:                                                                                                                                                                  \r\n    <[\"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00001\"]>                                                                    \r\n   to be equal to:                                                                                                                                                             \r\n    <[\"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00000\",                                                                     \r\n       \"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00001\",                                                                    \r\n       \"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00002\",                                                                    \r\n       \"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00003\",                                                                    \r\n       \"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00004\",                                                                    \r\n       \"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00005\",                                                                    \r\n       \"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00006\",                                                                    \r\n       \"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00007\",                                                                    \r\n       \"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00008\",                                                                    \r\n       \"s3a://stevel-london/job-00-fork-0004/test/ITestS3ACommitterMRJob-execute-partitioned/part-m-00009\"]>                                                                   \r\n   but was not.                                                                                                                                                                \r\n   [INFO]   Run 3: PASS\r\n   ```\r\n   \r\n   This doesn't happen standalone.\r\n   \r\n   In the IDE I get java8 errors, probably need to log out and log in again now I've switched my default jvm to 17. \r\n   \r\n   I'm not worrying about this. \n\n\n", "steveloughran commented on code in PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#discussion_r2490905201\n\n\n##########\nhadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/testing.md:\n##########\n@@ -736,9 +749,23 @@ For stores with stricter semantics, these test cases must be disabled.\n   </property>\n ```\n \n+### Changing expectations on multipart upload retries: `ITestS3AContractMultipartUploader` and `ITestUploadRecovery`\n+\n+If the store reports errors when trying to list/abort completed multipart uploads,\n+expect failures in `ITestUploadRecovery` and `ITestS3AContractMultipartUploader`.\n+The tests can be reconfigured to expect failure by setting the option\n+`fs.s3a.ext.multipart.commit.consumes.upload.id` to true.\n+\n+Note how this can be set as a per-bucket option.\n+\n+```xml\n+  <property>\n+    <name>fs.s3a.ext.multipart.commit.consumes.upload.id</name>\n\nReview Comment:\n   fs.s3a.ext.test indicate this is a test option\n\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3487082391\n\n   test failure with ITestConnectionTimeouts and store set to use analytics stream. Fix: make sure we only use classic stream here.\r\n   ```\r\n   [ERROR] Failures: \r\n   [ERROR]   ITestConnectionTimeouts.testObjectUploadTimeouts:254 Expected a java.lang.Exception to be thrown, but got the result: : \"0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\"                                                                                                               \r\n   ```\r\n   \n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3487531114\n\n   I think I'm done here, @mukund-thakur and @ahmarsuhail .... Testing against corner case deployments are finding corners of test configurations, not actual code failures\r\n   \r\n   I am getting failures of some MR jobs since the JDK/junit updates; with no obvious cause.\r\n   They do work standalone, just not in batch runs. Going to see if this surfaces on\r\n   trunk and if so declare unrelated.\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3487855879\n\n   \n   \n   everything running mr job can't spawn process properly as the launched jvm is always java8. \n   I've mostly fixed this but some of the spawned mr jobs still play up in batch runs but not standalone\n   ```\n   Error: A JNI error has occurred, please check your installation and try again\n   Exception in thread \"main\" java.lang.UnsupportedClassVersionError: org/apache/hadoop/mapreduce/v2/app/MRAppMaster has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 52.0\n   \tat java.lang.ClassLoader.defineClass1(Native Method)\n   ```\n   \n   One regression is a JUnit5 regression; the configurable timeouts of scale tests are no longer being picked up, slow tests are timing out\n   ```\n   [ERROR] org.apache.hadoop.fs.s3a.scale.ITestS3AConcurrentOps.testThreadPoolCoolDown ", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3491921936\n\n   The intermittent test failures happen on trunk when running with java17; it's related to the parallel test runner. I am *not* investigating it here.\n\n\n", "ahmarsuhail commented on code in PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#discussion_r2499741481\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:\n##########\n@@ -390,7 +416,7 @@ public void testIfNoneMatchOverwriteWithEmptyFile() throws Throwable {\n \n     // close the stream, should throw RemoteFileChangedException\n     RemoteFileChangedException exception = intercept(RemoteFileChangedException.class, stream::close);\n-    assertS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception);\n+    verifyS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception);\n\nReview Comment:\n   checked..the answer is that it's MPU that has divergence, put object which these tests do will return 412\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/third_party_stores.md:\n##########\n@@ -414,7 +480,47 @@ Fix: path style access\n   </property>\n ```\n \n-# Connecting to Google Cloud Storage through the S3A connector\n+# Settings for Specific Stores\n+\n+## Dell ECS through the S3A Connector\n+\n+As of November 2025 and the 2.35.4 AWS SDK, the settings needed to interact with Dell ECS\n+at [ECS Test Drive](https://portal.ecstestdrive.com/) were\n+\n+```xml\n+<property>\n+  <name>fs.s3a.region</name>\n+  <value>region</value>\n+  <description>arbitrary name</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.endpoint.region</name>\n+  <value>region</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.path.style.access</name>\n+  <value>true</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.create.conditional.enabled</name>\n+  <value>false</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.request.md5.header</name>\n+  <value>false</value>\n\nReview Comment:\n   confused - I thought this should be true, otherwise the SDK won't generate the MD5's, which was causing the compatibility issues with third party stores?\n\n\n\n", "steveloughran commented on code in PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#discussion_r2500457817\n\n\n##########\nhadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/third_party_stores.md:\n##########\n@@ -414,7 +480,47 @@ Fix: path style access\n   </property>\n ```\n \n-# Connecting to Google Cloud Storage through the S3A connector\n+# Settings for Specific Stores\n+\n+## Dell ECS through the S3A Connector\n+\n+As of November 2025 and the 2.35.4 AWS SDK, the settings needed to interact with Dell ECS\n+at [ECS Test Drive](https://portal.ecstestdrive.com/) were\n+\n+```xml\n+<property>\n+  <name>fs.s3a.region</name>\n+  <value>region</value>\n+  <description>arbitrary name</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.endpoint.region</name>\n+  <value>region</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.path.style.access</name>\n+  <value>true</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.create.conditional.enabled</name>\n+  <value>false</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.request.md5.header</name>\n+  <value>false</value>\n\nReview Comment:\n   yes, you are right. somehow got that wrong in my port. will fix\n\n\n\n", "steveloughran commented on code in PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#discussion_r2500458341\n\n\n##########\nhadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/third_party_stores.md:\n##########\n@@ -414,7 +480,47 @@ Fix: path style access\n   </property>\n ```\n \n-# Connecting to Google Cloud Storage through the S3A connector\n+# Settings for Specific Stores\n+\n+## Dell ECS through the S3A Connector\n+\n+As of November 2025 and the 2.35.4 AWS SDK, the settings needed to interact with Dell ECS\n+at [ECS Test Drive](https://portal.ecstestdrive.com/) were\n+\n+```xml\n+<property>\n+  <name>fs.s3a.region</name>\n\nReview Comment:\n   cut this\n\n\n\n", "steveloughran merged PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882\n\n\n", "mukund-thakur commented on code in PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#discussion_r2500550665\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/DefaultS3ClientFactory.java:\n##########\n@@ -202,11 +205,34 @@ private <BuilderT extends S3BaseClientBuilder<BuilderT, ClientT>, ClientT> Build\n \n     configureEndpointAndRegion(builder, parameters, conf);\n \n+    // add a plugin to add a Content-MD5 header.\n+    // this is required when performing some operations with third party stores\n+    // (for example: bulk delete), and is somewhat harmless when working with AWS S3.\n+    if (parameters.isMd5HeaderEnabled()) {\n+      LOG.debug(\"MD5 header enabled\");\n+      builder.addPlugin(LegacyMd5Plugin.create());\n+    }\n+\n+    //when to calculate request checksums.\n+    final RequestChecksumCalculation checksumCalculation =\n+        parameters.isChecksumCalculationEnabled()\n+            ? RequestChecksumCalculation.WHEN_SUPPORTED\n\nReview Comment:\n   Its confusing. What happens if it is required but not supported. \n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/DefaultS3ClientFactory.java:\n##########\n@@ -202,11 +205,34 @@ private <BuilderT extends S3BaseClientBuilder<BuilderT, ClientT>, ClientT> Build\n \n     configureEndpointAndRegion(builder, parameters, conf);\n \n+    // add a plugin to add a Content-MD5 header.\n+    // this is required when performing some operations with third party stores\n+    // (for example: bulk delete), and is somewhat harmless when working with AWS S3.\n+    if (parameters.isMd5HeaderEnabled()) {\n+      LOG.debug(\"MD5 header enabled\");\n+      builder.addPlugin(LegacyMd5Plugin.create());\n+    }\n+\n+    //when to calculate request checksums.\n+    final RequestChecksumCalculation checksumCalculation =\n+        parameters.isChecksumCalculationEnabled()\n+            ? RequestChecksumCalculation.WHEN_SUPPORTED\n\nReview Comment:\n   I was thinking if we could have some docs around the WHEN_SUPPORTED and WHEN_REQUIRED \n\n\n\n", "mukund-thakur commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3499098574\n\n   `[ERROR]   ITestConnectionTimeouts.testObjectUploadTimeouts:247 [Duration of write] \r\n   Expecting:\r\n    <PT16.579S>\r\n   to be less than:\r\n    <PT10S> `\r\n   \r\n   this can ignored. \r\n   \r\n   `[ERROR]   ITestBucketTool.testRecreateTestBucketNonS3Express:145  Expected to find 'BucketAlreadyOwnedByYouException' but got unexpected exception: org.apache.hadoop.fs.s3a.AWSBadRequestException: create on s3a://mthakur-us-west-1: software.amazon.awssdk.services.s3.model.S3Exception: The us-east-2 location constraint is incompatible for the region specific endpoint this request was sent to. (Service: S3, Status Code: 400, Request ID: 305KXAWSAP63FKGA, Extended Request ID: BmoZx1BgwGVhyQB6vYFjEFHCkXA4UbowUMEMJhPeNs0SU8q2KPtIbaWg5sZ1gg2XB5LVJfBXQHw=) (SDK Attempt Count: 1):IllegalLocationConstraintException: The us-east-2 location constraint is incompatible for the region specific endpoint this request was sent to. (Service: S3, Status Code: 400, Request ID: 305KXAWSAP63FKGA, Extended Request ID: BmoZx1BgwGVhyQB6vYFjEFHCkXA4UbowUMEMJhPeNs0SU8q2KPtIbaWg5sZ1gg2XB5LVJfBXQHw=) (SDK Attempt Count: 1)\r\n   \tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:271)\r\n   \tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)\r\n   \tat org.apache.hadoop.fs.s3a.tools.BucketTool.run(BucketTool.java:267)\r\n   \tat org.apache.hadoop.fs.s3a.tools.BucketTool.exec(BucketTool.java:154)\r\n   \tat org.apache.hadoop.fs.s3a.tools.ITestBucketTool.lambda$testRecreateTestBucketNonS3Express$1(ITestBucketTool.java:146)`\r\n   \r\n   this one I thinking what is going on \n\n\n", "steveloughran opened a new pull request, #8059:\nURL: https://github.com/apache/hadoop/pull/8059\n\n   \r\n   AWS SDK upgraded to 2.35.4.\r\n   \r\n   This SDK has changed checksum/checksum headers handling significantly, causing problems with third party stores, and, in some combinations AWS S3 itself.\r\n   \r\n   The S3A connector has retained old behavior; options to change these settings are now available.\r\n   \r\n   The default settings are chosen for maximum compatiblity and performance.\r\n   \r\n   fs.s3a.request.md5.header:       true\r\n   fs.s3a.checksum.generation:      false\r\n   fs.s3a.create.checksum.algorithm: \"\"\r\n   \r\n   Consult the documentation for more details.\r\n   \r\n   Contributed by Steve Loughran\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   in progress. \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #8059:\nURL: https://github.com/apache/hadoop/pull/8059#issuecomment-3501555132\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   7m 18s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 44 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   2m 45s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  24m 53s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |   9m 27s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 36s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 21s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  15m 24s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m  3s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 40s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 13s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |  16m 38s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8059/1/artifact/out/branch-spotbugs-root-warnings.html) |  root in branch-3.4 has 2 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 35s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  17m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   8m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 53s |  |  root: The patch generated 0 new + 78 unchanged - 6 fixed = 78 total (was 84)  |\r\n   | +1 :green_heart: |  mvnsite  |  11m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   4m 47s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 36s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 11s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 31s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 613m 37s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8059/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 55s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 832m 23s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.security.ssl.TestDelegatingSSLSocketFactory |\r\n   |   | hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2 |\r\n   |   | hadoop.mapred.gridmix.TestGridmixSubmission |\r\n   |   | hadoop.mapred.gridmix.TestLoadJob |\r\n   |   | hadoop.mapred.gridmix.TestSleepJob |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8059/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8059 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint shellcheck shelldocs |\r\n   | uname | Linux 01cb1a8c3251 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / eed647c511a3613dcc2ec11b32b9f7fc4bb2c82c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8059/1/testReport/ |\r\n   | Max. process+thread count | 4098 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8059/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on code in PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#discussion_r2503587272\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/DefaultS3ClientFactory.java:\n##########\n@@ -202,11 +205,34 @@ private <BuilderT extends S3BaseClientBuilder<BuilderT, ClientT>, ClientT> Build\n \n     configureEndpointAndRegion(builder, parameters, conf);\n \n+    // add a plugin to add a Content-MD5 header.\n+    // this is required when performing some operations with third party stores\n+    // (for example: bulk delete), and is somewhat harmless when working with AWS S3.\n+    if (parameters.isMd5HeaderEnabled()) {\n+      LOG.debug(\"MD5 header enabled\");\n+      builder.addPlugin(LegacyMd5Plugin.create());\n+    }\n+\n+    //when to calculate request checksums.\n+    final RequestChecksumCalculation checksumCalculation =\n+        parameters.isChecksumCalculationEnabled()\n+            ? RequestChecksumCalculation.WHEN_SUPPORTED\n\nReview Comment:\n   some operations require checksums (bulk delete?) and everything which implemented them has had to expect checksums. This new generation option, \"when supported\" is what broke things as it really means \"generate checksums on all requests\". There are only two values in the enum, so the sdk always has to choose one.\r\n   \r\n   when_supported\r\n   * doesn't work for most third party stores\r\n   * seems to break MPUs if you don't set a content checksum for put/posted data.\r\n   \r\n   I think having a generation \"true/false\" is simpler for people to understand than the nuances of when_supported vs when_required.\r\n   \n\n\n\n", "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3502632939\n\n   > ITestBucketTool.testRecreateTestBucketNonS3Express\r\n   \r\n   was looking at this in the regions patch as it fails for sdk and ec2 regions.\r\n   \r\n   we are trying to issue a create command and need to know the bucket region for the call. The test will have to explicitly ask for it via a HEAD call. \r\n   we are expecting an error FWIW; we could just look for two different error texts and accept them both \"BucketExists\" and \"IllegalLocationConstraint\". That might be easiest.\r\n   \r\n   (my pr currently skips the test if the region is sdk or ec2, as well as the existing non-aws/non s3-express options).\r\n   \r\n   \n\n\n", "mukund-thakur commented on code in PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#discussion_r2504825661\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/DefaultS3ClientFactory.java:\n##########\n@@ -202,11 +205,34 @@ private <BuilderT extends S3BaseClientBuilder<BuilderT, ClientT>, ClientT> Build\n \n     configureEndpointAndRegion(builder, parameters, conf);\n \n+    // add a plugin to add a Content-MD5 header.\n+    // this is required when performing some operations with third party stores\n+    // (for example: bulk delete), and is somewhat harmless when working with AWS S3.\n+    if (parameters.isMd5HeaderEnabled()) {\n+      LOG.debug(\"MD5 header enabled\");\n+      builder.addPlugin(LegacyMd5Plugin.create());\n+    }\n+\n+    //when to calculate request checksums.\n+    final RequestChecksumCalculation checksumCalculation =\n+        parameters.isChecksumCalculationEnabled()\n+            ? RequestChecksumCalculation.WHEN_SUPPORTED\n\nReview Comment:\n   Yes it should be just true/false. @ahmarsuhail  could you please talk to the SDK team for this. Why they did this way? \n\n\n\n", "mukund-thakur commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3504444626\n\n   reading about the stack trace, the reason for failure is both s3client and the create bucket request should have the same configured region. \r\n   Debugging more I found we should be setting the region here as well https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/ITestBucketTool.java#L115 \r\n   once set it will get propagated as location constraint in the create bucket request. \r\n   \r\n   but even if setting and verifying that propagation is happening correctly, it fails with the same reason. \r\n   \r\n   Yes just accepting both error, the test will be fine and I am wondering what is going on. \n\n\n", "steveloughran merged PR #8059:\nURL: https://github.com/apache/hadoop/pull/8059\n\n\n"], "labels": ["pull-request-available"], "summary": "Upgrade to a recent version of 2", "qna": [{"question": "What is the issue title?", "answer": "Upgrade AWS SDK to 2.35.4"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19653", "project": "HADOOP", "title": "[JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode", "status": "Resolved", "reporter": "Cheng Pan", "created": "2025-08-18T03:11:13.000+0000", "description": null, "comments": ["pan3793 opened a new pull request, #7879:\nURL: https://github.com/apache/hadoop/pull/7879\n\n   \u2026tecode\r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   ```\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "slfan1989 commented on code in PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#discussion_r2281285636\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -146,7 +146,7 @@\n     <netty4.version>4.1.118.Final</netty4.version>\n     <snappy-java.version>1.1.10.4</snappy-java.version>\n     <lz4-java.version>1.7.1</lz4-java.version>\n-    <byte-buddy.version>1.15.11</byte-buddy.version>\n+    <byte-buddy.version>1.17.6</byte-buddy.version>\n\nReview Comment:\n   Should the LICENSE-binary be updated?\n\n\n\n", "pan3793 commented on code in PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#discussion_r2281290385\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -146,7 +146,7 @@\n     <netty4.version>4.1.118.Final</netty4.version>\n     <snappy-java.version>1.1.10.4</snappy-java.version>\n     <lz4-java.version>1.7.1</lz4-java.version>\n-    <byte-buddy.version>1.15.11</byte-buddy.version>\n+    <byte-buddy.version>1.17.6</byte-buddy.version>\n\nReview Comment:\n   they are test-only deps, not present in LICENSE/NOTICE files\n\n\n\n", "slfan1989 commented on code in PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#discussion_r2281293747\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -146,7 +146,7 @@\n     <netty4.version>4.1.118.Final</netty4.version>\n     <snappy-java.version>1.1.10.4</snappy-java.version>\n     <lz4-java.version>1.7.1</lz4-java.version>\n-    <byte-buddy.version>1.15.11</byte-buddy.version>\n+    <byte-buddy.version>1.17.6</byte-buddy.version>\n\nReview Comment:\n   LGTM.\n\n\n\n", "hadoop-yetus commented on PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#issuecomment-3195141474\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  76m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 45s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 19s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 117m  4s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7879/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7879 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 395fb749b4d0 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fee75ef1efffd7f6da1fa67bdefd54b076f51df3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7879/1/testReport/ |\r\n   | Max. process+thread count | 549 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7879/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "stoty commented on PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196190107\n\n   Have you tried building with Java 25 ?\r\n   Last time I updated ByteBuddy, I run into maven-shade-plugin incompatibilities.\n\n\n", "pan3793 commented on PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196229409\n\n   @stoty this PR indeed fixes the Java 25 building\n\n\n", "stoty commented on PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196566191\n\n   Thanks.\r\n   \r\n   I can also successfully build with the patch.\r\n   \r\n   The shade plugin problem doesn't surface because we're actually compiling JDK17 bytecode.\n\n\n", "pan3793 commented on PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196591873\n\n   > The shade plugin problem doesn't surface because we're actually compiling JDK17 bytecode.\r\n   \r\n   @stoty This is actually addressed by upgrading ASM deps used by `maven-shade-plugin`, without this change, you will see error\r\n   \r\n   ```\r\n   [ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.6.0:shade (default) on project hadoop-client-minicluster: Error creating shaded jar: Problem shading JAR /home/chengpan/.m2/repository/net/bytebuddy/byte-buddy/1.17.6/byte-buddy-1.17.6.jar entry META-INF/versions/24/net/bytebuddy/jar/asmjdkbridge/JdkClassWriter$1.class: java.lang.IllegalArgumentException: Unsupported class file major version 68 -> [Help 1]\r\n   ```\n\n\n", "stoty commented on PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196844666\n\n   I've also tried to set the target version to 25, but some other plugin failed, the build didn't even get as far the shade plugin.\r\n   \r\n   But again, that is not a major issue, as we're building for Java 17 compatibility.\r\n   \n\n\n", "slfan1989 merged PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879\n\n\n", "slfan1989 commented on PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#issuecomment-3198832279\n\n   @pan3793 Thanks for the contribution! @stoty Thanks for the review!\n\n\n"], "labels": ["pull-request-available"], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "[JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode"}, {"question": "Who reported this issue?", "answer": "Cheng Pan"}]}
{"key": "HADOOP-19652", "project": "HADOOP", "title": "Fix dependency exclusion list of hadoop-client-runtime.", "status": "Resolved", "reporter": "Cheng Pan", "created": "2025-08-18T02:31:16.000+0000", "description": null, "comments": ["pan3793 opened a new pull request, #7878:\nURL: https://github.com/apache/hadoop/pull/7878\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   When trying to use the Hadoop trunk version client with Spark 4.0.0, `NoClassDefFoundError` was raised.\r\n   \r\n   ```\r\n   Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/shaded/javax/ws/rs/WebApplicationException\r\n   \tat java.base/java.lang.ClassLoader.defineClass1(Native Method)\r\n   \tat java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:962)\r\n   \tat java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:144)\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:776)\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:691)\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:620)\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:578)\r\n   \tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:490)\r\n   \tat org.apache.spark.deploy.yarn.YarnRMClient.getAmIpFilterParams(YarnRMClient.scala:109)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster.addAmIpFilter(ApplicationMaster.scala:698)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster.runExecutorLauncher(ApplicationMaster.scala:555)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:265)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:942)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:941)\r\n   \tat java.base/jdk.internal.vm.ScopedValueContainer.callWithoutScope(ScopedValueContainer.java:162)\r\n   \tat java.base/jdk.internal.vm.ScopedValueContainer.call(ScopedValueContainer.java:147)\r\n   \tat java.base/java.lang.ScopedValue$Carrier.call(ScopedValue.java:419)\r\n   \tat java.base/javax.security.auth.Subject.callAs(Subject.java:331)\r\n   \tat org.apache.hadoop.util.SubjectUtil.callAs(SubjectUtil.java:134)\r\n   \tat org.apache.hadoop.util.SubjectUtil.doAs(SubjectUtil.java:166)\r\n   \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:2039)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:941)\r\n   \tat org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:973)\r\n   \tat org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala)\r\n   Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.shaded.javax.ws.rs.WebApplicationException\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:580)\r\n   \tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:490)\r\n   \t... 24 more\r\n   ```\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Hadoop Client Runtime 3.4.2 RC2\r\n   <img width=\"407\" height=\"459\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9ba6eb0c-2d52-4034-8f97-01c73195d795\" />\r\n   \r\n   Hadoop Client Runtime 3.5.0-SNAPSHOT trunk\r\n   <img width=\"516\" height=\"432\" alt=\"image\" src=\"https://github.com/user-attachments/assets/81115ab8-3f86-4336-9d95-fe61093b09d1\" />\r\n   \r\n   Hadoop Client Runtime 3.5.0-SNAPSHOT HADOOP-19652\r\n   <img width=\"509\" height=\"433\" alt=\"image\" src=\"https://github.com/user-attachments/assets/eb08a2d8-6b52-45f4-b56e-4ea3a92e950a\" />\r\n   \r\n   Tested by submitting a Spark application to a YARN cluster.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "pan3793 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3194939043\n\n   cc @slfan1989 @cnauroth, this is caused by Jersey upgrading.\n\n\n", "pan3793 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3194946944\n\n   AFAIK, there is no solid integration test for the Hadoop Shaded client, I will continue to test it with Spark and fix issues I encounter.\n\n\n", "hadoop-yetus commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3195086656\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  76m 30s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   5m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  35m 43s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 20s |  |  hadoop-client-runtime in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 121m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7878 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 84d5837e24b5 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 16187caf9fa1ca13f0b60d9a5e2dabd40c2aba12 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/1/testReport/ |\r\n   | Max. process+thread count | 648 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-client-modules/hadoop-client-runtime U: hadoop-client-modules/hadoop-client-runtime |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on code in PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#discussion_r2281293084\n\n\n##########\nhadoop-client-modules/hadoop-client-runtime/pom.xml:\n##########\n@@ -176,14 +176,11 @@\n                       <exclude>org.glassfish.jersey.core:*</exclude>\n                       <exclude>org.glassfish.hk2.external:*</exclude>\n                       <exclude>org.glassfish.jaxb:*</exclude>\n-                      <exclude>jakarta.ws.rs:*</exclude>\n                       <exclude>jakarta.annotation:*</exclude>\n                       <exclude>jakarta.validation:*</exclude>\n-                      <exclude>jakarta.servlet:*</exclude>\n-                      <exclude>javax.annotation:*</exclude>\n                       <exclude>org.hamcrest:*</exclude>\n+                      <exclude>org.javassist:*</exclude>\n\nReview Comment:\n   Can this line remain unchanged?\n\n\n\n", "pan3793 commented on code in PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#discussion_r2281304950\n\n\n##########\nhadoop-client-modules/hadoop-client-runtime/pom.xml:\n##########\n@@ -176,14 +176,11 @@\n                       <exclude>org.glassfish.jersey.core:*</exclude>\n                       <exclude>org.glassfish.hk2.external:*</exclude>\n                       <exclude>org.glassfish.jaxb:*</exclude>\n-                      <exclude>jakarta.ws.rs:*</exclude>\n                       <exclude>jakarta.annotation:*</exclude>\n                       <exclude>jakarta.validation:*</exclude>\n-                      <exclude>jakarta.servlet:*</exclude>\n-                      <exclude>javax.annotation:*</exclude>\n                       <exclude>org.hamcrest:*</exclude>\n+                      <exclude>org.javassist:*</exclude>\n\nReview Comment:\n   this is the corrected version of\r\n   ```\r\n   <exclude>javassist:*</exclude>\r\n   ```\n\n\n\n", "pan3793 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3195164574\n\n   Not a real error, just caused by token expiration, push a new empty commit to retrigger CI.\r\n   \r\n   ```\r\n   12:47:42  ============================================================================\r\n   12:47:42  ============================================================================\r\n   12:47:42                           Adding comment to Github\r\n   12:47:42  ============================================================================\r\n   12:47:42  ============================================================================\r\n   12:47:42  \r\n   12:47:42  \r\n   12:47:46  ERROR: Failed to write github status. Token expired or missing repo:status write?\r\n   12:47:46  ERROR: Failed to write github status. Token expired or missing repo:status write?\r\n   ```\r\n   https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/1/console\n\n\n", "hadoop-yetus commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3195503178\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  76m  3s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   5m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  35m 29s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 20s |  |  hadoop-client-runtime in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 49s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 121m 21s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7878 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux f59cfeb748b8 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 830cdae5363f8db2dede0b6097accfce6a51f7b9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/2/testReport/ |\r\n   | Max. process+thread count | 560 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-client-modules/hadoop-client-runtime U: hadoop-client-modules/hadoop-client-runtime |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3196039890\n\n   @pan3793 https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/2/artifact/out/patch-shadedclient.txt\r\n   \r\n   ```\r\n   ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.5.0:enforce (enforce-banned-dependencies) on project hadoop-client-check-test-invariants: \r\n   [ERROR] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message:\r\n   [ERROR] Duplicate classes found:\r\n   [ERROR] \r\n   [ERROR]   Found in:\r\n   [ERROR]     org.apache.hadoop:hadoop-client-minicluster:jar:3.5.0-SNAPSHOT:compile\r\n   [ERROR]     org.apache.hadoop:hadoop-client-runtime:jar:3.5.0-SNAPSHOT:compile\r\n   ......\r\n   [ERROR] After correcting the problems, you can resume the build with the command\r\n   [ERROR]   mvn <args> -rf :hadoop-client-check-test-invariants\r\n   ```\n\n\n", "pan3793 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3196049320\n\n   @slfan1989 I see, will fix soon\n\n\n", "hadoop-yetus commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3197368091\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 54s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 50s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  79m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  22m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 20s |  |  hadoop-client-runtime in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 21s |  |  hadoop-client-minicluster in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7878 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 5bff763e4c87 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0e4ad11e173fee8758c0267f4ee503448bfcdc97 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/3/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster U: hadoop-client-modules |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3198828530\n\n   > @slfan1989 I see, will fix soon\r\n   \r\n   @pan3793 Thank you for your contribution! I don\u2019t see any other issues. Let\u2019s wait for one more day, and if there are no further comments, I\u2019ll merge this PR.\n\n\n", "slfan1989 merged PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878\n\n\n", "slfan1989 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3203410020\n\n   @pan3793 Thanks for the contribution! Merged into trunk.\n\n\n"], "labels": ["pull-request-available"], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "Fix dependency exclusion list of hadoop-client-runtime."}, {"question": "Who reported this issue?", "answer": "Cheng Pan"}]}
{"key": "HADOOP-19651", "project": "HADOOP", "title": "Upgrade libopenssl to 3.5.2-1 needed for rsync", "status": "Resolved", "reporter": "Gautham Banasandra", "created": "2025-08-15T19:19:04.000+0000", "description": "The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n\r\nThe Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows.\r\n\r\n{code}\r\n00:25:33 SUCCESS: Specified value was saved.\r\n00:25:46 Removing intermediate container 5ce7355571a1\r\n00:25:46 ---> a13a4bc69545\r\n00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n00:25:46 ---> Running in d2dafad446f9\r\n00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n00:25:54 \u001b[0m\u001b[91mAt line:1 char:1\r\n00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ...\r\n00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:Htt\r\n{code}\r\n\r\nThus, we need to upgrade to the latest version to address this.", "comments": ["GauthamBanasandra opened a new pull request, #7875:\nURL: https://github.com/apache/hadoop/pull/7875\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   The currently used `libopenssl-3.1.4-1` is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n   \r\n   The Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows.\r\n   \r\n   ```\r\n   00:25:33 SUCCESS: Specified value was saved.\r\n   00:25:46 Removing intermediate container 5ce7355571a1\r\n   00:25:46 ---> a13a4bc69545\r\n   00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n   00:25:46 ---> Running in d2dafad446f9\r\n   00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n   00:25:54 \u001b[0m\u001b[91mAt line:1 char:1\r\n   00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ...\r\n   00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n   00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:Htt\r\n   ```\r\n   \r\n   Thus, we need to upgrade to the latest available version to address this.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Jenkins CI - In progress.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7875:\nURL: https://github.com/apache/hadoop/pull/7875#issuecomment-3192521324\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/console in case of problems.\r\n   \n\n\n", "hadoop-yetus commented on PR #7875:\nURL: https://github.com/apache/hadoop/pull/7875#issuecomment-3192810626\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  25m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 21s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  54m 29s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7875 |\r\n   | Optional Tests | dupname asflicense |\r\n   | uname | Linux 7a9ce5519195 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 988bb50936dcb2b9808ed2ffaa95851a4c228fd6 |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "GauthamBanasandra commented on PR #7875:\nURL: https://github.com/apache/hadoop/pull/7875#issuecomment-3193865491\n\n   Build is going through now - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-win10-x86_64/1030/console.\n\n\n", "GauthamBanasandra merged PR #7875:\nURL: https://github.com/apache/hadoop/pull/7875\n\n\n", "Merged PR to trunk - https://github.com/apache/hadoop/pull/7875."], "labels": ["pull-request-available"], "summary": "The currently used libopenssl-3", "qna": [{"question": "What is the issue title?", "answer": "Upgrade libopenssl to 3.5.2-1 needed for rsync"}, {"question": "Who reported this issue?", "answer": "Gautham Banasandra"}]}
{"key": "HADOOP-19650", "project": "HADOOP", "title": "ABFS: NPE when close() called on uninitialized filesystem", "status": "Open", "reporter": "Steve Loughran", "created": "2025-08-15T17:27:57.000+0000", "description": "code\r\n{code}\r\n  public void testABFSConstructor() throws Throwable {\r\n    new AzureBlobFileSystem().close();\r\n  }\r\n{code}\r\n\r\nstack\r\n{code}\r\n[ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR!\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null\r\n        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.close(AzureBlobFileSystem.java:800)\r\n        at org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor(TestRuntimeValid.java:49)\r\n{code}\r\n\r\n\r\n", "comments": ["Thanks for reporting this.\r\nWill work on the patch on priority.", "hadoop-yetus commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3197386039\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 11s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 141m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7880 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 160d07de834c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fd85bee514d271d663f801bf3498b3145ec08fec |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/testReport/ |\r\n   | Max. process+thread count | 530 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "steveloughran commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2282920640\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -148,7 +150,7 @@ public class AzureBlobFileSystem extends FileSystem\n   private URI uri;\n   private Path workingDir;\n   private AzureBlobFileSystemStore abfsStore;\n-  private boolean isClosed;\n+  private boolean isClosed = true;\n\nReview Comment:\n   so this really means inited and closed. Mention that in javadocs.\n\n\n\n", "hadoop-yetus commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3197699593\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m 15s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 1 unchanged - 0 fixed = 3 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7880 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 741539520d55 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 18fa0f27d00c849b858b19383a3c213a71a9aacb |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/3/testReport/ |\r\n   | Max. process+thread count | 564 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3201478753\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 34s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 1 unchanged - 0 fixed = 3 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 45s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7880 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 4b13a9c47991 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 62141e105067253aaf4a3fc516b5c06f84f8da9f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/4/testReport/ |\r\n   | Max. process+thread count | 558 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3204164409\n\n   ------------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 862, Failures: 0, Errors: 0, Skipped: 209\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 865, Failures: 0, Errors: 0, Skipped: 161\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 704, Failures: 0, Errors: 0, Skipped: 240\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 862, Failures: 0, Errors: 0, Skipped: 220\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 709, Failures: 0, Errors: 0, Skipped: 135\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 242\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 706, Failures: 0, Errors: 0, Skipped: 147\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 703, Failures: 0, Errors: 0, Skipped: 189\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 736, Failures: 0, Errors: 0, Skipped: 216\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 239\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   Time taken: 199 mins 4 secs.\r\n   \n\n\n", "anujmodi2021 commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2286986125\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -148,7 +150,7 @@ public class AzureBlobFileSystem extends FileSystem\n   private URI uri;\n   private Path workingDir;\n   private AzureBlobFileSystemStore abfsStore;\n-  private boolean isClosed;\n+  private boolean isClosed = true;\n\nReview Comment:\n   Yes, added the javadcoc.\n\n\n\n", "bhattmanish98 commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2287000680\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce\n \n   @Override\n   public synchronized void close() throws IOException {\n-    if (isClosed) {\n+    if (isClosed()) {\n\nReview Comment:\n   Should we throw an exception in case someone is trying to close non initialized or already closed file system?\n\n\n\n", "bhattmanish98 commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2287009604\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestFileSystemInitialization.java:\n##########\n@@ -81,21 +89,123 @@ public void testFileSystemCapabilities() throws Throwable {\n \n     final Path p = new Path(\"}\");\n     // etags always present\n-    Assertions.assertThat(fs.hasPathCapability(p, ETAGS_AVAILABLE))\n+    assertThat(fs.hasPathCapability(p, ETAGS_AVAILABLE))\n         .describedAs(\"path capability %s in %s\", ETAGS_AVAILABLE, fs)\n         .isTrue();\n     // readahead always correct\n-    Assertions.assertThat(fs.hasPathCapability(p, CAPABILITY_SAFE_READAHEAD))\n+    assertThat(fs.hasPathCapability(p, CAPABILITY_SAFE_READAHEAD))\n         .describedAs(\"path capability %s in %s\", CAPABILITY_SAFE_READAHEAD, fs)\n         .isTrue();\n \n     // etags-over-rename and ACLs are either both true or both false.\n     final boolean etagsAcrossRename = fs.hasPathCapability(p, ETAGS_PRESERVED_IN_RENAME);\n     final boolean acls = fs.hasPathCapability(p, FS_ACLS);\n-    Assertions.assertThat(etagsAcrossRename)\n+    assertThat(etagsAcrossRename)\n         .describedAs(\"capabilities %s=%s and %s=%s in %s\",\n             ETAGS_PRESERVED_IN_RENAME, etagsAcrossRename,\n             FS_ACLS, acls, fs)\n         .isEqualTo(acls);\n   }\n+\n+  /**\n+   * Test that the AzureBlobFileSystem close without init works\n+   * @throws Exception if an error occurs\n+   */\n+  @Test\n+  public void testABFSCloseWithoutInit() throws Exception {\n+    AzureBlobFileSystem fs = new AzureBlobFileSystem();\n+    assertThat(fs.isClosed()).isTrue();\n+    fs.close();\n+    fs.initialize(this.getFileSystem().getUri(), getRawConfiguration());\n+    assertThat(fs.isClosed()).isFalse();\n+    fs.close();\n+    assertThat(fs.isClosed()).isTrue();\n+  }\n+\n+  /**\n+   * Test that the AzureBlobFileSystem throws an exception\n+   * when trying to perform an operation without initialization.\n+   * @throws Exception if an error occurs\n+   */\n+  @Test\n+  public void testABFSUninitializedFileSystem() throws Exception {\n+    AzureBlobFileSystem fs = new AzureBlobFileSystem();\n+    assertThat(fs.isClosed()).isTrue();\n+    Path testPath = new Path(\"testPath\");\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        fs::toString);\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.open(testPath, ONE_MB));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.create(testPath, FsPermission.getDefault(), false, ONE_MB,\n+            fs.getDefaultReplication(testPath), ONE_MB, null));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.createNonRecursive(testPath, FsPermission.getDefault(), false, ONE_MB,\n+            fs.getDefaultReplication(testPath), ONE_MB, null));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.append(testPath, ONE_MB, null));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.rename(testPath, testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.delete(testPath, true));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.listStatus(testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.mkdirs(testPath, FsPermission.getDefault()));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.getFileStatus(testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.breakLease(testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.makeQualified(testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.setOwner(testPath, \"\", \"\"));\n\nReview Comment:\n   EMPTY_STRING can be used here\n\n\n\n", "hadoop-yetus commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3204282967\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  30m 22s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 31s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 30s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -0 :warning: |  checkstyle  |   0m 29s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt) |  The patch fails to run checkstyle in hadoop-azure  |\r\n   | -1 :x: |  mvnsite  |   0m 30s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  spotbugs  |   0m 30s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   3m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) |  The patch fails to run checkstyle in hadoop-azure  |\r\n   | -1 :x: |  mvnsite  |   0m 42s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  spotbugs  |   0m 24s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   5m  1s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 24s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 24s |  |  ASF License check generated no output?  |\r\n   |  |   |  45m 10s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7880 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 4535df39a5aa 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 579d0674d77eb78689c28a0df8aa4b49aac222c8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/testReport/ |\r\n   | Max. process+thread count | 65 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3204495167\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 54s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 48s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  9s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 24s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7880 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 84fe5e904fca 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 77e3546d9fc5ff4a180abe2300956dbbd003e079 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/5/testReport/ |\r\n   | Max. process+thread count | 576 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2287442313\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce\n \n   @Override\n   public synchronized void close() throws IOException {\n-    if (isClosed) {\n+    if (isClosed()) {\n\nReview Comment:\n   fs.close() is supposed to be idempotent IMO\n\n\n\n", "anujmodi2021 commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2287443157\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestFileSystemInitialization.java:\n##########\n@@ -81,21 +89,123 @@ public void testFileSystemCapabilities() throws Throwable {\n \n     final Path p = new Path(\"}\");\n     // etags always present\n-    Assertions.assertThat(fs.hasPathCapability(p, ETAGS_AVAILABLE))\n+    assertThat(fs.hasPathCapability(p, ETAGS_AVAILABLE))\n         .describedAs(\"path capability %s in %s\", ETAGS_AVAILABLE, fs)\n         .isTrue();\n     // readahead always correct\n-    Assertions.assertThat(fs.hasPathCapability(p, CAPABILITY_SAFE_READAHEAD))\n+    assertThat(fs.hasPathCapability(p, CAPABILITY_SAFE_READAHEAD))\n         .describedAs(\"path capability %s in %s\", CAPABILITY_SAFE_READAHEAD, fs)\n         .isTrue();\n \n     // etags-over-rename and ACLs are either both true or both false.\n     final boolean etagsAcrossRename = fs.hasPathCapability(p, ETAGS_PRESERVED_IN_RENAME);\n     final boolean acls = fs.hasPathCapability(p, FS_ACLS);\n-    Assertions.assertThat(etagsAcrossRename)\n+    assertThat(etagsAcrossRename)\n         .describedAs(\"capabilities %s=%s and %s=%s in %s\",\n             ETAGS_PRESERVED_IN_RENAME, etagsAcrossRename,\n             FS_ACLS, acls, fs)\n         .isEqualTo(acls);\n   }\n+\n+  /**\n+   * Test that the AzureBlobFileSystem close without init works\n+   * @throws Exception if an error occurs\n+   */\n+  @Test\n+  public void testABFSCloseWithoutInit() throws Exception {\n+    AzureBlobFileSystem fs = new AzureBlobFileSystem();\n+    assertThat(fs.isClosed()).isTrue();\n+    fs.close();\n+    fs.initialize(this.getFileSystem().getUri(), getRawConfiguration());\n+    assertThat(fs.isClosed()).isFalse();\n+    fs.close();\n+    assertThat(fs.isClosed()).isTrue();\n+  }\n+\n+  /**\n+   * Test that the AzureBlobFileSystem throws an exception\n+   * when trying to perform an operation without initialization.\n+   * @throws Exception if an error occurs\n+   */\n+  @Test\n+  public void testABFSUninitializedFileSystem() throws Exception {\n+    AzureBlobFileSystem fs = new AzureBlobFileSystem();\n+    assertThat(fs.isClosed()).isTrue();\n+    Path testPath = new Path(\"testPath\");\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        fs::toString);\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.open(testPath, ONE_MB));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.create(testPath, FsPermission.getDefault(), false, ONE_MB,\n+            fs.getDefaultReplication(testPath), ONE_MB, null));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.createNonRecursive(testPath, FsPermission.getDefault(), false, ONE_MB,\n+            fs.getDefaultReplication(testPath), ONE_MB, null));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.append(testPath, ONE_MB, null));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.rename(testPath, testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.delete(testPath, true));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.listStatus(testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.mkdirs(testPath, FsPermission.getDefault()));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.getFileStatus(testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.breakLease(testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.makeQualified(testPath));\n+\n+    intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE,\n+        () -> fs.setOwner(testPath, \"\", \"\"));\n\nReview Comment:\n   Will take up this.\n\n\n\n", "bhattmanish98 commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2287814760\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce\n \n   @Override\n   public synchronized void close() throws IOException {\n-    if (isClosed) {\n+    if (isClosed()) {\n\nReview Comment:\n   There is chance that more two threads can attempt to close store and client twice. \n\n\n\n", "bhattmanish98 commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2287814760\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce\n \n   @Override\n   public synchronized void close() throws IOException {\n-    if (isClosed) {\n+    if (isClosed()) {\n\nReview Comment:\n   There is chance that more two threads can attempt to close store and client which can cause similar issue what we got now. \n\n\n\n", "steveloughran commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2288107530\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce\n \n   @Override\n   public synchronized void close() throws IOException {\n-    if (isClosed) {\n+    if (isClosed()) {\n\nReview Comment:\n   @bhattmanish98 I don't see concurrency issues because close() is synchronized, and once closed it can't be closed again. Of course, once one thread has closed it, nobody else can use the instance. fix: don't do that. \n\n\n\n", "anujmodi2021 commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3206294039\n\n   > +1\r\n   > \r\n   > ready to merge.\r\n   \r\n   Sure, just did a commit to rerun yetus, will merge as soon as green.\n\n\n", "bhattmanish98 commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2288137399\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce\n \n   @Override\n   public synchronized void close() throws IOException {\n-    if (isClosed) {\n+    if (isClosed()) {\n\nReview Comment:\n   @steveloughran Make sense, I overlooked synchronized in the method definition. PR Looks good. Approved the changes.\n\n\n\n", "hadoop-yetus commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3206925500\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 145m 34s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7880 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a9e8d7a37f31 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5a51d6a271f282878677fdab891549d77dac2e6d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/7/testReport/ |\r\n   | Max. process+thread count | 536 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 merged PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880\n\n\n", "anujmodi2021 opened a new pull request, #7888:\nURL: https://github.com/apache/hadoop/pull/7888\n\n   ### Description of PR\r\n   PR on trunk: https://github.com/apache/hadoop/pull/7880\r\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19650\n\n\n", "anujmodi2021 opened a new pull request, #7889:\nURL: https://github.com/apache/hadoop/pull/7889\n\n   ### Description of PR\r\n   PR on trunk: https://github.com/apache/hadoop/pull/7880\r\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19650\n\n\n", "anujmodi2021 commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3207359625\n\n   Will do backport for 3.4 and 3.4.2\n\n\n", "hadoop-yetus commented on PR #7889:\nURL: https://github.com/apache/hadoop/pull/7889#issuecomment-3207450187\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   6m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 26s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  hadoop-tools/hadoop-azure: The patch generated 0 new + 1 unchanged - 1 fixed = 1 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  3s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  81m 35s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7889/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7889 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6822d049569c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / cc5116580b5f7b89333cf0c606432b50e29a543b |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7889/1/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7889/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7888:\nURL: https://github.com/apache/hadoop/pull/7888#issuecomment-3207584935\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  11m 45s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.4.2 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  37m 15s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  branch-3.4.2 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  branch-3.4.2 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  branch-3.4.2 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  branch-3.4.2 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  hadoop-tools/hadoop-azure: The patch generated 0 new + 1 unchanged - 1 fixed = 1 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  32m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 26s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 128m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7888/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7888 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5e18fda5c880 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4.2 / eb7913ebe57cf38ee41eba62cdba53d5d9a25bff |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7888/1/testReport/ |\r\n   | Max. process+thread count | 699 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7888/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on PR #7888:\nURL: https://github.com/apache/hadoop/pull/7888#issuecomment-3207755446\n\n   FYI, @steveloughran @ahmarsuhail \r\n   Thanks\r\n   \n\n\n", "anujmodi2021 commented on PR #7889:\nURL: https://github.com/apache/hadoop/pull/7889#issuecomment-3207761504\n\n   @steveloughran @ahmarsuhail \r\n   Let me know if this is good to merge.\r\n   Thanks\n\n\n", "ahmarsuhail merged PR #7888:\nURL: https://github.com/apache/hadoop/pull/7888\n\n\n", "ahmarsuhail merged PR #7889:\nURL: https://github.com/apache/hadoop/pull/7889\n\n\n", "ahmarsuhail opened a new pull request, #7920:\nURL: https://github.com/apache/hadoop/pull/7920\n\n   Reverts apache/hadoop#7888\n\n\n", "ahmarsuhail closed pull request #7920: Revert \"HADOOP-19650. [ABFS][Backport-3.4.2] Fixing NPE when close() called on uninitialized AzureBlobFileSystem\"\nURL: https://github.com/apache/hadoop/pull/7920\n\n\n", "hadoop-yetus commented on PR #7920:\nURL: https://github.com/apache/hadoop/pull/7920#issuecomment-3236755969\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 40s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.4.2 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  37m 27s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  branch-3.4.2 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  branch-3.4.2 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 38s |  |  branch-3.4.2 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  branch-3.4.2 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7920/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 1 unchanged - 0 fixed = 2 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 44s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 28s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 128m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7920/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7920 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 15ea4ce1ec3b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4.2 / 88dcaa58e8f0ca5850444ce56867884b3149f101 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7920/1/testReport/ |\r\n   | Max. process+thread count | 742 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7920/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "code\r\n{code}\r\n  public void testABFSConstructor() throws Throwable {\r\n    new Az", "qna": [{"question": "What is the issue title?", "answer": "ABFS: NPE when close() called on uninitialized filesystem"}, {"question": "Who reported this issue?", "answer": "Steve Loughran"}]}
{"key": "HADOOP-19649", "project": "HADOOP", "title": "ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade", "status": "Resolved", "reporter": "Anuj Modi", "created": "2025-08-13T06:11:03.000+0000", "description": "After https://issues.apache.org/jira/browse/HADOOP-19425\r\n\r\nmost of the integration tests are getting skipped. All tests need to be fixed with this PR\u00a0", "comments": ["anujmodi2021 opened a new pull request, #7868:\nURL: https://github.com/apache/hadoop/pull/7868\n\n   ### Description of PR\r\n   After https://issues.apache.org/jira/browse/HADOOP-19425\r\n   most of the integration tests are getting skipped. All tests need to be fixed with this PR \r\n   \r\n   ### How was this patch tested?\r\n   Test Suite ran.\n\n\n", "Copilot commented on code in PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#discussion_r2272827582\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java:\n##########\n@@ -608,12 +608,12 @@ public void testCreateExplicitDirectoryOverDfsAppendOverBlob()\n    **/\n   @Test\n   public void testRecreateAppendAndFlush() throws IOException {\n+      assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse();\n+      assumeThat(getIngressServiceType()).isEqualTo(AbfsServiceType.BLOB);\n\nReview Comment:\n   This assumption is placed after the assertThrows() call begins, but should be before it. The assumption should be checked before setting up the exception assertion to ensure the test conditions are met first.\n   ```suggestion\n         assumeThat(getIngressServiceType()).isEqualTo(AbfsServiceType.BLOB);\n   ```\n\n\n\n", "hadoop-yetus commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3183429913\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m  4s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 9 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 16s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7868 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 51d70a663e18 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 64f15bfbb96b28d201067683e6e619496d0683da |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/1/testReport/ |\r\n   | Max. process+thread count | 719 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3186336426\n\n   @anujmodi2021 Thank you for your contribution! LGTM.\n\n\n", "anujmodi2021 commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3187064274\n\n   ------------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 209\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 863, Failures: 0, Errors: 0, Skipped: 161\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 702, Failures: 0, Errors: 0, Skipped: 240\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 220\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 707, Failures: 0, Errors: 0, Skipped: 135\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 242\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 704, Failures: 0, Errors: 0, Skipped: 147\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 189\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 734, Failures: 0, Errors: 0, Skipped: 216\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 239\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\n\n\n", "slfan1989 commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3190296832\n\n   @anujmodi2021 Thank you for your contribution! I noticed a small improvement we could make in this PR: removing the junit-vintage-engine dependency. This dependency is used for running mixed JUnit 4 and JUnit 5 tests, but since this module has been fully migrated to JUnit 5, I believe we can safely remove it.\r\n   \r\n   https://github.com/apache/hadoop/blob/1abdf72dca0c530c265859362b2a2d574d8c9d72/hadoop-tools/hadoop-azure/pom.xml#L363-L367\n\n\n", "anmolanmol1234 commented on code in PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#discussion_r2281356462\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFilesystemAcl.java:\n##########\n@@ -56,33 +56,55 @@\n  * Test acl operations.\n  */\n public class ITestAzureBlobFilesystemAcl extends AbstractAbfsIntegrationTest {\n+\n\nReview Comment:\n   revert changes with extra spaces\n\n\n\n", "anmolanmol1234 commented on code in PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#discussion_r2281359848\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFilesystemAcl.java:\n##########\n@@ -238,41 +276,46 @@ public void testModifyAclEntriesStickyBit() throws Exception {\n     fs.modifyAclEntries(path, aclSpec);\n     AclStatus s = fs.getAclStatus(path);\n     AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\n-    assertArrayEquals(new AclEntry[]{aclEntry(ACCESS, USER, FOO, READ_EXECUTE),\n-        aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL),\n-        aclEntry(DEFAULT, USER, FOO, READ_EXECUTE), aclEntry(DEFAULT, GROUP, READ_EXECUTE),\n-        aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE)},\n+    assertArrayEquals(new AclEntry[]{\n+            aclEntry(ACCESS, USER, FOO, READ_EXECUTE),\n+            aclEntry(ACCESS, GROUP, READ_EXECUTE),\n+            aclEntry(DEFAULT, USER, ALL),\n+            aclEntry(DEFAULT, USER, FOO, READ_EXECUTE),\n+            aclEntry(DEFAULT, GROUP, READ_EXECUTE),\n+            aclEntry(DEFAULT, MASK, READ_EXECUTE),\n+            aclEntry(DEFAULT, OTHER, NONE)\n+        },\n         returned);\n     assertPermission(fs, (short) 01750);\n   }\n \n   @Test\n   public void testModifyAclEntriesPathNotFound() throws Exception {\n-      Assertions.assertThrows(FileNotFoundException.class, () -> {\n-          final AzureBlobFileSystem fs = this.getFileSystem();\n-          assumeTrue(getIsNamespaceEnabled(fs));\n-          path = new Path(testRoot, UUID.randomUUID().toString());\n-          List<AclEntry> aclSpec = Lists.newArrayList(\n-        aclEntry(ACCESS, USER, ALL),\n-        aclEntry(ACCESS, USER, FOO, ALL),\n-        aclEntry(ACCESS, GROUP, READ_EXECUTE),\n-        aclEntry(ACCESS, OTHER, NONE));\n-          fs.modifyAclEntries(path, aclSpec);\n-      });\n+    assumeTrue(getIsNamespaceEnabled(getFileSystem()));\n+    Assertions.assertThrows(FileNotFoundException.class, () -> {\n+      final AzureBlobFileSystem fs = this.getFileSystem();\n+      path = new Path(testRoot, UUID.randomUUID().toString());\n+      List<AclEntry> aclSpec = Lists.newArrayList(\n+          aclEntry(ACCESS, USER, ALL),\n+          aclEntry(ACCESS, USER, FOO, ALL),\n+          aclEntry(ACCESS, GROUP, READ_EXECUTE),\n+          aclEntry(ACCESS, OTHER, NONE));\n+      fs.modifyAclEntries(path, aclSpec);\n+    });\n   }\n \n   @Test\n   public void testModifyAclEntriesDefaultOnFile() throws Exception {\n-      Assertions.assertThrows(Exception.class, () -> {\n-          final AzureBlobFileSystem fs = this.getFileSystem();\n-          assumeTrue(getIsNamespaceEnabled(fs));\n-          path = new Path(testRoot, UUID.randomUUID().toString());\n-          fs.create(path).close();\n-          fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\n-          List<AclEntry> aclSpec = Lists.newArrayList(\n-        aclEntry(DEFAULT, USER, FOO, ALL));\n-          fs.modifyAclEntries(path, aclSpec);\n-      });\n+    Assertions.assertThrows(Exception.class, () -> {\n+      final AzureBlobFileSystem fs = this.getFileSystem();\n\nReview Comment:\n   same as above,  only space changes can be reverted \n\n\n\n", "hadoop-yetus commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3195393896\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 9 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 23s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  78m  3s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7868 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 14573deebbd7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bf4b108ed612d69707da0fd5a95294a2734b3986 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/2/testReport/ |\r\n   | Max. process+thread count | 700 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#discussion_r2282011563\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFilesystemAcl.java:\n##########\n@@ -56,33 +56,55 @@\n  * Test acl operations.\n  */\n public class ITestAzureBlobFilesystemAcl extends AbstractAbfsIntegrationTest {\n+\n\nReview Comment:\n   So when the last time these changes were made the proper code-style was not set by whoever made these changes.\r\n   In this PR these changes came in automatically due to importing the recommended code-style.\r\n   \r\n   Since this is the right thing to do, I feel like it might be okay to retain these changes. Anyways this is a small PR.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFilesystemAcl.java:\n##########\n@@ -238,41 +276,46 @@ public void testModifyAclEntriesStickyBit() throws Exception {\n     fs.modifyAclEntries(path, aclSpec);\n     AclStatus s = fs.getAclStatus(path);\n     AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\n-    assertArrayEquals(new AclEntry[]{aclEntry(ACCESS, USER, FOO, READ_EXECUTE),\n-        aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL),\n-        aclEntry(DEFAULT, USER, FOO, READ_EXECUTE), aclEntry(DEFAULT, GROUP, READ_EXECUTE),\n-        aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE)},\n+    assertArrayEquals(new AclEntry[]{\n+            aclEntry(ACCESS, USER, FOO, READ_EXECUTE),\n+            aclEntry(ACCESS, GROUP, READ_EXECUTE),\n+            aclEntry(DEFAULT, USER, ALL),\n+            aclEntry(DEFAULT, USER, FOO, READ_EXECUTE),\n+            aclEntry(DEFAULT, GROUP, READ_EXECUTE),\n+            aclEntry(DEFAULT, MASK, READ_EXECUTE),\n+            aclEntry(DEFAULT, OTHER, NONE)\n+        },\n         returned);\n     assertPermission(fs, (short) 01750);\n   }\n \n   @Test\n   public void testModifyAclEntriesPathNotFound() throws Exception {\n-      Assertions.assertThrows(FileNotFoundException.class, () -> {\n-          final AzureBlobFileSystem fs = this.getFileSystem();\n-          assumeTrue(getIsNamespaceEnabled(fs));\n-          path = new Path(testRoot, UUID.randomUUID().toString());\n-          List<AclEntry> aclSpec = Lists.newArrayList(\n-        aclEntry(ACCESS, USER, ALL),\n-        aclEntry(ACCESS, USER, FOO, ALL),\n-        aclEntry(ACCESS, GROUP, READ_EXECUTE),\n-        aclEntry(ACCESS, OTHER, NONE));\n-          fs.modifyAclEntries(path, aclSpec);\n-      });\n+    assumeTrue(getIsNamespaceEnabled(getFileSystem()));\n+    Assertions.assertThrows(FileNotFoundException.class, () -> {\n+      final AzureBlobFileSystem fs = this.getFileSystem();\n+      path = new Path(testRoot, UUID.randomUUID().toString());\n+      List<AclEntry> aclSpec = Lists.newArrayList(\n+          aclEntry(ACCESS, USER, ALL),\n+          aclEntry(ACCESS, USER, FOO, ALL),\n+          aclEntry(ACCESS, GROUP, READ_EXECUTE),\n+          aclEntry(ACCESS, OTHER, NONE));\n+      fs.modifyAclEntries(path, aclSpec);\n+    });\n   }\n \n   @Test\n   public void testModifyAclEntriesDefaultOnFile() throws Exception {\n-      Assertions.assertThrows(Exception.class, () -> {\n-          final AzureBlobFileSystem fs = this.getFileSystem();\n-          assumeTrue(getIsNamespaceEnabled(fs));\n-          path = new Path(testRoot, UUID.randomUUID().toString());\n-          fs.create(path).close();\n-          fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\n-          List<AclEntry> aclSpec = Lists.newArrayList(\n-        aclEntry(DEFAULT, USER, FOO, ALL));\n-          fs.modifyAclEntries(path, aclSpec);\n-      });\n+    Assertions.assertThrows(Exception.class, () -> {\n+      final AzureBlobFileSystem fs = this.getFileSystem();\n\nReview Comment:\n   Same as above\n\n\n\n", "anujmodi2021 commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3196145863\n\n   ------------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 209\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 863, Failures: 0, Errors: 0, Skipped: 161\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 702, Failures: 0, Errors: 0, Skipped: 240\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 220\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 707, Failures: 0, Errors: 0, Skipped: 135\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 242\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 704, Failures: 0, Errors: 0, Skipped: 147\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 189\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 734, Failures: 0, Errors: 0, Skipped: 216\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 239\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   Time taken: 198 mins 45 secs.\r\n   \n\n\n", "slfan1989 commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3198853166\n\n   Thanks everyone for pushing this PR forward and for the review.\n\n\n", "anujmodi2021 merged PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868\n\n\n", "anujmodi2021 commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3199223431\n\n   > Thanks everyone for pushing this PR forward and for the review.\r\n   \r\n   Thanks for review. Have merged the PR.\n\n\n", "anujmodi2021 commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3199224446\n\n   @slfan1989 are we pursuing this upgrade for upcoming 3.4.2 release as well? Or is it targetted for 3.5.0 only?\n\n\n", "slfan1989 commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3199287275\n\n   > @slfan1989 are we pursuing this upgrade for upcoming 3.4.2 release as well? Or is it targetted for 3.5.0 only?\r\n   \r\n   Personally, I think it\u2019s needed, but it hasn\u2019t gone through community discussion yet. For now, we should apply it to version 3.5 first.\r\n   \n\n\n"], "labels": ["pull-request-available"], "summary": "After https://issues", "qna": [{"question": "What is the issue title?", "answer": "ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade"}, {"question": "Who reported this issue?", "answer": "Anuj Modi"}]}
{"key": "HADOOP-19648", "project": "HADOOP", "title": "cos use token credential will lost token field", "status": "Resolved", "reporter": "sanqingleo", "created": "2025-08-11T02:44:05.000+0000", "description": "Hi,\r\n\r\nI've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token).\r\n\r\nIn the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n\r\nFurthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n!image-2025-08-11-10-37-12-451.png|width=1048,height=540! \r\n!image-2025-08-11-10-42-36-375.png!", "comments": ["leosanqing opened a new pull request, #7866:\nURL: https://github.com/apache/hadoop/pull/7866\n\n   \r\n   ### Description of PR\r\n   In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n   Furthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [\u2714] Does the title or this PR starts with the corresponding JIRA issue id \r\n   \n\n\n", "leosanqing closed pull request #7866: HADOOP-19648. [hotfix] Cos use token credential will lose token field\nURL: https://github.com/apache/hadoop/pull/7866\n\n\n", "leosanqing opened a new pull request, #7867:\nURL: https://github.com/apache/hadoop/pull/7867\n\n   \r\n   ### Description of PR\r\n   \r\n   In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n   Furthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [\u2714] Does the title or this PR starts with the corresponding JIRA issue id \r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3173339571\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 11s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 22s |  |  hadoop-cos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 156m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7867 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 0a4e3db4c896 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a2fb2f41505dd147136a842dc66395632cee38f8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/1/testReport/ |\r\n   | Max. process+thread count | 540 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3175078496\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 21s |  |  hadoop-cos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 138m 35s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7867 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 3d08225597f2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4d939984a47fcbbb4e3ca1cd5ffa32a0cc953838 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/2/testReport/ |\r\n   | Max. process+thread count | 535 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "leosanqing commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3188421349\n\n   @slfan1989 @cnauroth hi guys\uff0ccould you help me review this pr\uff1fthx   : )\n\n\n", "cnauroth commented on code in PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#discussion_r2279916489\n\n\n##########\nhadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java:\n##########\n@@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration)\n       }\n     }\n   }\n+\n+  @Test\n+  public void testTmpTokenCredentialsProvider() throws Throwable {\n+    Configuration configuration = new Configuration();\n+    // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials\n+    // Provider.\n+    configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER,\n+        \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\");\n+    validateTmpTokenCredentials(this.fsUri, configuration);\n+  }\n+\n+  private void validateTmpTokenCredentials(URI uri, Configuration configuration)\n+      throws IOException {\n+    if (null != configuration) {\n+      COSCredentialsProvider credentialsProvider =\n+          CosNUtils.createCosCredentialsProviderSet(uri, configuration);\n+      COSCredentials cosCredentials = credentialsProvider.getCredentials();\n+      assertNotNull(cosCredentials, \"The cos credentials obtained is null.\");\n+      assertTrue(\n+          StringUtils.equalsIgnoreCase(configuration.get(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER),\n+              \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"),\n+          \"CredentialsProvider must be DynamicTemporaryCosnCredentialsProvider\");\n+\n+      if (!(cosCredentials instanceof BasicSessionCredentials)) {\n\nReview Comment:\n   This could be simplified to:\r\n   \r\n   ```\r\n   assertTrue(cosCredentials instanceof BasicSessionCredentials, \"...\");\r\n   ```\r\n   \n\n\n\n##########\nhadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java:\n##########\n@@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration)\n       }\n     }\n   }\n+\n+  @Test\n+  public void testTmpTokenCredentialsProvider() throws Throwable {\n+    Configuration configuration = new Configuration();\n+    // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials\n+    // Provider.\n+    configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER,\n+        \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\");\n+    validateTmpTokenCredentials(this.fsUri, configuration);\n+  }\n+\n+  private void validateTmpTokenCredentials(URI uri, Configuration configuration)\n+      throws IOException {\n+    if (null != configuration) {\n\nReview Comment:\n   Is this null check required? It seems like `configuration` will always be non-null.\n\n\n\n##########\nhadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java:\n##########\n@@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration)\n       }\n     }\n   }\n+\n+  @Test\n+  public void testTmpTokenCredentialsProvider() throws Throwable {\n+    Configuration configuration = new Configuration();\n+    // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials\n+    // Provider.\n+    configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER,\n+        \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\");\n+    validateTmpTokenCredentials(this.fsUri, configuration);\n+  }\n+\n+  private void validateTmpTokenCredentials(URI uri, Configuration configuration)\n+      throws IOException {\n+    if (null != configuration) {\n+      COSCredentialsProvider credentialsProvider =\n+          CosNUtils.createCosCredentialsProviderSet(uri, configuration);\n+      COSCredentials cosCredentials = credentialsProvider.getCredentials();\n+      assertNotNull(cosCredentials, \"The cos credentials obtained is null.\");\n+      assertTrue(\n+          StringUtils.equalsIgnoreCase(configuration.get(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER),\n+              \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"),\n+          \"CredentialsProvider must be DynamicTemporaryCosnCredentialsProvider\");\n+\n+      if (!(cosCredentials instanceof BasicSessionCredentials)) {\n+        fail(\"cosCredentials must be instanceof BasicSessionCredentials\");\n+      }\n+\n+      if (!StringUtils.equals(cosCredentials.getCOSAccessKeyId(), \"ak\") || !StringUtils.equals(\n\nReview Comment:\n   Instead of conditional logic around `fail`, I suggest that this should be 3 separate `assertEquals` for access key, secret key and session token.\n\n\n\n", "leosanqing commented on code in PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#discussion_r2284575792\n\n\n##########\nhadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java:\n##########\n@@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration)\n       }\n     }\n   }\n+\n+  @Test\n+  public void testTmpTokenCredentialsProvider() throws Throwable {\n+    Configuration configuration = new Configuration();\n+    // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials\n+    // Provider.\n+    configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER,\n+        \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\");\n+    validateTmpTokenCredentials(this.fsUri, configuration);\n+  }\n+\n+  private void validateTmpTokenCredentials(URI uri, Configuration configuration)\n+      throws IOException {\n+    if (null != configuration) {\n\nReview Comment:\n   yep, you'r right.\n\n\n\n", "leosanqing commented on code in PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#discussion_r2284603156\n\n\n##########\nhadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java:\n##########\n@@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration)\n       }\n     }\n   }\n+\n+  @Test\n+  public void testTmpTokenCredentialsProvider() throws Throwable {\n+    Configuration configuration = new Configuration();\n+    // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials\n+    // Provider.\n+    configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER,\n+        \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\");\n+    validateTmpTokenCredentials(this.fsUri, configuration);\n+  }\n+\n+  private void validateTmpTokenCredentials(URI uri, Configuration configuration)\n+      throws IOException {\n+    if (null != configuration) {\n+      COSCredentialsProvider credentialsProvider =\n+          CosNUtils.createCosCredentialsProviderSet(uri, configuration);\n+      COSCredentials cosCredentials = credentialsProvider.getCredentials();\n+      assertNotNull(cosCredentials, \"The cos credentials obtained is null.\");\n+      assertTrue(\n+          StringUtils.equalsIgnoreCase(configuration.get(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER),\n+              \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"),\n+          \"CredentialsProvider must be DynamicTemporaryCosnCredentialsProvider\");\n+\n+      if (!(cosCredentials instanceof BasicSessionCredentials)) {\n\nReview Comment:\n   : ).  Thx for your suggestion. I've also found a more standardized way to write the code.   \r\n   \r\n   `assertInstanceOf(BasicSessionCredentials.class, cosCredentials,\r\n           \"cosCredentials must be instanceof BasicSessionCredentials\");`\n\n\n\n", "leosanqing commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3199934458\n\n   > @leosanqing , thank you for the patch. I commented with a few suggestions.\r\n   > \r\n   > Can you or someone else make sure the integration tests are passing? I don't have the means to run those myself.\r\n   \r\n   Thx bro. \r\n   \r\n   Sure, I'v added a new ITest, and pass the test on my local env.(the failed tests are Junit method not found, I've looked up each failed tests)\r\n   <img width=\"2510\" height=\"481\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f3f6b152-6dc5-471f-8dbb-f21e4d55c33c\" />\r\n   \r\n   \r\n   When I'm not change code. This ITest will throw exception like this.\r\n   <img width=\"2419\" height=\"1374\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f381bc6d-0ab7-4a1e-920d-0fae8e6eb3e5\" />\r\n   \r\n   \r\n   and this is the new code for testing.\r\n   <img width=\"2509\" height=\"475\" alt=\"image\" src=\"https://github.com/user-attachments/assets/69ad045e-449b-47ca-b13c-6d606767c099\" />\r\n   \r\n   \n\n\n", "leosanqing commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200102193\n\n   > @leosanqing , thank you for the patch. I commented with a few suggestions.\r\n   > \r\n   > Can you or someone else make sure the integration tests are passing? I don't have the means to run those myself.\r\n   \r\n   For newest, I add the test dependence, all tests are passed.\r\n   <img width=\"2278\" height=\"1151\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2ab96a2e-c789-4c11-b1c3-7a2d825a3f7a\" />\r\n   \n\n\n", "hadoop-yetus commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200299993\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 37s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/artifact/out/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt) |  The patch fails to run checkstyle in hadoop-cos  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 21s |  |  hadoop-cos in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/artifact/out/results-asflicense.txt) |  The patch generated 2 ASF License warnings.  |\r\n   |  |   | 129m 59s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7867 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux d39db1ea78c7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 23868390c030e1b63dce02867f5d5baf816a45c5 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/testReport/ |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200543127\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 35s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/artifact/out/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt) |  The patch fails to run checkstyle in hadoop-cos  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 22s |  |  hadoop-cos in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/artifact/out/results-asflicense.txt) |  The patch generated 2 ASF License warnings.  |\r\n   |  |   | 136m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7867 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 4e15f55ee773 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0a56214a7af88643a157e0c743258af4fceb4f2d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/testReport/ |\r\n   | Max. process+thread count | 531 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "leosanqing commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200678324\n\n   Sorry for committing so many times, I'm not used for hadoop code style. Pls forgive me. \n\n\n", "hadoop-yetus commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200958862\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 11s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 14s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 15s |  |  hadoop-cos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  76m 13s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7867 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 6c557ebeaf9a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0bab4e6ecff4636cbb8fe5af133af5652cef157e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/6/testReport/ |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3201095625\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  1s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/5/artifact/out/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt) |  The patch fails to run checkstyle in hadoop-cos  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 21s |  |  hadoop-cos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 136m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7867 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 1b78dd2042d8 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f2dbd3e7e6603a5ceccf86d2c17acbc9ab06ee62 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/5/testReport/ |\r\n   | Max. process+thread count | 587 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "cnauroth commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3204031971\n\n   > Sorry for committing so many times, I'm not used for hadoop code style. Pls forgive me.\r\n   \r\n   No worries! We appreciate the contribution!\n\n\n", "leosanqing commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3218110763\n\n   @cnauroth Hey, bro, sorry to ping you on a weekend. Just a friendly reminder about this PR, looks like it hasn't been merged yet. \r\n   \r\n   Happy weekend!\n\n\n", "cnauroth closed pull request #7867: HADOOP-19648. [hotfix] Cos use token credential will lose token field\nURL: https://github.com/apache/hadoop/pull/7867\n\n\n", "cnauroth commented on PR #7867:\nURL: https://github.com/apache/hadoop/pull/7867#issuecomment-3221459343\n\n   @leosanqing , thank you for the reminder. I have committed this to trunk. This would ship the fix in Hadoop 3.5.0. I looked at backporting to branch-3.4, but there were conflicts. If you need the patch in earlier versions, please send up a separate pull request targeting branch-3.4. Thank you for the contribution!\n\n\n", "leosanqing opened a new pull request, #7911:\nURL: https://github.com/apache/hadoop/pull/7911\n\n   \r\n   \r\n   ### Description of PR\r\n   In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n   Furthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n   \r\n   This is same as https://github.com/apache/hadoop/pull/7867,but there were some conflicts. so I send up a separate pull request targeting branch-3.4.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n    ITests  are passed on my local env.\r\n   \r\n   <img width=\"2528\" height=\"576\" alt=\"image\" src=\"https://github.com/user-attachments/assets/fff41a10-f423-46ee-a8f9-5a806175c97f\" />\r\n   \r\n   ### For code changes:\r\n   \r\n   - [\u2714] Does the title or this PR starts with the corresponding JIRA issue id (yes)?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7911:\nURL: https://github.com/apache/hadoop/pull/7911#issuecomment-3231866522\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  19m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 58s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 26s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 54s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 24s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 21s |  |  hadoop-cos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 145m 34s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7911/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7911 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 5b677ee4dfa6 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / 7cf257320847af9fc5a7075abe7ffda257bee4ee |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7911/1/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7911/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "leosanqing commented on PR #7911:\nURL: https://github.com/apache/hadoop/pull/7911#issuecomment-3231885127\n\n   @cnauroth hi, bro, I send a new pr to merge targeting brach-3.4. Previous conflicts are test dependencies.  ITest's results are here. Cloud you help me  to review this pr?\r\n   <img width=\"2528\" height=\"576\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5098c1c2-7dd4-4b4f-afed-3de28355d47d\" />\r\n   \n\n\n", "cnauroth closed pull request #7911: HADOOP-19648. Cos use token credential will lose token field\nURL: https://github.com/apache/hadoop/pull/7911\n\n\n", "cnauroth commented on PR #7911:\nURL: https://github.com/apache/hadoop/pull/7911#issuecomment-3238260120\n\n   Thank you again @leosanqing . I committed this to branch-3.4.\n\n\n", "leosanqing commented on PR #7911:\nURL: https://github.com/apache/hadoop/pull/7911#issuecomment-3238639566\n\n   > Thank you again @leosanqing . I committed this to branch-3.4.\r\n   Hey\uff0cthank you for your merging.\r\n   \r\n   I'm not sure if branch-3.3 is still being maintained, but I have tested this PR against branch-3.3 in my local environment, and it passed. If you are still releasing new versions for 3.3, this could be merged into branch-3.3 as well, since it was the original branch where this COS feature was first introduced.\n\n\n", "cnauroth commented on PR #7911:\nURL: https://github.com/apache/hadoop/pull/7911#issuecomment-3245771396\n\n   > > Thank you again @leosanqing . I committed this to branch-3.4.\r\n   > > Hey\uff0cthank you for your merging.\r\n   > \r\n   > I'm not sure if branch-3.3 is still being maintained, but I have tested this PR against branch-3.3 in my local environment, and it passed. If you are still releasing new versions for 3.3, this could be merged into branch-3.3 as well, since it was the original branch where this COS feature was first introduced.\r\n   \r\n   I'm not aware of any specific schedule for another 3.3 release, but there are still a few patches going in there. I merged this to branch-3.3. Thanks again, @leosanqing .\n\n\n"], "labels": ["pull-request-available"], "summary": "Hi,\r\n\r\nI've discovered a bug when accessing COSN using temporary credentials (ac", "qna": [{"question": "What is the issue title?", "answer": "cos use token credential will lost token field"}, {"question": "Who reported this issue?", "answer": "sanqingleo"}]}
{"key": "HADOOP-19647", "project": "HADOOP", "title": "ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations", "status": "Open", "reporter": "Anuj Modi", "created": "2025-08-07T07:54:02.000+0000", "description": "AbfsInputStream should take in account the Read Policy set by user with Open File Options. Based on the read policy set, appropriate optimizations should be enabled and kicked in.", "comments": ["key ones are random, whole file and sequential, with recognising avro, parquet, a bonus\r\n\r\n* parquet does now open files with \"parquet\" as first entry\r\n* distcp always uses whole-file where a few large 64MB+  blocks deliver great performance"], "labels": [], "summary": "AbfsInputStream should take in account the Read Policy set by user with Open Fil", "qna": [{"question": "What is the issue title?", "answer": "ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations"}, {"question": "Who reported this issue?", "answer": "Anuj Modi"}]}
{"key": "HADOOP-19646", "project": "HADOOP", "title": "S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions", "status": "Resolved", "reporter": "Shilun Fan", "created": "2025-08-06T05:12:27.000+0000", "description": "This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability.\r\nh4. Scope of changes:\r\n * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}Assumptions.assumeTrue(...){}}};\r\n\r\n * Ensure the assertion logic remains consistent with the original behavior;\r\n\r\n * Update any outdated import statements referencing JUnit4's {{{}Assume{}}};\r\n\r\n * Verify that all affected unit tests pass correctly under JUnit5.", "comments": ["slfan1989 opened a new pull request, #7858:\nURL: https://github.com/apache/hadoop/pull/7858\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19646. [JDK17] Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3157797389\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 23s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7858 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5abd35351f22 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6ca49d5da145efa997723827e84e0d9b047683a3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3158154572\n\n   @steveloughran @anujmodi2021 Could you please review this PR? Thank you very much!\n\n\n", "steveloughran commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3161012685\n\n   Can we move to AssertJ here?\r\n   1. it's a better syntax, and nicely extensible\r\n   1. it lets us cherrypick into java4 branches without any problems\r\n   1. everyone who has already used it knows the syntax\r\n   \r\n   I don't want to invest any time learning JUnit5's assert syntax, not given AssertJ is good and I'm still learning the nuances in what is a very powerful assertion language\n\n\n", "slfan1989 commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3162122354\n\n   > Can we move to AssertJ here?\r\n   > \r\n   > 1. it's a better syntax, and nicely extensible\r\n   > 2. it lets us cherrypick into java4 branches without any problems\r\n   > 3. everyone who has already used it knows the syntax\r\n   > \r\n   > I don't want to invest any time learning JUnit5's assert syntax, not given AssertJ is good and I'm still learning the nuances in what is a very powerful assertion language\r\n   \r\n   Thank you very much for your feedback! I completely agree with your suggestions. I will make improvements in this PR accordingly. Once the upgrade to JUnit 5 is fully completed, I will create a separate JIRA ticket to batch-convert JUnit 5 assertions to AssertJ.\n\n\n", "hadoop-yetus commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3169619857\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m  7s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 56s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 27s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 139m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7858 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 9af81ee0d453 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 21b0dfe8a802763f56d63b29a99ee3e2281a3fa1 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/2/testReport/ |\r\n   | Max. process+thread count | 549 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3173954615\n\n   @steveloughran Could you please help review this PR? Thank you very much! I\u2019ve updated it to use AssertJ.\n\n\n", "slfan1989 merged PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858\n\n\n", "slfan1989 commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3177411353\n\n   > LGTM\r\n   > +1\r\n   \r\n   @steveloughran Thank you very much for the review!\n\n\n", "zhtttylz opened a new pull request, #7951:\nURL: https://github.com/apache/hadoop/pull/7951\n\n   ### Description of PR\r\n   JIRA:HADOOP-19646. [Addendum] [JDK17] Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions.\r\n   \r\n   ### How was this patch tested?\r\n   Junit Test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "pan3793 commented on code in PR #7951:\nURL: https://github.com/apache/hadoop/pull/7951#discussion_r2340668453\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestTreewalkProblems.java:\n##########\n@@ -316,7 +316,7 @@ public void testDistCp() throws Throwable {\n           options, getConfiguration());\n     } else {\n       // distcp fails if uploads are visible\n-      intercept(org.junit.ComparisonFailure.class, () -> {\n+      intercept(org.opentest4j.AssertionFailedError.class, () -> {\n\nReview Comment:\n   why not import it at the beginning?\n\n\n\n", "zhtttylz commented on code in PR #7951:\nURL: https://github.com/apache/hadoop/pull/7951#discussion_r2340699995\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestTreewalkProblems.java:\n##########\n@@ -316,7 +316,7 @@ public void testDistCp() throws Throwable {\n           options, getConfiguration());\n     } else {\n       // distcp fails if uploads are visible\n-      intercept(org.junit.ComparisonFailure.class, () -> {\n+      intercept(org.opentest4j.AssertionFailedError.class, () -> {\n\nReview Comment:\n   Thanks for the feedback\u2014I\u2019ll make the updates right away!\n\n\n\n", "hadoop-yetus commented on PR #7951:\nURL: https://github.com/apache/hadoop/pull/7951#issuecomment-3280996134\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 47s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 26s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 142m 35s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7951 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2780b877b219 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 775a0d2508eb5c067af32f20f99a59c3e6ec9748 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/1/testReport/ |\r\n   | Max. process+thread count | 526 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7951:\nURL: https://github.com/apache/hadoop/pull/7951#issuecomment-3281110579\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  28m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 24s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 15s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 35s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  82m 40s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7951 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 05afa27f9842 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 75dad0ad76f5d20b28f9cd4db43206bbee1b3e03 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/2/testReport/ |\r\n   | Max. process+thread count | 703 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "slfan1989 commented on PR #7951:\nURL: https://github.com/apache/hadoop/pull/7951#issuecomment-3282915596\n\n   @ahmarsuhail Could you help review this PR? Thank you very much!\n\n\n", "ahmarsuhail commented on PR #7951:\nURL: https://github.com/apache/hadoop/pull/7951#issuecomment-3285924530\n\n   Thanks @zhtttylz! +1, LGTM.\r\n   \r\n   \n\n\n", "slfan1989 merged PR #7951:\nURL: https://github.com/apache/hadoop/pull/7951\n\n\n", "slfan1989 commented on PR #7951:\nURL: https://github.com/apache/hadoop/pull/7951#issuecomment-3286046276\n\n   @zhtttylz Thanks for the contriburion! @pan3793 @ahmarsuhail Thanks for the review!\n\n\n"], "labels": ["pull-request-available"], "summary": "This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4", "qna": [{"question": "What is the issue title?", "answer": "S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions"}, {"question": "Who reported this issue?", "answer": "Shilun Fan"}]}
{"key": "HADOOP-19645", "project": "HADOOP", "title": "ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done.", "status": "Resolved", "reporter": "Anuj Modi", "created": "2025-07-29T12:45:56.000+0000", "description": "There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header.\r\n\r\nFollowing are types of read we want to identify:\r\n # Direct Read: Read from a given position in remote file. This will be synchronous read\r\n # Normal Read: Read from current seeked position where read ahead was bypassed. This will be synchronous read.\r\n # Prefetch Read: Read triggered from background threads filling up in memory cache. This will be asynchronous read.\r\n # Missed Cache Read: Read triggered after nothing was received from read ahead. This will be synchronous read.\r\n # Footer Read: Read triggered as part of footer read optimization. This will be synchronous.\r\n # Small File Read: Read triggered as a part of small file read. This will be synchronous read.\r\n\r\nWe will add another field in the Tracing Header (Client Request Id) for each request. We can call this field \"Operation Specific Header\" very similar to how we have \"Retry Header\" today. As part of this we will only use it for read operations keeping it empty for other operations. Moving ahead f we need to publish any operation specific info, same header can be used.", "comments": ["hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135193019\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 14 new + 1 unchanged - 0 fixed = 15 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 59s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 51s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c64bcd4ab2e9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ea1572a40346a9ddfa4e80f4f1a4925308205175 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135296997\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 17 new + 1 unchanged - 0 fixed = 18 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 28s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 28s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 59s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 30s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7be945cb4d05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 42ecdd05eb6954bdfd26d5022bf4c8a517c607ba |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/testReport/ |\r\n   | Max. process+thread count | 540 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135629353\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  2s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  2s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 58s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 18 new + 1 unchanged - 0 fixed = 19 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  46m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 148m 56s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5f43a0e97825 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 224f712fee069bd839f8c27a979367e61cec8c17 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/testReport/ |\r\n   | Max. process+thread count | 596 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135851673\n\n   -----------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 34\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 172\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 34\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 395\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 35\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 234\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 58\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 285\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 29\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 400\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 35\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 301\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 29\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 346\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 53\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 356\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 34\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 397\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 35\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   \n\n\n", "Copilot commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2244303281\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +289,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n+    String retryHeader = String.format(\"%d\", retryCount);\n+    if (previousFailure == null) {\n+      return retryHeader;\n+    }\n+    if (CONNECTION_TIMEOUT_ABBREVIATION.equals(previousFailure) && retryPolicyAbbreviation != null) {\n+      return String.format(\"%s_%s_%s\", retryHeader, previousFailure, retryPolicyAbbreviation);\n+    }\n+    return String.format(\"%s_%s\", retryHeader, previousFailure);\n+  }\n+\n+  private String getOperationSpecificHeader(FSOperationType opType) {\n+    // Similar header can be added for other operations in the future.\n+    switch (opType) {\n+      case READ:\n+        return readSpecificHeader();\n+      default:\n+        return EMPTY_STRING; // no operation specific header\n+    }\n+  }\n+\n+  private String readSpecificHeader() {\n+    // More information on read can be added to this header in the future.\n+    // As underscore separated values.\n+    String readHeader = String.format(\"%s\", readType.toString());\n\nReview Comment:\n   The String.format with \"%s\" is unnecessary here. Use readType.toString() directly for better readability and performance.\n   ```suggestion\n       String readHeader = readType.toString();\n   ```\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,31 +213,35 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n+    case ALL_ID_FORMAT:\n       header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n+          clientCorrelationID + \":\" +\n+          clientRequestId + \":\" +\n+          fileSystemID + \":\" +\n+          getPrimaryRequestIdForHeader(retryCount > 0) + \":\" +\n+          streamID + \":\" +\n+          opType + \":\" +\n+          getRetryHeader(previousFailure, retryPolicyAbbreviation) + \":\" +\n+          ingressHandler + \":\" +\n+          position + \":\" +\n+          operatedBlobCount + \":\" +\n+          httpOperation.getTracingContextSuffix() + \":\" +\n+          getOperationSpecificHeader(opType);\n+\n       metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n       break;\n     case TWO_ID_FORMAT:\n-      header = clientCorrelationID + \":\" + clientRequestId;\n+      header =\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n+          clientCorrelationID + \":\" + clientRequestId;\n       metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n       break;\n     default:\n       //case SINGLE_ID_FORMAT\n-      header = clientRequestId;\n+      header =\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n\nReview Comment:\n   The hardcoded V1 version is used in multiple places. Consider using TracingHeaderVersion.getCurrentVersion() consistently to centralize version management.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,31 +213,35 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n+    case ALL_ID_FORMAT:\n       header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n\nReview Comment:\n   The hardcoded V1 version is used in multiple places. Consider using TracingHeaderVersion.getCurrentVersion() consistently to centralize version management.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,31 +213,35 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n+    case ALL_ID_FORMAT:\n       header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n+          clientCorrelationID + \":\" +\n+          clientRequestId + \":\" +\n+          fileSystemID + \":\" +\n+          getPrimaryRequestIdForHeader(retryCount > 0) + \":\" +\n+          streamID + \":\" +\n+          opType + \":\" +\n+          getRetryHeader(previousFailure, retryPolicyAbbreviation) + \":\" +\n+          ingressHandler + \":\" +\n+          position + \":\" +\n+          operatedBlobCount + \":\" +\n+          httpOperation.getTracingContextSuffix() + \":\" +\n+          getOperationSpecificHeader(opType);\n+\n       metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n       break;\n     case TWO_ID_FORMAT:\n-      header = clientCorrelationID + \":\" + clientRequestId;\n+      header =\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n+          clientCorrelationID + \":\" + clientRequestId;\n       metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n       break;\n     default:\n       //case SINGLE_ID_FORMAT\n-      header = clientRequestId;\n+      header =\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n\nReview Comment:\n   The hardcoded V1 version is used in multiple places. Consider using TracingHeaderVersion.getCurrentVersion() consistently to centralize version management.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderValidator.java:\n##########\n@@ -81,82 +85,93 @@ public TracingHeaderValidator(String clientCorrelationId, String fileSystemId,\n   }\n \n   private void validateTracingHeader(String tracingContextHeader) {\n-    String[] idList = tracingContextHeader.split(\":\");\n+    String[] idList = tracingContextHeader.split(\":\", -1);\n\nReview Comment:\n   [nitpick] Consider defining the split limit (-1) as a named constant to improve code readability and maintainability.\n   ```suggestion\n       String[] idList = tracingContextHeader.split(\":\", SPLIT_NO_LIMIT);\n   ```\n\n\n\n", "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3138813312\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 6 new + 5 unchanged - 0 fixed = 11 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 23s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 27s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  79m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ba7a35768638 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0d926b14ba007e88c0099c5880f78176988d2442 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2245426192\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +289,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n+    String retryHeader = String.format(\"%d\", retryCount);\n+    if (previousFailure == null) {\n+      return retryHeader;\n+    }\n+    if (CONNECTION_TIMEOUT_ABBREVIATION.equals(previousFailure) && retryPolicyAbbreviation != null) {\n+      return String.format(\"%s_%s_%s\", retryHeader, previousFailure, retryPolicyAbbreviation);\n+    }\n+    return String.format(\"%s_%s\", retryHeader, previousFailure);\n+  }\n+\n+  private String getOperationSpecificHeader(FSOperationType opType) {\n+    // Similar header can be added for other operations in the future.\n+    switch (opType) {\n+      case READ:\n+        return readSpecificHeader();\n+      default:\n+        return EMPTY_STRING; // no operation specific header\n+    }\n+  }\n+\n+  private String readSpecificHeader() {\n+    // More information on read can be added to this header in the future.\n+    // As underscore separated values.\n+    String readHeader = String.format(\"%s\", readType.toString());\n\nReview Comment:\n   Rataining it as in future we might add more info to the same field\n\n\n\n", "bhattmanish98 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2245456486\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -128,6 +128,7 @@ public final class AbfsHttpConstants {\n   public static final String STAR = \"*\";\n   public static final String COMMA = \",\";\n   public static final String COLON = \":\";\n+  public static final String HYPHEN = \"-\";\n\nReview Comment:\n   We already have CHAR_HYPHEN defined for this.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n+  }\n+\n+  public String getVersion() {\n+    return V1.version;\n\nReview Comment:\n   Same as above, it should be `return this.version`?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +286,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n\nReview Comment:\n   Please add javadoc to all newly added methods\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n\nReview Comment:\n   Shouldn't it be just `return this.fieldCount`?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n\nReview Comment:\n   Java Doc missing\n\n\n\n", "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3140309047\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 44s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  78m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b44a21d0bd2d 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9bb6cdbeda33155f0957108cfdf87b63dcefe53a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/5/testReport/ |\r\n   | Max. process+thread count | 676 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2246885435\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -544,7 +555,9 @@ private int readInternal(final long position, final byte[] b, final int offset,\n       }\n \n       // got nothing from read-ahead, do our own read now\n-      receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext));\n+      TracingContext tc = new TracingContext(tracingContext);\n+      tc.setReadType(ReadType.MISSEDCACHE_READ);\n+      receivedBytes = readRemote(position, b, offset, length, tc);\n       return receivedBytes;\n     } else {\n       LOG.debug(\"read ahead disabled, reading remote\");\n\nReview Comment:\n   Should we add readtype as normal read for this TC as well?\n\n\n\n", "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2246929766\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -442,6 +451,7 @@ private int optimisedRead(final byte[] b, final int off, final int len,\n     //  bCursor that means the user requested data has not been read.\n     if (fCursor < contentLength && bCursor > limit) {\n       restorePointerState();\n+      tracingContext.setReadType(ReadType.NORMAL_READ);\n\nReview Comment:\n   Before readOneBlock we're setting TC as normal read both here and line 439. In readOneBlock method- we're setting TC again to normal read- do we need it twice?\r\n   We can keep it once in the method only otherwise\n\n\n\n", "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2246993787\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n+          + position + COLON\n+          + operatedBlobCount + COLON\n+          + httpOperation.getTracingContextSuffix() + COLON\n+          + getOperationSpecificHeader(opType);\n\nReview Comment:\n   should we keep the op specific header before adding the HTTP client? It would get all req related info together and then network client. \r\n   Eg- .....:RE:1_EGR:NR:JDK\n\n\n\n", "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247014906\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +286,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n\nReview Comment:\n   we can remove the addFailureReasons method- it has no usage now\n\n\n\n", "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247066421\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n\nReview Comment:\n   Since the next versions would be V1.1/V1.2- so should we consider starting with V1.0/V1.1?\r\n   And with the version updates- would we update the version field in V1 only or new V1.1 enum?\n\n\n\n", "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247336266\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Footer Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(false).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(true).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Small File Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(true).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(false).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls);\n+  }\n+\n+  private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs, int fileSize, ReadType readType, int numOfReadCalls) throws Exception {\n+    Path testPath = new Path(\"testFile\");\n+    byte[] fileContent = getRandomBytesArray(fileSize);\n+    try (FSDataOutputStream oStream = fs.create(testPath)) {\n+      oStream.write(fileContent);\n+      oStream.flush();\n+    }\n+    try (FSDataInputStream iStream = fs.open(testPath)) {\n+      int bytesRead = iStream.read(new byte[fileSize], 0,\n+          fileSize);\n+      Assertions.assertThat(fileSize)\n+          .describedAs(\"Read size should match file size\")\n+          .isEqualTo(bytesRead);\n+    }\n+\n+    ArgumentCaptor<String> captor1 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> captor2 = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<byte[]> captor3 = ArgumentCaptor.forClass(byte[].class);\n+    ArgumentCaptor<Integer> captor4 = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Integer> captor5 = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<String> captor6 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<String> captor7 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n+    ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n+\n+    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+        captor1.capture(), captor2.capture(), captor3.capture(),\n+        captor4.capture(), captor5.capture(), captor6.capture(),\n+        captor7.capture(), captor8.capture(), captor9.capture());\n+    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n+    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+  }\n+\n+  private void verifyHeaderForReadTypeInTracingContextHeader(TracingContext tracingContext, ReadType readType) {\n+    AbfsHttpOperation mockOp = Mockito.mock(AbfsHttpOperation.class);\n+    doReturn(EMPTY_STRING).when(mockOp).getTracingContextSuffix();\n+    tracingContext.constructHeader(mockOp, null, null);\n+    String[] idList = tracingContext.getHeader().split(COLON, SPLIT_NO_LIMIT);\n+    Assertions.assertThat(idList).describedAs(\"Client Request Id should have all fields\").hasSize(\n+        TracingHeaderVersion.getCurrentVersion().getFieldCount());\n+    Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Operation Type Should Be Read\")\n+        .contains(FSOperationType.READ.toString());\n+    Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Read type in tracing context header should match\")\n+        .contains(readType.toString());\n+  }\n+\n+//  private testReadTypeInTracingContextHeaderInternal(ReadType readType) throws Exception {\n\nReview Comment:\n   Nit- we can remove this\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247415571\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -77,8 +81,7 @@ public class TracingContext {\n    * this field shall not be set.\n    */\n   private String primaryRequestIdForRetry;\n-\n-  private Integer operatedBlobCount = null;\n+  private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set.\n\nReview Comment:\n   why is it changed from null to 1 ?\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247428588\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n\nReview Comment:\n   So every time we add a new header, we need to add a new version ?\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247438249\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n\nReview Comment:\n   +1\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247441472\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n\nReview Comment:\n   will this need to be updated everytime a new version is introduced ?\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247441472\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n\nReview Comment:\n   this needs to be updated everytime a new version is introduced, can it be dynamically fetched ?\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247447664\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n\nReview Comment:\n   should we use getCurrentVersion here ?\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247451122\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n\nReview Comment:\n   these empty string checks are needed \n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247460310\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n+          + position + COLON\n+          + operatedBlobCount + COLON\n+          + httpOperation.getTracingContextSuffix() + COLON\n+          + getOperationSpecificHeader(opType);\n+\n+      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : EMPTY_STRING;\n       break;\n     case TWO_ID_FORMAT:\n-      header = clientCorrelationID + \":\" + clientRequestId;\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n\nReview Comment:\n   same as above getCurrentVersion ?\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247472876\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestTracingContext.java:\n##########\n@@ -326,8 +329,8 @@ fileSystemId, FSOperationType.CREATE_FILESYSTEM, tracingHeaderFormat, new Tracin\n   }\n \n   private void checkHeaderForRetryPolicyAbbreviation(String header, String expectedFailureReason, String expectedRetryPolicyAbbreviation) {\n-    String[] headerContents = header.split(\":\");\n-    String previousReqContext = headerContents[6];\n+    String[] headerContents = header.split(\":\", SPLIT_NO_LIMIT);\n\nReview Comment:\n   colon constant here as well since we are changing at other places\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247491342\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n\nReview Comment:\n   should we also verify that it is normal_read for all the three calls made ?\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247493079\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n\nReview Comment:\n   same here verify that 2 calls have prefetch_read\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247491342\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n\nReview Comment:\n   should we also verify that it is normal_read for all the three calls made, currently it verifies for contains \n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247501549\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Footer Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(false).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(true).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Small File Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(true).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(false).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls);\n+  }\n\nReview Comment:\n   One test for direct read as well ?\n\n\n\n", "violetnspct commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2249151187\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -578,6 +591,7 @@ int readRemote(long position, byte[] b, int offset, int length, TracingContext t\n         streamStatistics.remoteReadOperation();\n       }\n       LOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length);\n+      tracingContext.setPosition(String.valueOf(position));\n\nReview Comment:\n   Is there a test to verify position is correctly added to tracing context? Position is a key identifier for read operations.\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250024646\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -544,7 +555,9 @@ private int readInternal(final long position, final byte[] b, final int offset,\n       }\n \n       // got nothing from read-ahead, do our own read now\n-      receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext));\n+      TracingContext tc = new TracingContext(tracingContext);\n+      tc.setReadType(ReadType.MISSEDCACHE_READ);\n+      receivedBytes = readRemote(position, b, offset, length, tc);\n       return receivedBytes;\n     } else {\n       LOG.debug(\"read ahead disabled, reading remote\");\n\nReview Comment:\n   This is coming directly from readOneBlock() so will always be normal\n\n\n\n", "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3148617353\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  77m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5948820d65fb 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 132893fcbf3d7eb4b31ca01dbaef26c186560dd3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/6/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250346945\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -128,6 +128,7 @@ public final class AbfsHttpConstants {\n   public static final String STAR = \"*\";\n   public static final String COMMA = \",\";\n   public static final String COLON = \":\";\n+  public static final String HYPHEN = \"-\";\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +286,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250347466\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n\nReview Comment:\n   Fixed, Thanks for pointing out\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n+  }\n+\n+  public String getVersion() {\n+    return V1.version;\n\nReview Comment:\n   Fixed\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n\nReview Comment:\n   Added\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250348114\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -442,6 +451,7 @@ private int optimisedRead(final byte[] b, final int off, final int len,\n     //  bCursor that means the user requested data has not been read.\n     if (fCursor < contentLength && bCursor > limit) {\n       restorePointerState();\n+      tracingContext.setReadType(ReadType.NORMAL_READ);\n\nReview Comment:\n   Nice Catch, that seemed redundant, hence removed\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250348323\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n+          + position + COLON\n+          + operatedBlobCount + COLON\n+          + httpOperation.getTracingContextSuffix() + COLON\n+          + getOperationSpecificHeader(opType);\n\nReview Comment:\n   Sounds Better, Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +286,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250349750\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n\nReview Comment:\n   We will have simple version strings like v0, v1, v2 and so on. This will help reduce char count in clientReqId.\r\n   \r\n   With any new changes in the schema of Tracing Header (add/delete/rearrange) we need to bump up version and update the schema and getCurrentVersion method to return the latest version.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Footer Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(false).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(true).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Small File Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(true).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(false).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls);\n+  }\n+\n+  private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs, int fileSize, ReadType readType, int numOfReadCalls) throws Exception {\n+    Path testPath = new Path(\"testFile\");\n+    byte[] fileContent = getRandomBytesArray(fileSize);\n+    try (FSDataOutputStream oStream = fs.create(testPath)) {\n+      oStream.write(fileContent);\n+      oStream.flush();\n+    }\n+    try (FSDataInputStream iStream = fs.open(testPath)) {\n+      int bytesRead = iStream.read(new byte[fileSize], 0,\n+          fileSize);\n+      Assertions.assertThat(fileSize)\n+          .describedAs(\"Read size should match file size\")\n+          .isEqualTo(bytesRead);\n+    }\n+\n+    ArgumentCaptor<String> captor1 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> captor2 = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<byte[]> captor3 = ArgumentCaptor.forClass(byte[].class);\n+    ArgumentCaptor<Integer> captor4 = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Integer> captor5 = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<String> captor6 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<String> captor7 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n+    ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n+\n+    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+        captor1.capture(), captor2.capture(), captor3.capture(),\n+        captor4.capture(), captor5.capture(), captor6.capture(),\n+        captor7.capture(), captor8.capture(), captor9.capture());\n+    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n+    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+  }\n+\n+  private void verifyHeaderForReadTypeInTracingContextHeader(TracingContext tracingContext, ReadType readType) {\n+    AbfsHttpOperation mockOp = Mockito.mock(AbfsHttpOperation.class);\n+    doReturn(EMPTY_STRING).when(mockOp).getTracingContextSuffix();\n+    tracingContext.constructHeader(mockOp, null, null);\n+    String[] idList = tracingContext.getHeader().split(COLON, SPLIT_NO_LIMIT);\n+    Assertions.assertThat(idList).describedAs(\"Client Request Id should have all fields\").hasSize(\n+        TracingHeaderVersion.getCurrentVersion().getFieldCount());\n+    Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Operation Type Should Be Read\")\n+        .contains(FSOperationType.READ.toString());\n+    Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Read type in tracing context header should match\")\n+        .contains(readType.toString());\n+  }\n+\n+//  private testReadTypeInTracingContextHeaderInternal(ReadType readType) throws Exception {\n\nReview Comment:\n   Removed\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250351378\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -77,8 +81,7 @@ public class TracingContext {\n    * this field shall not be set.\n    */\n   private String primaryRequestIdForRetry;\n-\n-  private Integer operatedBlobCount = null;\n+  private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set.\n\nReview Comment:\n   Because it was coming out as null in ClientReqId. Having a null value does not looks good and can be prone to NPE if someone used this value anywhere.\r\n   Since this is set only in rename/delete other ops are prone to NPE.\r\n   \r\n   As to why set to 1, I thought for every operation this has to be 1. I am open to suggestions for a better default value but strongly feel null should be avoided.\r\n   \r\n   Thoughts?\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250351858\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n\nReview Comment:\n   We need to update it to the latest version every time we do a version upgrade.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n\nReview Comment:\n   Fixed\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353075\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n\nReview Comment:\n   With empty checks we cannot have a fixed schema. We need the proper defined schema where each position after split is fixed for all the headers and analysis can be done easily without worrying about the position of info we need to analyse.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n+          + position + COLON\n+          + operatedBlobCount + COLON\n+          + httpOperation.getTracingContextSuffix() + COLON\n+          + getOperationSpecificHeader(opType);\n+\n+      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : EMPTY_STRING;\n       break;\n     case TWO_ID_FORMAT:\n-      header = clientCorrelationID + \":\" + clientRequestId;\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n\nReview Comment:\n   Fixed\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353287\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestTracingContext.java:\n##########\n@@ -326,8 +329,8 @@ fileSystemId, FSOperationType.CREATE_FILESYSTEM, tracingHeaderFormat, new Tracin\n   }\n \n   private void checkHeaderForRetryPolicyAbbreviation(String header, String expectedFailureReason, String expectedRetryPolicyAbbreviation) {\n-    String[] headerContents = header.split(\":\");\n-    String previousReqContext = headerContents[6];\n+    String[] headerContents = header.split(\":\", SPLIT_NO_LIMIT);\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n\nReview Comment:\n   Taken\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353532\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Footer Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(false).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(true).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Small File Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(true).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(false).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls);\n+  }\n\nReview Comment:\n   Added\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353998\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -578,6 +591,7 @@ int readRemote(long position, byte[] b, int offset, int length, TracingContext t\n         streamStatistics.remoteReadOperation();\n       }\n       LOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length);\n+      tracingContext.setPosition(String.valueOf(position));\n\nReview Comment:\n   Thanks for the suggestion. I updated the current test to assert on position as well.\n\n\n\n", "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3149195033\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  79m  1s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5a4dce188fdc 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6345ec99eeb23cd09b5d7197e777b1dec23a35e0 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/7/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250858558\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -77,8 +81,7 @@ public class TracingContext {\n    * this field shall not be set.\n    */\n   private String primaryRequestIdForRetry;\n-\n-  private Integer operatedBlobCount = null;\n+  private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set.\n\nReview Comment:\n   But there was a null check before it was added to the header which would avoid the NPE\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250885278\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -886,40 +928,54 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs,\n     ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n     ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n \n-    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+    verify(fs.getAbfsStore().getClient(), times(totalReadCalls)).read(\n         captor1.capture(), captor2.capture(), captor3.capture(),\n         captor4.capture(), captor5.capture(), captor6.capture(),\n         captor7.capture(), captor8.capture(), captor9.capture());\n-    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n-    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+    List<TracingContext> tracingContextList = captor9.getAllValues();\n+    if (readType == PREFETCH_READ) {\n+      /*\n+       * For Prefetch Enabled, first read can be Normal or Missed Cache Read.\n+       * Sow e will assert only for last 2 calls which should be Prefetched Read.\n\nReview Comment:\n   nit typo: so\n\n\n\n", "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250972747\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderValidator.java:\n##########\n@@ -206,7 +207,7 @@ public void updateReadType(ReadType readType) {\n   }\n \n   /**\n-   * Sets the value of the number of blobs operated on\n+   * Sets the value of the number of blobs operated on976345\n\nReview Comment:\n   typo issue\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -886,40 +928,54 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs,\n     ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n     ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n \n-    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+    verify(fs.getAbfsStore().getClient(), times(totalReadCalls)).read(\n         captor1.capture(), captor2.capture(), captor3.capture(),\n         captor4.capture(), captor5.capture(), captor6.capture(),\n         captor7.capture(), captor8.capture(), captor9.capture());\n-    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n-    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+    List<TracingContext> tracingContextList = captor9.getAllValues();\n+    if (readType == PREFETCH_READ) {\n+      /*\n+       * For Prefetch Enabled, first read can be Normal or Missed Cache Read.\n+       * Sow e will assert only for last 2 calls which should be Prefetched Read.\n+       * Since calls are asynchronous, we can not guarantee the order of calls.\n+       * Therefore, we cannot assert on exact position here.\n+       */\n+      for (int i = tracingContextList.size() - (numOfReadCalls - 1); i < tracingContextList.size(); i++) {\n+        verifyHeaderForReadTypeInTracingContextHeader(tracingContextList.get(i), readType, -1);\n+      }\n+    } else if (readType == DIRECT_READ) {\n+      int expectedReadPos = ONE_MB/3;\n\nReview Comment:\n   comment for why are we starting with this position will help in clarity\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestApacheHttpClientFallback.java:\n##########\n@@ -61,15 +61,15 @@ private TracingContext getSampleTracingContext(int[] jdkCallsRegister,\n           answer.callRealMethod();\n           AbfsHttpOperation op = answer.getArgument(0);\n           if (op instanceof AbfsAHCHttpOperation) {\n-            Assertions.assertThat(tc.getHeader()).contains(APACHE_IMPL);\n+            Assertions.assertThat(tc.getHeader()).endsWith(APACHE_IMPL);\n             apacheCallsRegister[0]++;\n           }\n           if (op instanceof AbfsJdkHttpOperation) {\n             jdkCallsRegister[0]++;\n             if (AbfsApacheHttpClient.usable()) {\n-              Assertions.assertThat(tc.getHeader()).contains(JDK_IMPL);\n+              Assertions.assertThat(tc.getHeader()).endsWith(JDK_IMPL);\n\nReview Comment:\n   this might fail if we add new header where the network library is not maintained as the last header, so contains looks better to me\n\n\n\n", "bhattmanish98 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2251145110\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -64,9 +67,10 @@ public class TracingContext {\n   //final concatenated ID list set into x-ms-client-request-id header\n   private String header = EMPTY_STRING;\n   private String ingressHandler = EMPTY_STRING;\n-  private String position = EMPTY_STRING;\n+  private String position = String.valueOf(0); // position of read/write in remote file\n\nReview Comment:\n   Any reason we are changing this default value?\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2251498573\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -77,8 +81,7 @@ public class TracingContext {\n    * this field shall not be set.\n    */\n   private String primaryRequestIdForRetry;\n-\n-  private Integer operatedBlobCount = null;\n+  private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set.\n\nReview Comment:\n   Yes but we decided to keep the header schema fix and publishing this value as null does not look good in Client Request Id as it can be exposed to user.\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2251499536\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -64,9 +67,10 @@ public class TracingContext {\n   //final concatenated ID list set into x-ms-client-request-id header\n   private String header = EMPTY_STRING;\n   private String ingressHandler = EMPTY_STRING;\n-  private String position = EMPTY_STRING;\n+  private String position = String.valueOf(0); // position of read/write in remote file\n\nReview Comment:\n   No reason, will revert.\n\n\n\n", "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2251505718\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -886,40 +928,54 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs,\n     ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n     ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n \n-    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+    verify(fs.getAbfsStore().getClient(), times(totalReadCalls)).read(\n         captor1.capture(), captor2.capture(), captor3.capture(),\n         captor4.capture(), captor5.capture(), captor6.capture(),\n         captor7.capture(), captor8.capture(), captor9.capture());\n-    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n-    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+    List<TracingContext> tracingContextList = captor9.getAllValues();\n+    if (readType == PREFETCH_READ) {\n+      /*\n+       * For Prefetch Enabled, first read can be Normal or Missed Cache Read.\n+       * Sow e will assert only for last 2 calls which should be Prefetched Read.\n+       * Since calls are asynchronous, we can not guarantee the order of calls.\n+       * Therefore, we cannot assert on exact position here.\n+       */\n+      for (int i = tracingContextList.size() - (numOfReadCalls - 1); i < tracingContextList.size(); i++) {\n+        verifyHeaderForReadTypeInTracingContextHeader(tracingContextList.get(i), readType, -1);\n+      }\n+    } else if (readType == DIRECT_READ) {\n+      int expectedReadPos = ONE_MB/3;\n\nReview Comment:\n   Already added in comment above.\n\n\n\n", "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3151118542\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 36s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  79m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2f9b7814e558 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9059784765d6d3519f9649968b07fd94ef9798e8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/8/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3152193026\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 56s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 142m  5s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3280cde505cb 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c787d3b39af6bacd3d9a003fe3b6f81ccc10e194 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/9/testReport/ |\r\n   | Max. process+thread count | 536 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/9/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 merged PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837\n\n\n", "anujmodi2021 opened a new pull request, #7881:\nURL: https://github.com/apache/hadoop/pull/7881\n\n   ### Description of PR\r\n   Backport for 3.4\r\n   Jira: https://issues.apache.org/jira/browse/HADOOP-19645\r\n   \n\n\n", "anujmodi2021 commented on PR #7881:\nURL: https://github.com/apache/hadoop/pull/7881#issuecomment-3197856944\n\n   ------------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 32\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 172\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 32\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 395\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 33\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 234\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 56\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 285\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 27\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 400\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 33\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 301\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 27\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 346\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 51\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 356\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 32\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 397\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 33\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24\n\n\n", "hadoop-yetus commented on PR #7881:\nURL: https://github.com/apache/hadoop/pull/7881#issuecomment-3198211215\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  12m 11s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 33s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  32m 30s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  32m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 27s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 127m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7881/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7881 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux da6887b03eda 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / dc1821731086534899f15dc2abcc2d9bf2c61ae4 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7881/1/testReport/ |\r\n   | Max. process+thread count | 728 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7881/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "anujmodi2021 merged PR #7881:\nURL: https://github.com/apache/hadoop/pull/7881\n\n\n"], "labels": ["pull-request-available"], "summary": "There are a number of ways in which ABFS driver can trigger a network call to re", "qna": [{"question": "What is the issue title?", "answer": "ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done."}, {"question": "Who reported this issue?", "answer": "Anuj Modi"}]}
{"key": "HADOOP-19644", "project": "HADOOP", "title": "ABFS: [ReadAheadV2] Negative tests for Read Buffer Manager V2 and dynamic scaling", "status": "Open", "reporter": "Anuj Modi", "created": "2025-07-29T10:52:56.000+0000", "description": null, "comments": [], "labels": [], "summary": "", "qna": [{"question": "What is the issue title?", "answer": "ABFS: [ReadAheadV2] Negative tests for Read Buffer Manager V2 and dynamic scaling"}, {"question": "Who reported this issue?", "answer": "Anuj Modi"}]}
{"key": "HADOOP-19643", "project": "HADOOP", "title": "upgrade gson due to security fixes", "status": "Open", "reporter": "PJ Fanning", "created": "2025-07-29T10:11:17.000+0000", "description": "not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE\r\n\r\nlinked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson", "comments": ["pjfanning opened a new pull request, #7833:\nURL: https://github.com/apache/hadoop/pull/7833\n\n   Update LICENSE-binary\r\n   \r\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   https://issues.apache.org/jira/browse/HADOOP-19643\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "hadoop-yetus commented on PR #7833:\nURL: https://github.com/apache/hadoop/pull/7833#issuecomment-3134038336\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 23s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  33m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m 49s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 49s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  51m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 37s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  30m 48s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |  15m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 51s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 22s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  56m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 339m 51s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 16s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 629m  8s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.federation.policies.amrmproxy.TestLocalityMulticastAMRMProxyPolicy |\r\n   |   | hadoop.crypto.key.kms.server.TestKMSAudit |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7833 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 3d08bdf50c29 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 04292f85b2cde016fe4e4c12bfe4c8cd1c040dd4 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/testReport/ |\r\n   | Max. process+thread count | 3066 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "github-actions[bot] commented on PR #7833:\nURL: https://github.com/apache/hadoop/pull/7833#issuecomment-3524493023\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution.\n\n\n", "github-actions[bot] closed pull request #7833: HADOOP-19643. Upgrade to gson 2.13.1\nURL: https://github.com/apache/hadoop/pull/7833\n\n\n"], "labels": ["pull-request-available"], "summary": "not sure why https://github", "qna": [{"question": "What is the issue title?", "answer": "upgrade gson due to security fixes"}, {"question": "Who reported this issue?", "answer": "PJ Fanning"}]}
{"key": "HADOOP-19642", "project": "HADOOP", "title": "upgrade nimbus-jose-jwt due to CVE-2025-53864", "status": "Resolved", "reporter": "PJ Fanning", "created": "2025-07-29T09:37:38.000+0000", "description": "https://www.cve.org/CVERecord?id=CVE-2025-53864", "comments": ["Hi [~fanningpj] , working on a patch for this. I had raised Jira for the same https://issues.apache.org/jira/browse/HADOOP-19632."], "labels": [], "summary": "https://www", "qna": [{"question": "What is the issue title?", "answer": "upgrade nimbus-jose-jwt due to CVE-2025-53864"}, {"question": "Who reported this issue?", "answer": "PJ Fanning"}]}
{"key": "HADOOP-19641", "project": "HADOOP", "title": "ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager", "status": "Open", "reporter": "Anuj Modi", "created": "2025-07-29T07:01:30.000+0000", "description": "We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote read in workload thread itself.\r\n\r\nTo avoid all this overhead, we will always bypass read ahead for the very first read of each input stream and trigger read aheads for second read onwards.", "comments": ["are you using the openFile seek policy as suggested? parquet will tell you when its a parquet file and its read policy is common: 8 byte footer, reall footer, rowgroups. ", "Thanks for the feedback steve. We will definitely incorporate that.\r\nI will hold onto this PR and will make this change with the Read Policy suggested by user taken into consideration.\r\n\r\nWill work diligently on all the read policies and have reads happening in way optimal for each one of them."], "labels": ["Performance"], "summary": "We have observed this across multiple workload runs that when we start reading d", "qna": [{"question": "What is the issue title?", "answer": "ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager"}, {"question": "Who reported this issue?", "answer": "Anuj Modi"}]}
{"key": "HADOOP-19640", "project": "HADOOP", "title": "Resource leak in AssumedRoleCredentialProvider", "status": "Open", "reporter": "Antoni Reus", "created": "2025-07-24T08:01:58.000+0000", "description": "When `org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider` is used in a Hadoop Configuration object, it will attempt to `resolveCredentials()` inside the constructor.\r\n\r\n(lines 165-167)\r\n{code:java}\r\n// and force in a fail-fast check just to keep the stack traces less\r\n// convoluted\r\nresolveCredentials();{code}\r\n\r\nIf this method fails, because the current identity is not able to assume role, the constructor will throw an exception, and fail to close the `stsClient`, `stsProvider` and any other resources that are created in the constructor, leaking threads and other resources.\r\n\u00a0\r\nIn a long running application, that handles Hadoop S3 file systems, where the user can dynamically change to configured role to assume, and external id, this will lead to eventually the system running out of resources due to the leaked threads created by the AWS SDK clients that are not closed when a wrong role or external id is used.\r\n\u00a0\r\n\r\nThere are two potential fixes for this problem:\r\n\r\n\u00a0- Don't attempt to `resolveCredentials()` inside the constructor\r\n\r\n\u00a0- Wrap the `resolveCredentials()` in the constructor in a try/catch block, that cleans the resources and rethrows the exception in the catch block.", "comments": [], "labels": [], "summary": "When `org", "qna": [{"question": "What is the issue title?", "answer": "Resource leak in AssumedRoleCredentialProvider"}, {"question": "Who reported this issue?", "answer": "Antoni Reus"}]}
